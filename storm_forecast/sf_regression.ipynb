{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left\">\n",
    "<img src=\"http://project.inria.fr/saclaycds/files/2017/02/logoUPSayPlusCDS_990.png\" width=\"800px\">\n",
    "</div>\n",
    "\n",
    "# [RAMP](https://www.ramp.studio/problems/storm_forecast_hackathon) on Tropical Storm Intensity Forecast (from reanalysis data)\n",
    "\n",
    "_Sophie Giffard-Roisin (CU/CNRS), Mo Yang (CNRS), Balazs Kegl (CNRS/CDS), Claire Monteleoni (CU/CNRS), Alexandre Boucaud (CNRS/CDS)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Introduction](#Introduction)\n",
    "2. [The prediction task](#The-prediction-task)\n",
    "2. [Installation of libraries](#Installation-of-libraries) : To do before coming!\n",
    "2. [The data](#The-data)\n",
    "3. [The pipeline](#The-pipeline)\n",
    "4. [Evaluation](#Evaluation)\n",
    "5. [Local testing/exploration](#Testing-the-submission)\n",
    "6. [Submission](#Submitting-to-the-online-challenge:-ramp.studio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The goal of the RAMP is to predict the intensity of tropical and extra-tropical storms (24h forecast) using information from past storms since 1979. The intensity can be measured as the maximum sustained wind over a period of one minute at 10 meters height. This speed, calculated every 6 hours, is usually explained in knots (1kt=0.514 m/s) and is used to define the hurricane category from the [Saffir-Simpson scale](https://en.wikipedia.org/wiki/Saffir–Simpson_scale). Estimating the intensity evolution of a storm is of course crucial for the population.\n",
    "\n",
    "<img src=\"https://github.com/sophiegif/ramp_kit_storm_forecast_new/blob/master/figures_pynb/all_storms_since1979_IBTrRACKS_newcats.png?raw=true\" width=\"70%\">\n",
    "<div style=\"text-align: center\">Database: tropical/extra-tropical storm tracks since 1979. Dots = initial position, color = maximal storm strength according to the Saffir-Simpson scale.</div>\n",
    "\n",
    "Today, the forecasts (track and intensity) are provided by a numerous number of guidance models (1). Dynamical models solve the physical equations governing motions in the atmosphere. Statistical models, in contrast, are based on historical relationships between storm behavior and various other parameters. However, the lack of improvement in intensity forecasting is attributed to the complexity of tropical systems and an incomplete understanding of factors that affect their development. What is mainly still hard to predict is the rapid intensification of hurricanes: in 1992, Andrew went from tropical depression to a category 5 hurricane in 24h. \n",
    "\n",
    "Machine learning (and deep learning) methods have been only scarcely tested, and there is hope in that it can improve storm forecasts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The prediction task\n",
    "\n",
    "<ul class=\"list-unstyled list-inline text-center\">\n",
    "  <li>\n",
    "    <img src=\"https://github.com/sophiegif/ramp_kit_storm_forecast_new/blob/master/figures_pynb/storm_shema3.png?raw=true\" alt= \"image1\" width=\"350\" height=\"350\">\n",
    "    <figcaption>Goal: estimate the 24h-forecast intensity of all storms.</figcaption>\n",
    "  </li>\n",
    "  \n",
    "  <li>\n",
    "    <img src=\"https://github.com/sophiegif/ramp_kit_storm_forecast_new/blob/master/figures_pynb/hurricane_pb.png?raw=true\" alt= \"image2\" width=\"350\" height=\"350\">\n",
    "    <figcaption>Feature data: centered maps of wind, altitude, sst, slp, humidity...</figcaption>\n",
    "  </li>\n",
    "</ul>\n",
    "\n",
    "This challenge proposes to design the best algorithm to predict for a large number of storms the 24h-forecast intensity every 6 hours. The (real) database is composed of more than 3000 extra-tropical and tropical storm tracks, and it also provides the intensity and some local physical information at each timestep (2). Moreover, we also provide some 700-hPa and 1000-hPa feature maps of the neighborhood of the storm (from ERA-interm reanalysis database (3)), that can be viewed as images centered on the current storm location (see right image).\n",
    "\n",
    "The goal is to provide for each time step of each storm (total number of instants = 90 000), the predicted 24h-forecast intensity, so 4 time steps in the future. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "\n",
    "1. National Hurricane Center Forecast Verification website, https://www.nhc.noaa.gov/verification/, updated 04 April 2017.\n",
    "\n",
    "2. Knapp, K. R., M. C. Kruk, D. H. Levinson, H. J. Diamond, and C. J. Neumann, 2010: The International Best Track Archive for Climate Stewardship (IBTrACS): Unifying tropical cyclone best track data. Bulletin of the American Meteorological Society, 91, 363-376 https://www.ncdc.noaa.gov/ibtracs/index.php?name=wmo-data\n",
    "\n",
    "3. Dee, D. P. et al.(2011), The ERA-Interim reanalysis: configuration and performance of the data assimilation system. Q.J.R. Meteorol. Soc., 137: 553–597. https://rmets.onlinelibrary.wiley.com/doi/abs/10.1002/qj.828"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation of libraries\n",
    "\n",
    "To get this notebook running and test your models locally using the `ramp_test_submission`, we recommend that you use the Python distribution from [Anaconda](https://www.anaconda.com/download/) or [Miniconda](https://docs.anaconda.com/docs_oss/conda/install/quick#miniconda-quick-install-requirements). (uncomment the lines before running them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install -y -c conda conda-env     # First install conda-env to ease the creation of virtual envs in conda\n",
    "# !conda env create                        # Uses the local environment.yml to create the 'storm_forecast_2' env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OR** if you have Python already installed but are **not using Anaconda**, you'll want to use `pip` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installation of ramp-workflow\n",
    "\n",
    "For being able to test submissions, you also need to have the `ramp-workflow` package locally. You can install the latest version with pip from github:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/paris-saclay-cds/ramp-workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download data (optional)\n",
    "\n",
    "If the data has not yet been downloaded locally, uncomment the following cell and run it.\n",
    "The starting kit data is 260 MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python download_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data\n",
    "\n",
    "The 3000 storms have been separated in a train set, a test set and a local starting kit (train+test sets). The data from `download_data.py` (local starting kit) includes only 1/4 storms of the total database; and the train set on which your code will run on the platform has another half. They are disjoined. \n",
    "\n",
    "Let's have a look at the local train data (only the first rows are plotted)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from problem import get_train_data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 100\n",
    "pd.options.display.max_columns = 100\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# RMSE will be in the same units as the output variable(windspeed in knots)\n",
    "# Data Exploration: the training set has 15777 training examples with 859 features each.\n",
    "data_train, y_true = get_train_data()\n",
    "\n",
    "\n",
    "#Other Ideas:\n",
    "# Gaussian Processs\n",
    "# Arima times series\n",
    "#0D, 0D+ZUV, 0D+SSHV, 0D+ZUV+SSHV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15777, 392)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "import itertools\n",
    "\n",
    "# This actually decreases performance:\n",
    "'''\n",
    "data_train['previous_windspeed'] = data_train['windspeed'].shift(1)\n",
    "data_train['previous_windspeed'].loc[data_train['stormid'] != data_train['stormid'].shift(1)] = np.nan\n",
    "data_train['previous_windspeed'].loc[data_train['previous_windspeed'] == np.nan] = data_train['windspeed']\n",
    "data_train['previous_windspeed'].fillna(method='bfill', inplace = True)\n",
    "'''\n",
    "\n",
    "# This is an illegal lookahead???:\n",
    "'''\n",
    "data_train['previous_y'] = y_true\n",
    "data_train['previous_y'] = data_train['previous_y'].shift(1)\n",
    "data_train['previous_y'].loc[data_train['stormid'] != data_train['stormid'].shift(1)] = np.nan\n",
    "data_train['previous_y'].fillna(method='bfill', inplace = True)\n",
    "'''\n",
    "\n",
    "# Manually add dummy variables for the basins:\n",
    "extra_dummy = pd.DataFrame(data = np.zeros((len(data_train),8)), columns = ['basin_7', 'basin_8', 'basin_9', 'basin_10', 'basin_11', 'basin_12', 'basin_13', 'basin_14'])\n",
    "\n",
    "# For \"illegal lookahead\" features:\n",
    "#zero_d = pd.concat([data_train.loc[:,'stormid'], data_train.loc[:,'latitude':'max_wind_change_12h'], data_train.loc[:,'dist2land'], pd.get_dummies(data_train.nature, prefix = 'nature'), pd.get_dummies(data_train.basin, prefix = 'basin'), extra_dummy, data_train.loc[:,'previous_y']], axis = 1)\n",
    "\n",
    "# For basin and nature features:\n",
    "zero_d = pd.concat([data_train.loc[:,'stormid'], data_train.loc[:,'latitude':'max_wind_change_12h'], data_train.loc[:,'dist2land'], pd.get_dummies(data_train.nature, prefix = 'nature'), pd.get_dummies(data_train.basin, prefix = 'basin'), extra_dummy], axis = 1)\n",
    "\n",
    "# For basin, nature, and previous windspeed features:\n",
    "#zero_d = pd.concat([data_train.loc[:,'stormid'], data_train.loc[:,'latitude':'max_wind_change_12h'], data_train.loc[:,'dist2land'], pd.get_dummies(data_train.nature, prefix = 'nature'), pd.get_dummies(data_train.basin, prefix = 'basin'), extra_dummy, data_train.loc[:,'previous_windspeed']], axis = 1)\n",
    "\n",
    "# For nature feature:\n",
    "#zero_d = pd.concat([data_train.loc[:,'latitude':'max_wind_change_12h'], data_train.loc[:,'dist2land'], pd.get_dummies(data_train.nature, prefix = 'nature')], axis = 1)\n",
    "\n",
    "all_features = pd.concat([data_train.loc[:,'stormid'], data_train.loc[:,'latitude':'max_wind_change_12h'],pd.get_dummies(data_train.nature, prefix = 'nature'), pd.get_dummies(data_train.basin, prefix = 'basin'), extra_dummy, data_train.loc[:,'dist2land':] ], axis = 1)\n",
    "\n",
    "zero_d_uvz = pd.concat([zero_d, data_train[[col for col in data_train.columns if col.startswith(('z_','u_','v_'))]]], axis = 1)\n",
    "\n",
    "zero_d_sst= pd.concat([zero_d, data_train[[col for col in data_train.columns if col.startswith(('sst_','slp_','hum_'))]]], axis = 1)\n",
    "#print(all_features.loc[:,'sst_0_0':'sst_10_10'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy of their cross-validation function: \n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "def get_cv(X, y, num_splits):\n",
    "    group = np.array(X['stormid'])\n",
    "    X, y, group = shuffle(X, y, group, random_state=3)\n",
    "    gkf = GroupKFold(n_splits=num_splits).split(X, y, group)\n",
    "    return gkf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements in Dict: 1\n",
      "Elements in Dict: 2\n",
      "Elements in Dict: 3\n",
      "Elements in Dict: 4\n",
      "Elements in Dict: 5\n",
      "Elements in Dict: 6\n",
      "Elements in Dict: 7\n",
      "Elements in Dict: 8\n",
      "Elements in Dict: 9\n",
      "Elements in Dict: 10\n",
      "Elements in Dict: 11\n",
      "Elements in Dict: 12\n",
      "Elements in Dict: 13\n",
      "Elements in Dict: 14\n",
      "Elements in Dict: 15\n",
      "Elements in Dict: 16\n",
      "Elements in Dict: 17\n",
      "Elements in Dict: 18\n",
      "Elements in Dict: 19\n",
      "Elements in Dict: 20\n",
      "Elements in Dict: 21\n",
      "Elements in Dict: 22\n",
      "Elements in Dict: 23\n",
      "Elements in Dict: 24\n",
      "Elements in Dict: 25\n",
      "Elements in Dict: 26\n",
      "Elements in Dict: 27\n",
      "Elements in Dict: 28\n",
      "Elements in Dict: 29\n",
      "Elements in Dict: 30\n",
      "Elements in Dict: 31\n",
      "Elements in Dict: 32\n",
      "Elements in Dict: 33\n",
      "Elements in Dict: 34\n",
      "Elements in Dict: 35\n",
      "Elements in Dict: 36\n",
      "Elements in Dict: 37\n",
      "Elements in Dict: 38\n",
      "Elements in Dict: 39\n",
      "Elements in Dict: 40\n",
      "Elements in Dict: 41\n",
      "Elements in Dict: 42\n",
      "Elements in Dict: 43\n",
      "Elements in Dict: 44\n",
      "Elements in Dict: 45\n",
      "Elements in Dict: 46\n",
      "Elements in Dict: 47\n",
      "Elements in Dict: 48\n",
      "Elements in Dict: 49\n",
      "Elements in Dict: 50\n",
      "Elements in Dict: 51\n",
      "Elements in Dict: 52\n",
      "Elements in Dict: 53\n",
      "Elements in Dict: 54\n",
      "Elements in Dict: 55\n",
      "Elements in Dict: 56\n",
      "Elements in Dict: 57\n",
      "Elements in Dict: 58\n",
      "Elements in Dict: 59\n",
      "Elements in Dict: 60\n",
      "Elements in Dict: 61\n",
      "Elements in Dict: 62\n",
      "Elements in Dict: 63\n",
      "Elements in Dict: 64\n",
      "Elements in Dict: 65\n",
      "Elements in Dict: 66\n",
      "Elements in Dict: 67\n",
      "Elements in Dict: 68\n",
      "Elements in Dict: 69\n",
      "Elements in Dict: 70\n",
      "Elements in Dict: 71\n",
      "Elements in Dict: 72\n",
      "Elements in Dict: 73\n",
      "Elements in Dict: 74\n",
      "Elements in Dict: 75\n",
      "Elements in Dict: 76\n",
      "Elements in Dict: 77\n",
      "Elements in Dict: 78\n",
      "Elements in Dict: 79\n",
      "Elements in Dict: 80\n",
      "Elements in Dict: 81\n",
      "Elements in Dict: 82\n",
      "Elements in Dict: 83\n",
      "Elements in Dict: 84\n",
      "Elements in Dict: 85\n",
      "Elements in Dict: 86\n",
      "Elements in Dict: 87\n",
      "Elements in Dict: 88\n",
      "Elements in Dict: 89\n",
      "Elements in Dict: 90\n"
     ]
    }
   ],
   "source": [
    "# Homemade Forest Parameter Tuning \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "\n",
    "max_depth = [5,10,15,20,25]\n",
    "max_features = ['sqrt', 'log2', None]\n",
    "n_estimators = [10,25,50,100,250,500]\n",
    "\n",
    "cart_prod = list(itertools.product(max_depth,max_features,n_estimators))\n",
    "n_splits = 5\n",
    "forest_dict = {}\n",
    "\n",
    "for param_combo in cart_prod:\n",
    "    mse_sum = 0\n",
    "    for train_index, test_index in get_cv(zero_d_sst,y_true,n_splits):\n",
    "        forest_regressor = Pipeline([('imputer', Imputer(strategy='median')), \n",
    "            ('scale', StandardScaler()),       \n",
    "            ('regressor', RandomForestRegressor(max_depth = param_combo[0], max_features = param_combo[1], n_estimators = param_combo[2]))])\n",
    "        drop = zero_d_sst.drop(columns = ['stormid'])\n",
    "        \n",
    "        X_train, X_test = drop.iloc[train_index,:], drop.iloc[test_index,:]\n",
    "        y_train, y_test = y_true[train_index], y_true[test_index]\n",
    "        \n",
    "        forest_regressor.fit(X_train,y_train)\n",
    "        y_pred = forest_regressor.predict(X_test)\n",
    "        mse_sum += mean_squared_error(y_test, y_pred)\n",
    "        \n",
    "    forest_dict[param_combo] = np.sqrt(mse_sum/n_splits)\n",
    "    print('Elements in Dict:', len(forest_dict))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(5, 'sqrt', 10): 20.70002509543211, (5, 'sqrt', 25): 20.791312717638935, (5, 'sqrt', 50): 20.27275259414644, (5, 'sqrt', 100): 20.51600651494993, (5, 'sqrt', 250): 20.36121316020932, (5, 'sqrt', 500): 20.294065011222955, (5, 'log2', 10): 22.746135950886444, (5, 'log2', 25): 21.94312625956107, (5, 'log2', 50): 22.360862247821707, (5, 'log2', 100): 22.409064403616096, (5, 'log2', 250): 22.386451013987692, (5, 'log2', 500): 22.282772045796328, (5, None, 10): 15.061363347785248, (5, None, 25): 15.019704322685703, (5, None, 50): 14.983472238851288, (5, None, 100): 14.9765434707819, (5, None, 250): 14.952821993819397, (5, None, 500): 14.950635939674426, (10, 'sqrt', 10): 17.461374659423992, (10, 'sqrt', 25): 16.865439456330048, (10, 'sqrt', 50): 16.6044826407133, (10, 'sqrt', 100): 16.489662507337698, (10, 'sqrt', 250): 16.41717315979207, (10, 'sqrt', 500): 16.39304283271776, (10, 'log2', 10): 19.53569064219458, (10, 'log2', 25): 18.78502301966588, (10, 'log2', 50): 18.80017365807198, (10, 'log2', 100): 18.794801012673684, (10, 'log2', 250): 18.80726840983818, (10, 'log2', 500): 18.74469414430586, (10, None, 10): 13.505072263753988, (10, None, 25): 13.167786306576506, (10, None, 50): 13.158906097605302, (10, None, 100): 13.062266797707858, (10, None, 250): 13.05501486266437, (10, None, 500): 13.051014713490725, (15, 'sqrt', 10): 15.70268780968321, (15, 'sqrt', 25): 15.26957395773248, (15, 'sqrt', 50): 14.869574513057234, (15, 'sqrt', 100): 14.856713370694509, (15, 'sqrt', 250): 14.710955905840967, (15, 'sqrt', 500): 14.750843544922635, (15, 'log2', 10): 17.985830042893244, (15, 'log2', 25): 17.481656676583896, (15, 'log2', 50): 17.050343266513842, (15, 'log2', 100): 17.144262141654583, (15, 'log2', 250): 17.053853167692758, (15, 'log2', 500): 16.99117766569841, (15, None, 10): 13.304488904595607, (15, None, 25): 12.847521274831118, (15, None, 50): 12.669286198079103, (15, None, 100): 12.635837571269718, (15, None, 250): 12.55003073871048, (15, None, 500): 12.55314619697715, (20, 'sqrt', 10): 15.785889825588349, (20, 'sqrt', 25): 14.972130152087797, (20, 'sqrt', 50): 14.676376655666624, (20, 'sqrt', 100): 14.448018699333515, (20, 'sqrt', 250): 14.397431559068044, (20, 'sqrt', 500): 14.326868845193786, (20, 'log2', 10): 17.81368761110693, (20, 'log2', 25): 16.968740484110782, (20, 'log2', 50): 16.815791217536944, (20, 'log2', 100): 16.44715803158047, (20, 'log2', 250): 16.396037420245804, (20, 'log2', 500): 16.40693754280788, (20, None, 10): 13.164134311137992, (20, None, 25): 12.827068235807259, (20, None, 50): 12.573845177759551, (20, None, 100): 12.520505922992795, (20, None, 250): 12.478151786584718, (20, None, 500): 12.45895041447051, (25, 'sqrt', 10): 15.553308821584936, (25, 'sqrt', 25): 15.038803208720735, (25, 'sqrt', 50): 14.689813216366177, (25, 'sqrt', 100): 14.45036782052605, (25, 'sqrt', 250): 14.345801526496725, (25, 'sqrt', 500): 14.334903825102817, (25, 'log2', 10): 17.686802733710028, (25, 'log2', 25): 16.714616489970215, (25, 'log2', 50): 16.551921643008676, (25, 'log2', 100): 16.39317701669965, (25, 'log2', 250): 16.31400498947809, (25, 'log2', 500): 16.29133287178222, (25, None, 10): 13.241097117582298, (25, None, 25): 12.75900581370358, (25, None, 50): 12.618663347719727, (25, None, 100): 12.471851514076505, (25, None, 250): 12.474939312963281, (25, None, 500): 12.449815206643601}\n"
     ]
    }
   ],
   "source": [
    "print(forest_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements in Dict: 1\n",
      "Elements in Dict: 2\n",
      "Elements in Dict: 3\n",
      "Elements in Dict: 4\n",
      "Elements in Dict: 5\n",
      "Elements in Dict: 6\n",
      "Elements in Dict: 7\n",
      "Elements in Dict: 8\n",
      "Elements in Dict: 9\n",
      "Elements in Dict: 10\n",
      "Elements in Dict: 11\n",
      "Elements in Dict: 12\n",
      "Elements in Dict: 13\n",
      "Elements in Dict: 14\n",
      "Elements in Dict: 15\n",
      "Elements in Dict: 16\n",
      "Elements in Dict: 17\n",
      "Elements in Dict: 18\n"
     ]
    }
   ],
   "source": [
    "# Homemade Tree Parameter Tuning \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "max_depth = [5,10,15,20,25,50]\n",
    "max_features = [None, 'sqrt', 'log2']\n",
    "\n",
    "cart_prod = list(itertools.product(max_depth,max_features))\n",
    "n_splits = 5\n",
    "tree_dict = {}\n",
    "\n",
    "for param_combo in cart_prod:\n",
    "    mse_sum = 0\n",
    "    for train_index, test_index in get_cv(zero_d_sst,y_true,n_splits):\n",
    "        tree_regressor = Pipeline([('imputer', Imputer(strategy='median')), \n",
    "            ('scale', StandardScaler()),\n",
    "            ('regressor', DecisionTreeRegressor(max_depth = param_combo[0], max_features = param_combo[1]))])\n",
    "        drop = zero_d_sst.drop(columns = ['stormid'])\n",
    "        X_train, X_test = drop.iloc[train_index,:], drop.iloc[test_index,:]\n",
    "        y_train, y_test = y_true[train_index], y_true[test_index]\n",
    "        tree_regressor.fit(X_train,y_train)\n",
    "        y_pred = tree_regressor.predict(X_test)\n",
    "        mse_sum += mean_squared_error(y_test, y_pred)\n",
    "    tree_dict[param_combo] = np.sqrt(mse_sum/n_splits)\n",
    "    print('Elements in Dict:', len(tree_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements in Dict: 1\n",
      "Elements in Dict: 2\n",
      "Elements in Dict: 3\n",
      "Elements in Dict: 4\n",
      "Elements in Dict: 5\n",
      "Elements in Dict: 6\n",
      "Elements in Dict: 7\n",
      "Elements in Dict: 8\n",
      "Elements in Dict: 9\n",
      "Elements in Dict: 10\n",
      "Elements in Dict: 11\n",
      "Elements in Dict: 12\n",
      "Elements in Dict: 13\n",
      "Elements in Dict: 14\n",
      "Elements in Dict: 15\n",
      "Elements in Dict: 16\n",
      "Elements in Dict: 17\n",
      "Elements in Dict: 18\n",
      "Elements in Dict: 19\n",
      "Elements in Dict: 20\n",
      "Elements in Dict: 21\n",
      "Elements in Dict: 22\n",
      "Elements in Dict: 23\n",
      "Elements in Dict: 24\n",
      "Elements in Dict: 25\n",
      "Elements in Dict: 26\n",
      "Elements in Dict: 27\n",
      "Elements in Dict: 28\n",
      "Elements in Dict: 29\n",
      "Elements in Dict: 30\n",
      "Elements in Dict: 31\n",
      "Elements in Dict: 32\n",
      "Elements in Dict: 33\n",
      "Elements in Dict: 34\n",
      "Elements in Dict: 35\n",
      "Elements in Dict: 36\n",
      "Elements in Dict: 37\n",
      "Elements in Dict: 38\n",
      "Elements in Dict: 39\n",
      "Elements in Dict: 40\n",
      "Elements in Dict: 41\n",
      "Elements in Dict: 42\n",
      "Elements in Dict: 43\n",
      "Elements in Dict: 44\n",
      "Elements in Dict: 45\n",
      "Elements in Dict: 46\n",
      "Elements in Dict: 47\n",
      "Elements in Dict: 48\n",
      "Elements in Dict: 49\n",
      "Elements in Dict: 50\n",
      "Elements in Dict: 51\n",
      "Elements in Dict: 52\n",
      "Elements in Dict: 53\n",
      "Elements in Dict: 54\n",
      "Elements in Dict: 55\n",
      "Elements in Dict: 56\n"
     ]
    }
   ],
   "source": [
    "# Homemade SVR(RBF) Parameter Tuning \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "C = np.logspace(-3, 3, num=7)\n",
    "gamma = np.logspace(-5, 2, num=8)\n",
    "\n",
    "cart_prod = list(itertools.product(C,gamma))\n",
    "n_splits = 5\n",
    "rbf_dict = {}\n",
    "\n",
    "for param_combo in cart_prod:\n",
    "    mse_sum = 0\n",
    "    for train_index, test_index in get_cv(zero_d_sst,y_true,n_splits):\n",
    "        rbf_regressor = Pipeline([('imputer', Imputer(strategy='median')),\n",
    "            ('scale', StandardScaler()),\n",
    "            ('regressor', SVR(kernel = 'rbf',C = param_combo[0], gamma = param_combo[1]))])\n",
    "        drop = zero_d_sst.drop(columns = ['stormid'])\n",
    "        \n",
    "        X_train, X_test = drop.iloc[train_index,:], drop.iloc[test_index,:]\n",
    "        y_train, y_test = y_true[train_index], y_true[test_index]\n",
    "        rbf_regressor.fit(X_train,y_train)\n",
    "        y_pred = rbf_regressor.predict(X_test)\n",
    "        mse_sum += mean_squared_error(y_test, y_pred)\n",
    "        \n",
    "    rbf_dict[param_combo] = np.sqrt(mse_sum/n_splits)\n",
    "    print('Elements in Dict:', len(rbf_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements in Dict: 1\n",
      "Elements in Dict: 2\n",
      "Elements in Dict: 3\n",
      "Elements in Dict: 4\n",
      "Elements in Dict: 5\n",
      "Elements in Dict: 6\n",
      "Elements in Dict: 7\n",
      "Elements in Dict: 8\n",
      "Elements in Dict: 9\n",
      "Elements in Dict: 10\n",
      "Elements in Dict: 11\n",
      "Elements in Dict: 12\n",
      "Elements in Dict: 13\n",
      "Elements in Dict: 14\n",
      "Elements in Dict: 15\n",
      "Elements in Dict: 16\n",
      "Elements in Dict: 17\n",
      "Elements in Dict: 18\n",
      "Elements in Dict: 19\n",
      "Elements in Dict: 20\n",
      "Elements in Dict: 21\n",
      "Elements in Dict: 22\n",
      "Elements in Dict: 23\n",
      "Elements in Dict: 24\n",
      "Elements in Dict: 25\n",
      "Elements in Dict: 26\n",
      "Elements in Dict: 27\n",
      "Elements in Dict: 28\n",
      "Elements in Dict: 29\n",
      "Elements in Dict: 30\n",
      "Elements in Dict: 31\n",
      "Elements in Dict: 32\n",
      "Elements in Dict: 33\n",
      "Elements in Dict: 34\n",
      "Elements in Dict: 35\n",
      "Elements in Dict: 36\n",
      "Elements in Dict: 37\n",
      "Elements in Dict: 38\n",
      "Elements in Dict: 39\n",
      "Elements in Dict: 40\n",
      "Elements in Dict: 41\n",
      "Elements in Dict: 42\n",
      "Elements in Dict: 43\n",
      "Elements in Dict: 44\n",
      "Elements in Dict: 45\n",
      "Elements in Dict: 46\n",
      "Elements in Dict: 47\n",
      "Elements in Dict: 48\n",
      "Elements in Dict: 49\n",
      "Elements in Dict: 50\n",
      "Elements in Dict: 51\n",
      "Elements in Dict: 52\n",
      "Elements in Dict: 53\n",
      "Elements in Dict: 54\n",
      "Elements in Dict: 55\n",
      "Elements in Dict: 56\n",
      "Elements in Dict: 57\n",
      "Elements in Dict: 58\n",
      "Elements in Dict: 59\n",
      "Elements in Dict: 60\n",
      "Elements in Dict: 61\n",
      "Elements in Dict: 62\n",
      "Elements in Dict: 63\n",
      "Elements in Dict: 64\n",
      "Elements in Dict: 65\n",
      "Elements in Dict: 66\n",
      "Elements in Dict: 67\n",
      "Elements in Dict: 68\n",
      "Elements in Dict: 69\n"
     ]
    }
   ],
   "source": [
    "# Homemade Boosted Forest Parameter Tuning \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "max_depth = [5,10,15,20,25]\n",
    "max_features = ['sqrt', 'log2', None]\n",
    "n_estimators = [10,25,50,100,250,500]\n",
    "\n",
    "cart_prod = list(itertools.product(max_depth,max_features,n_estimators))\n",
    "n_splits = 5\n",
    "boost_dict = {}\n",
    "\n",
    "for param_combo in cart_prod:\n",
    "    mse_sum = 0\n",
    "    for train_index, test_index in get_cv(zero_d_sst,y_true,n_splits):\n",
    "        boost_regressor = Pipeline([('imputer', Imputer(strategy='median')),\n",
    "            ('scale', StandardScaler()),\n",
    "            ('regressor', GradientBoostingRegressor(max_depth = param_combo[0], max_features = param_combo[1], n_estimators = param_combo[2]))])\n",
    "        drop = zero_d_sst.drop(columns = ['stormid'])\n",
    "        \n",
    "        X_train, X_test = drop.iloc[train_index,:], drop.iloc[test_index,:]\n",
    "        y_train, y_test = y_true[train_index], y_true[test_index]\n",
    "        \n",
    "        boost_regressor.fit(X_train,y_train)\n",
    "        y_pred = boost_regressor.predict(X_test)\n",
    "        mse_sum += mean_squared_error(y_test, y_pred)\n",
    "        \n",
    "    boost_dict[param_combo] = np.sqrt(mse_sum/n_splits)\n",
    "    print('Elements in Dict:', len(boost_dict))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements in Dict: 1\n",
      "Elements in Dict: 2\n",
      "Elements in Dict: 3\n",
      "Elements in Dict: 4\n",
      "Elements in Dict: 5\n",
      "Elements in Dict: 6\n"
     ]
    }
   ],
   "source": [
    "# Homemade Lasso Parameter Tuning \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "alpha = [.00001, .0001, .001, .01, .1, 1]\n",
    "\n",
    "n_splits = 5\n",
    "lasso_dict = {}\n",
    "\n",
    "for a in alpha:\n",
    "    mse_sum = 0\n",
    "    for train_index, test_index in get_cv(zero_d_sst,y_true,n_splits):\n",
    "        lasso_regressor = Pipeline([('imputer', Imputer(strategy='median')),\n",
    "            ('scale', StandardScaler()),\n",
    "            ('regressor', Lasso(alpha = a))])\n",
    "        drop = zero_d_sst.drop(columns = ['stormid'])\n",
    "        \n",
    "        X_train, X_test = drop.iloc[train_index,:], drop.iloc[test_index,:]\n",
    "        y_train, y_test = y_true[train_index], y_true[test_index]\n",
    "        \n",
    "        lasso_regressor.fit(X_train,y_train)\n",
    "        y_pred = lasso_regressor.predict(X_test)\n",
    "        mse_sum += mean_squared_error(y_test, y_pred)\n",
    "        \n",
    "    lasso_dict[a] = np.sqrt(mse_sum/n_splits)\n",
    "    print('Elements in Dict:', len(lasso_dict))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements in Dict: 1\n",
      "Elements in Dict: 2\n",
      "Elements in Dict: 3\n",
      "Elements in Dict: 4\n",
      "Elements in Dict: 5\n",
      "Elements in Dict: 6\n",
      "Elements in Dict: 7\n",
      "Elements in Dict: 8\n",
      "Elements in Dict: 9\n",
      "Elements in Dict: 10\n"
     ]
    }
   ],
   "source": [
    "# Homemade Ridge Parameter Tuning \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "alpha = [1, 2, 5, 10, 25, 50, 100, 200, 500, 1000]\n",
    "\n",
    "n_splits = 5\n",
    "ridge_dict = {}\n",
    "\n",
    "for a in alpha:\n",
    "    mse_sum = 0\n",
    "    for train_index, test_index in get_cv(zero_d_sst,y_true,n_splits):\n",
    "        ridge_regressor = Pipeline([('imputer', Imputer(strategy='median')),\n",
    "            ('scale', StandardScaler()),\n",
    "            ('regressor', Ridge(alpha = a))])\n",
    "        drop = zero_d_sst.drop(columns = ['stormid'])\n",
    "        \n",
    "        X_train, X_test = drop.iloc[train_index,:], drop.iloc[test_index,:]\n",
    "        y_train, y_test = y_true[train_index], y_true[test_index]\n",
    "        \n",
    "        ridge_regressor.fit(X_train,y_train)\n",
    "        y_pred = ridge_regressor.predict(X_test)\n",
    "        mse_sum += mean_squared_error(y_test, y_pred)\n",
    "        \n",
    "    ridge_dict[a] = np.sqrt(mse_sum/n_splits)\n",
    "    print('Elements in Dict:', len(ridge_dict))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_d_sst_tuning = {'forest_regressor':forest_dict, 'tree_regressor':tree_dict, 'ridge_regressor': ridge_dict, 'lasso_regressor':lasso_dict, 'rbf_regressor':rbf_dict,} #,'boost_regressor':boost_dict, \n",
    "np.save('storm_forecast_zero_d_sst.npy', zero_d_sst_tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the data is a list of time instants (one every 6h). The first storm will result in x lines beginning with its stormid and the corresponding time step, with all the associated features on the same row. Then the time steps from the second storm will be below, and so on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tree_regressor': {(None, None): 15.989472125530884, (None, 'sqrt'): 16.347406954008108, (None, 'log2'): 16.702581169780256, (5, None): 16.034478132623686, (5, 'sqrt'): 20.723067925714563, (5, 'log2'): 21.877929581513452, (10, None): 15.28382747310722, (10, 'sqrt'): 16.71674378533502, (10, 'log2'): 17.683040662028652, (15, None): 15.606493138905133, (15, 'sqrt'): 15.798177651618964, (15, 'log2'): 16.20456265420892, (20, None): 15.907228246892817, (20, 'sqrt'): 15.878080586335606, (20, 'log2'): 16.005314806103403, (25, None): 16.065276844548134, (25, 'sqrt'): 16.257488353017443, (25, 'log2'): 16.203664915211405, (50, None): 15.972658958788609, (50, 'sqrt'): 16.02628997645109, (50, 'log2'): 16.64913966347705}, 'forest_regressor': {(5, 'sqrt', 10): 17.959449000684575, (5, 'sqrt', 25): 17.63518978197033, (5, 'sqrt', 50): 17.731902717171902, (5, 'sqrt', 100): 17.533463793897006, (5, 'sqrt', 250): 17.58808225631949, (5, 'sqrt', 500): 17.66336810779933, (5, 'log2', 10): 18.840011798189746, (5, 'log2', 25): 18.40494667617526, (5, 'log2', 50): 18.253418730916877, (5, 'log2', 100): 18.35051190923459, (5, 'log2', 250): 18.283910289886993, (5, 'log2', 500): 18.37255404369703, (5, None, 10): 15.653138373217082, (5, None, 25): 15.639203291707128, (5, None, 50): 15.627621006658819, (5, None, 100): 15.593103591290799, (5, None, 250): 15.594377867997096, (5, None, 500): 15.59494613953613, (10, 'sqrt', 10): 14.77194048824494, (10, 'sqrt', 25): 14.143325611965729, (10, 'sqrt', 50): 14.17139925882914, (10, 'sqrt', 100): 14.024876158527281, (10, 'sqrt', 250): 14.061387013171775, (10, 'sqrt', 500): 14.05953054242426, (10, 'log2', 10): 14.755353048729203, (10, 'log2', 25): 14.473106143043537, (10, 'log2', 50): 14.442068050923982, (10, 'log2', 100): 14.41944969931145, (10, 'log2', 250): 14.373616470820863, (10, 'log2', 500): 14.389658849790045, (10, None, 10): 13.765299039559808, (10, None, 25): 13.596243219798275, (10, None, 50): 13.54870399536185, (10, None, 100): 13.483590040746204, (10, None, 250): 13.472156264801313, (10, None, 500): 13.474649145354869, (15, 'sqrt', 10): 12.733403149452856, (15, 'sqrt', 25): 12.26733114532254, (15, 'sqrt', 50): 12.223719868850747, (15, 'sqrt', 100): 12.158687245180404, (15, 'sqrt', 250): 12.117416255830639, (15, 'sqrt', 500): 12.074878092138311, (15, 'log2', 10): 12.76287113041079, (15, 'log2', 25): 12.45108347747718, (15, 'log2', 50): 12.334546283006832, (15, 'log2', 100): 12.24883823458459, (15, 'log2', 250): 12.186436027929638, (15, 'log2', 500): 12.183148461244233, (15, None, 10): 12.878762259396552, (15, None, 25): 12.543340468758492, (15, None, 50): 12.351388510978712, (15, None, 100): 12.325842993779267, (15, None, 250): 12.303280611611005, (15, None, 500): 12.288268114405147, (20, 'sqrt', 10): 12.158825625218705, (20, 'sqrt', 25): 11.571809510034162, (20, 'sqrt', 50): 11.438341636836428, (20, 'sqrt', 100): 11.371491086940727, (20, 'sqrt', 250): 11.31172244012929, (20, 'sqrt', 500): 11.302093344600625, (20, 'log2', 10): 12.00683669701825, (20, 'log2', 25): 11.491356309031522, (20, 'log2', 50): 11.439843172408178, (20, 'log2', 100): 11.365121440674388, (20, 'log2', 250): 11.307320342884733, (20, 'log2', 500): 11.277480113408979, (20, None, 10): 12.587578668694169, (20, None, 25): 12.204503246388786, (20, None, 50): 12.05827870772903, (20, None, 100): 12.01162741108642, (20, None, 250): 11.970654198801997, (20, None, 500): 11.957343874271396, (25, 'sqrt', 10): 11.867214797320207, (25, 'sqrt', 25): 11.450381065568916, (25, 'sqrt', 50): 11.249214995142115, (25, 'sqrt', 100): 11.191984062133507, (25, 'sqrt', 250): 11.141991022692093, (25, 'sqrt', 500): 11.129031644312146, (25, 'log2', 10): 11.819250333867782, (25, 'log2', 25): 11.42465138535548, (25, 'log2', 50): 11.20238807028906, (25, 'log2', 100): 11.09755081984173, (25, 'log2', 250): 11.085719659528166, (25, 'log2', 500): 11.059725741413919, (25, None, 10): 12.616576887271309, (25, None, 25): 12.189706467236318, (25, None, 50): 12.06050511172002, (25, None, 100): 11.952506183788968, (25, None, 250): 11.921199887368028, (25, None, 500): 11.904063729877002}, 'rbf_regressor': {(0.001, 1e-05): 27.26005806066758, (0.001, 0.0001): 27.258579185224903, (0.001, 0.001): 27.24386902519786, (0.001, 0.01): 27.14223054656312, (0.001, 0.1): 27.10128549880395, (0.001, 1.0): 27.22166151708824, (0.001, 10.0): 27.25723042717665, (0.001, 100.0): 27.259682346056298, (0.01, 1e-05): 27.258573604808905, (0.01, 0.0001): 27.24331884287166, (0.01, 0.001): 27.095884134638634, (0.01, 0.01): 26.072626577521426, (0.01, 0.1): 25.81071954119345, (0.01, 1.0): 26.84592513985449, (0.01, 10.0): 27.230389717937186, (0.01, 100.0): 27.254864647520147, (0.1, 1e-05): 27.24326090426119, (0.1, 0.0001): 27.09001523068904, (0.1, 0.001): 25.659911328057593, (0.1, 0.01): 20.854140977678544, (0.1, 0.1): 20.30085703323045, (0.1, 1.0): 24.48138270871055, (0.1, 10.0): 26.94637517525358, (0.1, 100.0): 27.20711303596932, (1.0, 1e-05): 27.08939809724857, (1.0, 0.0001): 25.611136689559263, (1.0, 0.001): 19.788463386131422, (1.0, 0.01): 16.60296037910258, (1.0, 0.1): 15.708741629914456, (1.0, 1.0): 19.085942011478156, (1.0, 10.0): 25.087199477215865, (1.0, 100.0): 26.750662841189705, (10.0, 1e-05): 25.606171521271673, (10.0, 0.0001): 19.69389247966546, (10.0, 0.001): 17.00339308741588, (10.0, 0.01): 15.866416801901785, (10.0, 0.1): 14.683051596583365, (10.0, 1.0): 14.408900746652426, (10.0, 10.0): 19.84370326474744, (10.0, 100.0): 24.689585577714016, (100.0, 1e-05): 19.683975304646363, (100.0, 0.0001): 17.09602807880077, (100.0, 0.001): 16.71802548455496, (100.0, 0.01): 15.374500903863972, (100.0, 0.1): 14.153275695458898, (100.0, 1.0): 12.920761077089853, (100.0, 10.0): 16.224722245065973, (100.0, 100.0): 22.748115847630668, (1000.0, 1e-05): 17.106539822093406, (1000.0, 0.0001): 17.0727126996955, (1000.0, 0.001): 16.16579388190221, (1000.0, 0.01): 14.934400842042189, (1000.0, 0.1): 13.764540916550354}, 'boost_regressor': {(5, 'sqrt', 10): 44.52871744421699, (5, 'sqrt', 25): 44.52871744421699, (5, 'sqrt', 50): 44.52871744421699, (5, 'sqrt', 100): 44.52871744421699, (5, 'sqrt', 250): 44.52871744421699, (5, 'sqrt', 500): 44.52871744421699, (5, 'log2', 10): 44.52871744421699, (5, 'log2', 25): 44.52871744421699, (5, 'log2', 50): 44.52871744421699, (5, 'log2', 100): 44.52871744421699, (5, 'log2', 250): 44.52871744421699, (5, 'log2', 500): 44.52871744421699, (5, None, 10): 44.52871744421699, (5, None, 25): 44.52871744421699, (5, None, 50): 44.52871744421699, (5, None, 100): 44.52871744421699, (5, None, 250): 44.52871744421699, (5, None, 500): 44.52871744421699, (10, 'sqrt', 10): 44.52871744421699, (10, 'sqrt', 25): 44.52871744421699, (10, 'sqrt', 50): 44.52871744421699, (10, 'sqrt', 100): 44.52871744421699, (10, 'sqrt', 250): 44.52871744421699, (10, 'sqrt', 500): 44.52871744421699, (10, 'log2', 10): 44.52871744421699, (10, 'log2', 25): 44.52871744421699, (10, 'log2', 50): 44.52871744421699, (10, 'log2', 100): 44.52871744421699, (10, 'log2', 250): 44.52871744421699, (10, 'log2', 500): 44.52871744421699, (10, None, 10): 44.52871744421699, (10, None, 25): 44.52871744421699, (10, None, 50): 44.52871744421699, (10, None, 100): 44.52871744421699, (10, None, 250): 44.52871744421699, (10, None, 500): 44.52871744421699, (15, 'sqrt', 10): 44.52871744421699, (15, 'sqrt', 25): 44.52871744421699, (15, 'sqrt', 50): 44.52871744421699, (15, 'sqrt', 100): 44.52871744421699, (15, 'sqrt', 250): 44.52871744421699, (15, 'sqrt', 500): 44.52871744421699, (15, 'log2', 10): 44.52871744421699, (15, 'log2', 25): 44.52871744421699, (15, 'log2', 50): 44.52871744421699, (15, 'log2', 100): 44.52871744421699, (15, 'log2', 250): 44.52871744421699, (15, 'log2', 500): 44.52871744421699, (15, None, 10): 44.52871744421699, (15, None, 25): 44.52871744421699, (15, None, 50): 44.52871744421699, (15, None, 100): 44.52871744421699, (15, None, 250): 44.52871744421699, (15, None, 500): 44.52871744421699, (20, 'sqrt', 10): 44.52871744421699, (20, 'sqrt', 25): 44.52871744421699, (20, 'sqrt', 50): 44.52871744421699, (20, 'sqrt', 100): 44.52871744421699, (20, 'sqrt', 250): 44.52871744421699, (20, 'sqrt', 500): 44.52871744421699, (20, 'log2', 10): 44.52871744421699, (20, 'log2', 25): 44.52871744421699, (20, 'log2', 50): 44.52871744421699, (20, 'log2', 100): 44.52871744421699, (20, 'log2', 250): 44.52871744421699, (20, 'log2', 500): 44.52871744421699, (20, None, 10): 44.52871744421699, (20, None, 25): 44.52871744421699, (20, None, 50): 44.52871744421699, (20, None, 100): 44.52871744421699, (20, None, 250): 44.52871744421699, (20, None, 500): 44.52871744421699, (25, 'sqrt', 10): 44.52871744421699, (25, 'sqrt', 25): 44.52871744421699, (25, 'sqrt', 50): 44.52871744421699, (25, 'sqrt', 100): 44.52871744421699, (25, 'sqrt', 250): 44.52871744421699, (25, 'sqrt', 500): 44.52871744421699, (25, 'log2', 10): 44.52871744421699, (25, 'log2', 25): 44.52871744421699, (25, 'log2', 50): 44.52871744421699, (25, 'log2', 100): 44.52871744421699, (25, 'log2', 250): 44.52871744421699, (25, 'log2', 500): 44.52871744421699, (25, None, 10): 44.52871744421699, (25, None, 25): 44.52871744421699, (25, None, 50): 44.52871744421699, (25, None, 100): 44.52871744421699, (25, None, 250): 44.52871744421699, (25, None, 500): 44.52871744421699}, 'lasso_regressor': {1e-05: 16.83202289378715, 0.0001: 16.83202352839266, 0.001: 16.832032266456707, 0.01: 16.832328786388697, 0.1: 16.854341173885572, 1: 17.289730368363184}, 'ridge_regressor': {1: 16.83202003401634, 2: 16.832017926913906, 5: 16.83201570290947, 10: 16.832025465196395, 25: 16.832152268689242, 50: 16.832667264558555, 100: 16.834694381868978, 200: 16.84187131461211, 500: 16.877941032922738, 1000: 16.958886443914185}}\n"
     ]
    }
   ],
   "source": [
    "print(zero_D_tuningd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of storms in the local training set: {}'.format( len(set(data_train['stormid'])) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of time steps in the local training set: {}'.format(y_train.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 0D features from track data\n",
    "\n",
    "A set of simple features has been extracted for each storm at each time point: \n",
    "\n",
    "- latitude, longitude: in degrees\n",
    "- windspeed: current (max) windspeed (knots) \n",
    "- hemisphere:  South=0, North=1\n",
    "- Jday predictor:  Gaussian function of (Julian day of storm init - peak day of the hurricane season), see (1)\n",
    "- initial_max_wind: initial (max) windspeed of the storm \n",
    "- max_wind_change_12h: last 12h (max) windspeed change\n",
    "- basin = based on the present location: \n",
    "       0 = NA - North Atlantic / 1 = SA - South Atlantic    / 2 = WP - West Pacific       / 3 = EP - East Pacific /\n",
    "       4 = SP - South Pacific  / 5 = NI - North Indian      / 6 = SI - South Indian       / 7 = AS - Arabian Sea /\n",
    "       8 = BB - Bay of Bengal  / 9 = EA - Eastern Australia / 10 = WA - Western Australia / 11 = CP - Central Pacific\n",
    "       12 = CS - Carribbean Sea/ 13 = GM - Gulf of Mexico   / 14 = MM - Missing\n",
    "- nature = nature of the storm  \n",
    "       0 = TS - Tropical / 1 = SS - Subtropical / 2 = ET - Extratropical / 3 = DS - Disturbance /\n",
    "       4 = MX - Mix of conflicting reports / 5 = NR - Not Reported / 6 = MM - Missing / 7 =  - Missing\n",
    "- dist2land = current distance to the land (km)\n",
    "\n",
    "\n",
    "(1) DeMaria, Mark, et al. \"Further improvements to the statistical hurricane intensity prediction scheme (SHIPS).\" Weather and Forecasting 20.4 (2005): 531-543. https://journals.ametsoc.org/doi/full/10.1175/WAF862.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15777, 28)\n",
      "(15777,)\n"
     ]
    }
   ],
   "source": [
    "# Simple Regression With Only 0D Features:\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(test.shape)\n",
    "print(y_true.shape)\n",
    "# Data Exploration: There are now 11832 training examples and 3945 test examples\n",
    "X_train, X_test, y_train, y_test = train_test_split(test, y_true)\n",
    "\n",
    "#X_train = X_train.loc[:,'latitude':'dist2land']\n",
    "#X_test = X_test.loc[:,'latitude':'dist2land']\n",
    "#X_train = preprocessing.scale(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([    1,     2,     3, ..., 15771, 15774, 15775]), array([    0,     4,     6, ..., 15772, 15773, 15776]))\n",
      "(array([    0,     4,     6, ..., 15772, 15773, 15776]), array([    1,     2,     3, ..., 15771, 15774, 15775]))\n",
      "RandomizedSearchCV(cv=5, error_score='raise',\n",
      "          estimator=Pipeline(memory=None,\n",
      "     steps=[('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('regressor', Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=None,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False))]),\n",
      "          fit_params=None, iid=True, n_iter=2, n_jobs=None,\n",
      "          param_distributions={'regressor__alpha': [1, 10]},\n",
      "          pre_dispatch='2*n_jobs', random_state=1, refit=True,\n",
      "          return_train_score='warn', scoring='neg_mean_squared_error',\n",
      "          verbose=0)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-8bd560a4a373>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrbf_regressor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_distributions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrid_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'neg_mean_squared_error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;31m#print(grid.best_score_)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m#grid.best_params_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    638\u001b[0m                                   error_score=self.error_score)\n\u001b[1;32m    639\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[0;32m--> 640\u001b[0;31m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \"\"\"\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    211\u001b[0m                 Xt, fitted_transformer = fit_transform_one_cached(\n\u001b[1;32m    212\u001b[0m                     \u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                     **fit_params_steps[name])\n\u001b[0m\u001b[1;32m    214\u001b[0m                 \u001b[0;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;31m# transformer. This is necessary when loading the transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, weight, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    579\u001b[0m                        **fit_params):\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/imputation.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    164\u001b[0m                                                    \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m                                                    \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissing_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m                                                    self.axis)\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/imputation.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[0;34m(self, X, strategy, missing_values, axis)\u001b[0m\n\u001b[1;32m    271\u001b[0m                 masked_X.mask = np.logical_or(masked_X.mask,\n\u001b[1;32m    272\u001b[0m                                               np.isnan(X))\n\u001b[0;32m--> 273\u001b[0;31m             \u001b[0mmedian_masked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasked_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m             \u001b[0;31m# Avoid the warning \"Warning: converting a masked element to nan.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0mmedian\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmedian_masked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/ma/extras.py\u001b[0m in \u001b[0;36mmedian\u001b[0;34m(a, axis, out, overwrite_input, keepdims)\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m     r, k = _ureduce(a, func=_median, axis=axis, out=out,\n\u001b[0;32m--> 694\u001b[0;31m                     overwrite_input=overwrite_input)\n\u001b[0m\u001b[1;32m    695\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_ureduce\u001b[0;34m(a, func, **kwargs)\u001b[0m\n\u001b[1;32m   3248\u001b[0m         \u001b[0mkeepdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3250\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3251\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/ma/extras.py\u001b[0m in \u001b[0;36m_median\u001b[0;34m(a, axis, out, overwrite_input)\u001b[0m\n\u001b[1;32m    713\u001b[0m             \u001b[0masorted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m         \u001b[0masorted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/ma/core.py\u001b[0m in \u001b[0;36msort\u001b[0;34m(a, axis, kind, order, endwith, fill_value)\u001b[0m\n\u001b[1;32m   6710\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6711\u001b[0m         a.sort(axis=axis, kind=kind, order=order,\n\u001b[0;32m-> 6712\u001b[0;31m                endwith=endwith, fill_value=fill_value)\n\u001b[0m\u001b[1;32m   6713\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6714\u001b[0m         \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/ma/core.py\u001b[0m in \u001b[0;36msort\u001b[0;34m(self, axis, kind, order, endwith, fill_value)\u001b[0m\n\u001b[1;32m   5560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5561\u001b[0m         sidx = self.argsort(axis=axis, kind=kind, order=order,\n\u001b[0;32m-> 5562\u001b[0;31m                             fill_value=fill_value, endwith=endwith)\n\u001b[0m\u001b[1;32m   5563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5564\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake_along_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msidx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/ma/core.py\u001b[0m in \u001b[0;36margsort\u001b[0;34m(self, axis, kind, order, endwith, fill_value)\u001b[0m\n\u001b[1;32m   5407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5408\u001b[0m         \u001b[0mfilled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5409\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfilled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5411\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0margmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# SVR with RBF Kernel:\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "rbf_regressor = Pipeline([('scale', StandardScaler()),\n",
    "            ('imputer', Imputer(strategy='median')),\n",
    "            ('regressor', Lasso())])\n",
    "\n",
    "'''\n",
    "rbf_regressor.fit(X_train,y_train)\n",
    "y_pred = rbf_regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(mse)\n",
    "'''\n",
    "\n",
    "\n",
    "#grid_list = {\"regressor__C\": np.logspace(-3, 3, num=7), \"regressor__gamma\": np.logspace(-5, 2, num=8)}\n",
    "grid_list = { \n",
    "    'regressor__alpha': [1, 10]\n",
    "}\n",
    "folds = get_cv(data_train, y_true)\n",
    "for f in folds:\n",
    "    print(f)\n",
    "grid = RandomizedSearchCV(estimator=rbf_regressor, param_distributions=grid_list, n_jobs = None, random_state = 1, n_iter=2, scoring = 'neg_mean_squared_error', cv = 5)\n",
    "print(grid)\n",
    "grid.fit(tmp,y_true)\n",
    "#print(grid.best_score_)\n",
    "#grid.best_params_\n",
    "\n",
    "'''\n",
    "param_tuning['rbf_regressor'] = (grid.best_score_,grid.best_params_)\n",
    "np.save('storm_forecast_parameter_tuning.npy', param_tuning)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "poly_regressor = Pipeline([('scale', StandardScaler()),\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('regressor', SVR('poly'))])\n",
    "\n",
    "poly_regressor.fit(X_train,y_train)\n",
    "y_pred = poly_regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(mse)\n",
    "\n",
    "\n",
    "grid_list = {\"regressor__C\": np.logspace(-3, 3, num=7), \"regressor__gamma\": np.logspace(-5, 2, num=8)}\n",
    "grid = RandomizedSearchCV(estimator=poly_regressor, param_distributions=grid_list, n_jobs = -1, random_state = 1, n_iter=50, scoring = 'neg_mean_squared_error', cv = 5)\n",
    "grid.fit(X_train,y_train)\n",
    "print(grid.best_score_)\n",
    "grid.best_params_\n",
    "\n",
    "param_tuning['poly_regressor'] = (grid.best_score_,grid.best_params_)\n",
    "np.save('storm_forecast_parameter_tuning.npy', param_tuning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264.26308266441345\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [15777, 3945]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-bda75d46fe3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtree_regressor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_distributions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrid_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'neg_mean_squared_error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_cv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m#param_tuning['tree_regressor'] = (grid.best_score_,grid.best_params_)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mrefit_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'score'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 616\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    617\u001b[0m         \u001b[0mn_splits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_n_splits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m         \u001b[0;31m# Regenerate parameter iterable for each fit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 204\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [15777, 3945]"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "tree_regressor = Pipeline([('scale', StandardScaler()),\n",
    "            ('imputer', Imputer(strategy='median')),\n",
    "            ('regressor', DecisionTreeRegressor())])\n",
    "\n",
    "tree_regressor.fit(X_train,y_train)\n",
    "y_pred = tree_regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(mse)\n",
    "\n",
    "\n",
    "grid_list = {\"regressor__max_depth\": [None,5,10,15,20,25],'regressor__max_features': [None, 'sqrt', 'log2']}\n",
    "grid = RandomizedSearchCV(estimator=tree_regressor, param_distributions=grid_list, n_jobs = -1, random_state = 1, n_iter=10, scoring = 'neg_mean_squared_error', cv = get_cv(test,y_true))\n",
    "\n",
    "grid.fit(tmp,y_pred)\n",
    "\n",
    "#param_tuning['tree_regressor'] = (grid.best_score_,grid.best_params_)\n",
    "#np.save('storm_forecast_parameter_tuning.npy', param_tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "forest_regressor = Pipeline([('scale', StandardScaler()),\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('regressor', RandomForestRegressor())])\n",
    "\n",
    "forest_regressor.fit(X_train,y_train)\n",
    "y_pred = forest_regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(mse)\n",
    "\n",
    "\n",
    "grid_list = { \n",
    "    'regressor__n_estimators': [10,25,50,100,250,500,1000],\n",
    "    'regressor__max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'regressor__max_depth' : [None,5,10,15,20,25,50,75,100],\n",
    "}\n",
    "grid = RandomizedSearchCV(estimator=forest_regressor, param_distributions=grid_list, n_jobs = -1, random_state = 1, n_iter=50, scoring = 'neg_mean_squared_error', cv = 5)\n",
    "grid.fit(X_train,y_train)\n",
    "\n",
    "param_tuning['forest_regressor'] = (grid.best_score_,grid.best_params_)\n",
    "np.save('storm_forecast_parameter_tuning_basin+nature.npy', param_tuning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(param_tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "boost_regressor = Pipeline([('scale', StandardScaler()),\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('regressor', GradientBoostingRegressor())])\n",
    "\n",
    "boost_regressor.fit(X_train,y_train)\n",
    "y_pred = boost_regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(mse)\n",
    "\n",
    "grid_list = { \n",
    "    'regressor__n_estimators': [10,25,50,100,250,500,1000],\n",
    "    'regressor__max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'regressor__max_depth' : [None,3,5,10,15,20,25,50,75,100],\n",
    "}\n",
    "grid = RandomizedSearchCV(estimator=forest_regressor, param_distributions=grid_list, n_jobs = -1, random_state = 1, n_iter=50, scoring = 'neg_mean_squared_error', cv = 5)\n",
    "grid.fit(X_train,y_train)\n",
    "\n",
    "param_tuning['boost_regressor'] = (grid.best_score_,grid.best_params_)\n",
    "np.save('storm_forecast_parameter_tuning_basin+nature.npy', param_tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "elastic_regressor = Pipeline([('scale', StandardScaler()),\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('regressor', ElasticNet())])\n",
    "\n",
    "elastic_regressor.fit(X_train,y_train)\n",
    "y_pred = elasticregressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "[1, 50, 100, 200, 1000]\n",
    "\n",
    "ridge_regressor = Pipeline([('scale', StandardScaler()),\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('regressor', Ridge())])\n",
    "\n",
    "ridge_regressor.fit(X_train,y_train)\n",
    "y_pred = ridge_regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(mse)\n",
    "\n",
    "grid_list = { \n",
    "    'regressor__alpha': [1, 2, 5, 10, 25, 50, 100, 200, 500, 1000]\n",
    "}\n",
    "grid = RandomizedSearchCV(estimator=ridge_regressor, param_distributions=grid_list, n_jobs = -1, random_state = 1, n_iter=50, scoring = 'neg_mean_squared_error', cv = 5)\n",
    "grid.fit(X_train,y_train)\n",
    "\n",
    "param_tuning['ridge_regressor'] = (grid.best_score_,grid.best_params_)\n",
    "np.save('storm_forecast_parameter_tuning.npy', param_tuning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "lasso_regressor = Pipeline([('scale', StandardScaler()),\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('regressor', Lasso())])\n",
    "\n",
    "lasso_regressor.fit(X_train,y_train)\n",
    "y_pred = lasso_regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(mse)\n",
    "\n",
    "grid_list = { \n",
    "    'regressor__alpha': [.00001, .0001, .001, .01, .1, 1]\n",
    "}\n",
    "grid = RandomizedSearchCV(estimator=lasso_regressor, param_distributions=grid_list, n_jobs = -1, random_state = 1, n_iter=50, scoring = 'neg_mean_squared_error', cv = 5)\n",
    "grid.fit(X_train,y_train)\n",
    "\n",
    "param_tuning['lasso_regressor'] = (grid.best_score_,grid.best_params_)\n",
    "np.save('storm_forecast_parameter_tuning.npy', param_tuning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "regressor = Pipeline([('scale', StandardScaler()),\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('regressor', RandomForestRegressor())])\n",
    "\n",
    "regressor.fit(X_train,y_train)\n",
    "y_pred = regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(mse)\n",
    "\n",
    "grid_list = { \n",
    "    'n_estimators': [50, 100, 200, 300, 400, 500],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth' : [5,10,20],\n",
    "    'criterion' :['gini', 'entropy']\n",
    "}\n",
    "\n",
    "grid = RandomizedSearchCV(estimator=regressor, param_distributions=grid_list, n_jobs = -1, random_state = 1, n_iter=100, scoring = 'neg_mean_squared_error', cv = 5)\n",
    "print(grid)\n",
    "grid.fit(X_train,y_train)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "grid_list = {\"C\": [1,2,3],\n",
    "             \"gamma\": [.1,.2,.3]}\n",
    "\n",
    "regressor = Pipeline([('scale', StandardScaler()),\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('regressor', SVR(kernel = 'rbf', gamma = .1, C = 1))])\n",
    "regressor.fit(X_train,y_train)\n",
    "y_pred = regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(mse)\n",
    "\n",
    "grid_list = {\"regressor__C\": np.logspace(-2, 2, num=5), \"regressor__gamma\": np.logspace(-4, 0, num=5)}\n",
    "\n",
    "grid = RandomizedSearchCV(estimator=regressor, param_distributions=grid_list, n_jobs = -1, random_state = 1, n_iter=100, scoring = 'neg_mean_squared_error', cv = 5)\n",
    "print(grid)\n",
    "grid.fit(X_train,y_train)\n",
    "print(grid.best_score_)\n",
    "\n",
    "#print(grid.best_estimator_)\n",
    "#print(grid.best_estimator_.regressor__gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. The reanalysis data\n",
    "\n",
    "At each time step, we extracted 7 grids (11x11 pixels) of meteorological parameters centered on the current storm location. Their choice is based on the forecast literature, on personal experience and on known hypothesis of storm strengthening.\n",
    "\n",
    "#### a) 25x25 degree z, u and v at 700hPa-level\n",
    "First, we provide 3 maps of 25 x 25 degrees (lat/long) at 700hPa-level pressure: the altitude `z`, the u-wind `u` (positive if wind from the West) and the v-wind `v` (positive if wind from the South). These grids are subsampled to 11x11 pixels (1 pixel ~=2 degrees).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "sample_id=20 # sample number plotted - you can change it to see other storms and other instants\n",
    "grid_l=11 # size of all 2D-grids (in pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_25x25=['z','u','v']\n",
    "plt.figure(figsize=(10,4))\n",
    "for p,param in enumerate(params_25x25):\n",
    "    image=np.zeros([grid_l,grid_l])\n",
    "    for i in range(grid_l):\n",
    "         for j in range(grid_l):\n",
    "            image[i,j]=data_train[param+'_'+str(i)+'_'+str(j)][sample_id]\n",
    "    plt.subplot(1,3,p+1)\n",
    "    plt.imshow(np.array(image),extent=[-12,12,-12,12],\n",
    "               interpolation='nearest', origin='lower', cmap='seismic')\n",
    "    plt.xlabel('param '+param)\n",
    "t=plt.suptitle('Example of 700-hPa level maps 25x25 degrees, centered in the storm location.'\n",
    "         +'\\n (altitude, u-wind and v-wind)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) 11x11 degree sst, slp, humidity at 1000hPa,  and vorticity at 700hPa\n",
    "We provide some more localized maps of 11 x 11 degrees (lat/long) at the surface: the sea surface temperature `sst`, surface level pressure `slp`, the relative humidity `hum` at 1000hPa (near surface). These grids are sampled to 11x11 pixels (1 pixel = 1 degree). We also provide the vorticity at 700hPa `vo700`. \n",
    "\n",
    "NB: `sst` is only defined on the sea, so land has NaNs values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_11x11=['sst','slp','hum','vo700']\n",
    "plt.figure(figsize=(10,3))\n",
    "for p,param in enumerate(params_11x11):\n",
    "    image=np.zeros([grid_l,grid_l])\n",
    "    for i in range(grid_l):\n",
    "         for j in range(grid_l):\n",
    "            image[i,j]=data_train[param+'_'+str(i)+'_'+str(j)][sample_id]\n",
    "    plt.subplot(1,4,p+1)\n",
    "    plt.imshow(np.array(image),extent=[-5,5,-5,5],\n",
    "               interpolation='nearest', origin='lower', cmap='seismic')\n",
    "    plt.xlabel('param '+param)\n",
    "t=plt.suptitle('Example of 11x11 degrees maps, centered in the storm location.'\n",
    "         +'\\n (surf. temp., surf. pressure, 1000hPa humidity and 700hPa vorticity)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/sophiegif/ramp_kit_storm_forecast_new/blob/master/figures_pynb/pipeline.png?raw=true\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For submitting at the [RAMP site](http://ramp.studio), you will have to write two classes, saved in a specific file:   \n",
    "\n",
    "* a class `FeatureExtractor` in a `feature_extractor.py` file.\n",
    "* a class `Regressor` in a `regressor.py` file.\n",
    "\n",
    "You can look at the simple examples provided in /submissions:\n",
    "- starting_kit : using only the track data\n",
    "- starting_kit_pressure_map : using both track data and image data\n",
    "\n",
    "### Using data from previous time steps\n",
    "Of course, you can use the data from previous time steps, e.g., for the prediction of the intensity of storm S at t=3 you can use data from S at t=\\[0:2\\]. However, it is completely forbidden (and we check it!) to use future data like S at t=4,.. This is illustrated in the figure below, where the estimation of the 24h-forecast of the time instant 2 (red line) can use blue but not red features.\n",
    "\n",
    "- `illegal_lookahead`: this simple submission illustrates the error you will have if you are illegally looking ahead time of the same storm.\n",
    "- `legal_lookbefore` : this simple submission illustrates how to use information from previous time steps of the same storm.\n",
    "\n",
    "<img src=\"https://github.com/sophiegif/ramp_kit_storm_forecast_new/blob/master/figures_pynb/illegal_lookahead.png?raw=true\" width=\"70%\">\n",
    "<div style=\"text-align: center\">Data from previous steps are allowed, but data from future steps are forbidden.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "The framework is evaluated with a cross-validation approach. The metric used is the RMSE (root mean square error) in knots across all storm time instants. We also made visible three other metrics: `mae` is the mean absolute error, in knots. `mae_hurr` is the MAE using only time instants corresponding to hurricanes (windspeed>64 knots), while `rel_mae_hurr` is the relative RMSE on hurricanes. These metrics are interesting because the current forecasting practice is to exclude all other stages of development (e.g., extratropical, tropical wave...), see [this page](https://www.nhc.noaa.gov/verification/verify5.shtml?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the submission\n",
    "You can test locally our pipeline using `ramp_test_submission` command line (`-h` will give you all infos). For that, open a terminal in your `storm_forecast/` folder and type on a terminal `ramp_test_submission --submission starting_kit`. You can then copy a submission example in `submissions/<YOUR_SUBMISSION_NAME>/`and modify its codes as you want. Finally, test it on your computer with `ramp_test_submission --submission <YOUR_SUBMISSION_NAME>`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get to see the train and test scores, and no errors, then you can submit your model to the ramp.studio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some warnings when building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "\n",
    " <ul>\n",
    "  <li>If you want to use the features from previous time steps in your learning (for example using LSTMs), you will have to use the 'stormid' and the 'instant_t' columns. Moreover, you will have to handle separetly the first time steps, which are not provided with past data.</li>\n",
    "  <li>The intensity value to predict is the max windspeed. However, this value was measured empirically with a precision of ~5knots. </li>\n",
    "</ul> \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting to the online challenge: ramp.studio\n",
    "\n",
    "Once you have found a good model, you can submit them to [ramp.studio](http://www.ramp.studio) to enter the online challenge. First, if it is your first time using the RAMP platform, [sign up](http://www.ramp.studio/sign_up), otherwise [log in](http://www.ramp.studio/login). Then sign up to the event [storm_forecast_CI2018](http://www.ramp.studio/events/storm_forecast_CI2018). Sign up for the event. Both signups are controled by RAMP administrators, so there **can be a delay between asking for signup and being able to submit**.\n",
    "\n",
    "Once your signup request is accepted, you can go to your [sandbox](http://www.ramp.studio/events/storm_forecast_CI2018/sandbox) and copy-paste (or upload) [`feature_extractor.py`](/edit/submissions/starting_kit/feature_extractor.py) and [`classifier.py`](/edit/submissions/starting_kit/classifier.py). Save it, rename it, then submit it. The submission is trained and tested on our backend in the similar way as `ramp_test_submission` does it locally. While your submission is waiting in the queue and being trained, you can find it in the \"New submissions (pending training)\" table in [my submissions](http://www.ramp.studio/events/storm_forecast_CI2018/my_submissions). Once it is trained your submission shows up on the [public leaderboard](http://www.ramp.studio/events/storm_forecast_CI2018/leaderboard). \n",
    "If there is an error (despite having tested your submission locally with `ramp_test_submission`), it will show up in the \"Failed submissions\" table in [my submissions](http://www.ramp.studio/events/storm_forecast_CI2018/my_submissions). You can click on the error to see part of the trace.\n",
    "\n",
    "After submission, do not forget to give credit to the previous submissions you reused or integrated into your submission.\n",
    "\n",
    "The data set we use on the backend is usually different from what you find in the starting kit, so the score may be different.\n",
    "\n",
    "The official score in this RAMP (the first score column after \"historical contributivity\" on the [leaderboard](http://www.ramp.studio/events/storm_forecast_CI2018/leaderboard)) is the RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More information\n",
    "\n",
    "You can find more information in the [README](https://github.com/paris-saclay-cds/ramp-workflow/blob/master/README.md) of the [ramp-workflow library](https://github.com/paris-saclay-cds/ramp-workflow)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contact\n",
    "\n",
    "Don't hesitate to [contact us](mailto:admin@ramp.studio?subject=Storm forecast CI2018 ramp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homemade Forest Parameter Tuning. Attempt at Multiprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from multiprocessing import Pool\n",
    "import itertools\n",
    "\n",
    "\n",
    "def forest_search(max_depth, max_features, n_estimators):\n",
    "    \n",
    "    cart_prod = list(itertools.product(max_depth,max_features,n_estimators))\n",
    "    n_splits = 5\n",
    "    forest_dict = {}\n",
    "\n",
    "\n",
    "    for param_combo in cart_prod:\n",
    "        mse_sum = 0\n",
    "        for train_index, test_index in get_cv(all_features,y_true,n_splits):\n",
    "            forest_regressor = Pipeline([('imputer', Imputer(strategy='median')), \n",
    "                ('scale', StandardScaler()),       \n",
    "                ('regressor', RandomForestRegressor(max_depth = param_combo[0], max_features = param_combo[1], n_estimators = param_combo[2]))])\n",
    "            drop = all_features.drop(columns = ['stormid'])\n",
    "\n",
    "            X_train, X_test = drop.iloc[train_index,:], drop.iloc[test_index,:]\n",
    "            y_train, y_test = y_true[train_index], y_true[test_index]\n",
    "\n",
    "            forest_regressor.fit(X_train,y_train)\n",
    "            y_pred = forest_regressor.predict(X_test)\n",
    "            mse_sum += mean_squared_error(y_test, y_pred)\n",
    "\n",
    "        forest_dict[param_combo] = np.sqrt(mse_sum/n_splits)\n",
    "        print('Elements in Dict:', len(forest_dict))\n",
    "    return forest_dict\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    p = Pool(6)\n",
    "\n",
    "    max_depth = [5,10,15,20,25]\n",
    "    max_features = ['sqrt', 'log2', None]\n",
    "    n_estimators = [10,25,50,100,250,500]\n",
    "\n",
    "    print(type(max_depth))\n",
    "    forest_dict = p.starmap(forest_search, zip([5,10,15,20,25],['sqrt', 'log2', None], [10,25,50,100,250,500]))\n",
    "    p.close()\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "class Regressor(BaseEstimator):\n",
    "    def __init__(self, kernel = 'rbf', gamma = .1, C = 1):\n",
    "        self.reg = Pipeline([('scale', StandardScaler()),\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('regressor', SVR(kernel = kernel, gamma = gamma, C = C))                 \n",
    "        ])\n",
    "        self.kernel = kernel\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.reg.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        print(\"Kernel:\", self.kernel)\n",
    "        return self.reg.predict(X)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
