{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from umap import UMAP\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from sklearn.preprocessing import Imputer, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools as it\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel(fn, params, n_jobs=mp.cpu_count()):\n",
    "    pool = mp.Pool(n_jobs)\n",
    "    print('started new process pool with {} processes'.format(n_jobs))\n",
    "    try:\n",
    "        res = pool.map(fn, params)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "    except:\n",
    "        print('process pool interrupted, shutting down')\n",
    "        pool.terminate()\n",
    "        pool.join()\n",
    "        raise\n",
    "    return res\n",
    "\n",
    "mplog = mp.get_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utilities ##\n",
    "\n",
    "from typing import Dict\n",
    "from typing import Tuple\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.utils.graph_shortest_path import graph_shortest_path\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "def group_dict(iterable, keyfn, mapfn):\n",
    "    \"\"\"\n",
    "    Groups the iterable using the given key function and returns a dictionary\n",
    "    of keys to groups.\n",
    "    \"\"\"\n",
    "    groups = it.groupby(iterable, key=keyfn)\n",
    "    gdict = dict()\n",
    "    for k, g in groups:\n",
    "        gdict[k] = list(map(mapfn, g))\n",
    "    return gdict\n",
    "\n",
    "def stormid_dict(X_df: pd.DataFrame) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Partitions the storm forecast dataset into separate groups for each storm and\n",
    "    returns the result as a dictionary.\n",
    "    \"\"\"\n",
    "    groups = X_df.groupby(['stormid'])\n",
    "    storm_dict = dict()\n",
    "    for stormid, df in groups:\n",
    "        storm_dict[stormid] = df\n",
    "    return storm_dict\n",
    "\n",
    "def feature_groups(X_df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Partitions X_df into three groups by columns:\n",
    "    1) 0-D features\n",
    "    2) 11x11 z, u, v wind reanalysis data\n",
    "    3) 11x11 sst, slp, humidity, and vorticity reanalysis data\n",
    "    \"\"\"\n",
    "    feat_cols = X_df.get(['instant_t', 'windspeed', 'latitude', 'longitude','hemisphere','Jday_predictor','initial_max_wind','max_wind_change_12h','dist2land'])\n",
    "    nature_cols = pd.get_dummies(X_df.nature, prefix='nature', drop_first=False)\n",
    "    basin_cols = pd.get_dummies(X_df.basin, prefix='basin', drop_first=False)\n",
    "    X_0D = pd.concat([feat_cols, nature_cols, basin_cols], axis=1, sort=False)\n",
    "    X_zuv = X_df.get([col for col in X_df.columns if col.startswith('z_') or col.startswith('u_') or col.startswith('v_')])\n",
    "    X_sshv = X_df.get([col for col in X_df.columns if col.startswith('sst') or col.startswith('slp')\n",
    "                   or col.startswith('hum') or col.startswith('vo700')])\n",
    "    return X_0D, X_zuv, X_sshv\n",
    "\n",
    "def trust_cont_score(X, X_map, k=10, alpha=0.5, impute_strategy='median'):\n",
    "    \"\"\"\n",
    "    Computes the \"trustworthiness\" and \"continuity\" [1] of X_map with respect to X.\n",
    "    This is a port and extension of the implementation provided by Van der Maaten [2].\n",
    "    \n",
    "    Parameters:\n",
    "    X     : the data in its original representation\n",
    "    X_map : the lower dimensional representation of the data to be evaluated\n",
    "    k     : parameter that determines the size of the neighborhood for the T&C measure\n",
    "    alpha : mixing parameter in [0,1] that determines the weight given to trustworthiness vs. continuity; higher values will give more\n",
    "            weight to trustworthiness, lower values to continuity.\n",
    "    \n",
    "    [1] Kaski S, Nikkilä J, Oja M, Venna J, Törönen P, Castrén E. Trustworthiness and metrics in visualizing similarity of gene expression. BMC bioinformatics. 2003 Dec;4(1):48.\n",
    "    [2] Maaten L. Learning a parametric embedding by preserving local structure. InArtificial Intelligence and Statistics 2009 Apr 15 (pp. 384-391).\n",
    "    \"\"\"\n",
    "    # Impute X values\n",
    "    X = Imputer(strategy=impute_strategy).fit_transform(X)\n",
    "    # Compute pairwise distance matrices\n",
    "    D_h = pairwise_distances(X, X, metric='euclidean')\n",
    "    D_l = pairwise_distances(X_map, X_map, metric='euclidean')\n",
    "    # Compute neighborhood indices\n",
    "    ind_h = np.argsort(D_h, axis=1)\n",
    "    ind_l = np.argsort(D_l, axis=1)\n",
    "    # Compute trustworthiness\n",
    "    N = X.shape[0]\n",
    "    T = 0\n",
    "    C = 0\n",
    "    t_ranks = np.zeros((k, 1))\n",
    "    c_ranks = np.zeros((k, 1))\n",
    "    for i in range(N):\n",
    "        for j in range(k):\n",
    "            t_ranks[j] = np.where(ind_h[i,:] == ind_l[i, j+1])\n",
    "            c_ranks[j] = np.where(ind_l[i,:] == ind_h[i, j+1])\n",
    "        t_ranks -= k\n",
    "        c_ranks -= k\n",
    "        T += np.sum(t_ranks[np.where(t_ranks > 0)])\n",
    "        C += np.sum(c_ranks[np.where(c_ranks > 0)])\n",
    "    S = (2 / (N * k * (2 * N - 3 * k - 1)))\n",
    "    T = 1.0 - S*T\n",
    "    C = 1.0 - S*C\n",
    "    return alpha*T + (1.0-alpha)*C\n",
    "\n",
    "def sammon_stress(X, X_m, impute_strategy='median'):\n",
    "    X = Imputer(strategy=impute_strategy).fit_transform(X)\n",
    "    Dx = pairwise_distances(X, X, metric='euclidean')\n",
    "    Dy = pairwise_distances(X_m, X_m, metric='euclidean')\n",
    "    # Sammon Stress computes sums over indices where i < j\n",
    "    # We can interpet this as being the upper triangle of each matrix, from the k=1 diagonal\n",
    "    Dx_ut = np.triu(Dx, k=1)\n",
    "    Dy_ut = np.triu(Dy, k=1)\n",
    "    # Compute Sammon Stress, S\n",
    "    S = (1 / np.sum(Dx_ut))*np.sum(np.square(Dx_ut - Dy_ut) / (Dx_ut + np.ones(Dx.shape)))\n",
    "    return S\n",
    "    \n",
    "    \n",
    "def residual_variance(X, X_m, n_neighbors=20):\n",
    "    kng_h = kneighbors_graph(X, n_neighbors=n_neighbors, mode='distance', n_jobs=mp.cpu_count()).toarray()\n",
    "    D_h = graph_shortest_path(kng_h, method='D', directed=False)\n",
    "    #D_h = pairwise_distances(X, X, metric='euclidean')\n",
    "    #D_l = kneighbors_graph(X_m, n_neighbors=50, mode='distance').toarray()\n",
    "    D_l = pairwise_distances(X_m, X_m, metric='euclidean')\n",
    "    r,_ = spearmanr(D_h.flatten(), D_l.flatten())\n",
    "    return 1 - r**2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(X_df, n_components=2):\n",
    "    imputer = Imputer(strategy='median')\n",
    "    scaler = StandardScaler()\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca_pipeline = Pipeline([('med_imputer', imputer),('scaler', scaler),('pca',pca)])\n",
    "    X_pc = pca_pipeline.fit_transform(X_df)    \n",
    "    return X_pc\n",
    "\n",
    "def rand_projection(X_df, n_components='auto', eps=0.1):\n",
    "    imputer = Imputer(strategy='median')\n",
    "    scaler = StandardScaler()\n",
    "    proj = SparseRandomProjection(n_components=n_components, eps=eps)\n",
    "    proj_pipeline = Pipeline([('med_imputer', imputer),('scaler', scaler),('proj', proj)])\n",
    "    X_rp = proj_pipeline.fit_transform(X_df)\n",
    "    return X_rp, proj.n_components_\n",
    "\n",
    "def tsne(X_df, n_components=2, n_iter=5000, perplexity=30, learning_rate=100, init='pca'):\n",
    "    imputer = Imputer(strategy='median')\n",
    "    scaler = StandardScaler()\n",
    "    tsne = TSNE(n_components=n_components, perplexity=perplexity, learning_rate=learning_rate, init=init, n_iter=n_iter)\n",
    "    tsne_pipeline = Pipeline([('med_imputer', imputer),('scaler', scaler),('tsne', tsne)])\n",
    "    X_tsne = tsne_pipeline.fit_transform(X_df)\n",
    "    print(\"t-SNE completed after {} iterations with final KLD: {}\".format(tsne.n_iter_, tsne.kl_divergence_))\n",
    "    return X_tsne\n",
    "\n",
    "def umap(X_df, n_components=2, y=None, n_neighbors=5, min_dist=0.1, metric='correlation'):\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    imputer = Imputer(strategy='median')\n",
    "    scaler = StandardScaler()\n",
    "    umap = UMAP(n_components=n_components, n_neighbors=n_neighbors, min_dist=min_dist, metric=metric)\n",
    "    umap_pipeline = Pipeline([('med_imputer', imputer),('scaler', scaler),('umap', umap)])\n",
    "    X_umap = umap_pipeline.fit_transform(X_df, y)\n",
    "    warnings.resetwarnings()\n",
    "    return X_umap\n",
    "\n",
    "def evaluate(X, X_maps, k=20, top_n=1, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Evaluates a collection of X_maps relative to X using the trustworthiness/continuity metric.\n",
    "    Returns 'top_n' tuples of (X_map, TC score, index)\n",
    "    \n",
    "    X      : the original, untransformed data\n",
    "    X_maps : an iterable of lower dimensional mappings to evaluate\n",
    "    k      : neighborhood parameter for T&C metric\n",
    "    top_n  : # of mappings to return, in descending order of score\n",
    "    \"\"\"\n",
    "    assert top_n <= len(X_maps)\n",
    "    scores = np.array([trust_cont_score(X, X_m, k=k, alpha=alpha) for X_m in X_maps])\n",
    "    sorted_inds = np.argsort(scores)[::-1]\n",
    "    sorted_scores = scores[sorted_inds]\n",
    "    sorted_maps = [X_maps[i] for i in sorted_inds]\n",
    "    return list(zip(sorted_maps, sorted_scores, sorted_inds))[:top_n]\n",
    "\n",
    "def plot_mapping_vs_y_2d(X_map, ys, title=\"\", xlabel=\"\", ylabel=\"\", instant_labels: pd.DataFrame = None):\n",
    "    plt.scatter(X_map[:,0], X_map[:,1], c=ys, cmap='copper')\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.colorbar().set_label(\"Ice area\")\n",
    "    if instant_labels is not None:\n",
    "        for (i, xy) in enumerate(X_map):\n",
    "            if i % 2 == 0:\n",
    "                plt.annotate(instant_labels.values[i], xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ds = xr.open_dataset('data/train.nc')\n",
    "ys = np.load('data/train.npy') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 1500, 39, 58)\n"
     ]
    }
   ],
   "source": [
    "X_arr = X_ds.to_array()\n",
    "print(X_arr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per Variable analysis\n",
    "\n",
    "Produce 2D visualizations for each individual variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rp(params):\n",
    "    X, d = params\n",
    "    return rand_projection(X, n_components=d)[0]\n",
    "\n",
    "def run_tsne(params):\n",
    "    X, d, perp = params\n",
    "    # Reduce dims with PCA\n",
    "    X_pc = pca(X, n_components=50)\n",
    "    return tsne(X_pc, perplexity=perp, init='random')\n",
    "\n",
    "def run_umap(params):\n",
    "    X, d, nn, min_dist = params\n",
    "    return umap(X, n_components=d, n_neighbors=nn, min_dist=min_dist)\n",
    "\n",
    "def run_rp_with_tc_cv(X, d=2, n_runs=24, k=20):\n",
    "    X_rps = parallel(run_rp, [(X, d)]*n_runs)\n",
    "    X_rp, tc, _ = evaluate(X, X_rps, k=k)[0]\n",
    "    return X_rp, tc\n",
    "\n",
    "def run_tsne_with_tc_cv(X, perplexities, d=2, k=20):\n",
    "    params = it.product([X], [d], perplexities)\n",
    "    X_tsnes = parallel(run_tsne, params)\n",
    "    X_tsne, tc, _ = evaluate(X, X_tsnes, k=k)[0]\n",
    "    return X_tsne, tc\n",
    "\n",
    "def run_umap_with_tc_cv(X, n_neighbors, min_dists, d=2, k=20):\n",
    "    params = it.product([X], [d], n_neighbors, min_dists)\n",
    "    X_umaps = parallel(run_umap, params)\n",
    "    X_umap, tc, _ = evaluate(X, X_umaps, k=k)[0]\n",
    "    return X_umap, tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running single variable tests for ice_area\n",
      "started new process pool with 8 processes\n",
      "started new process pool with 8 processes\n",
      "t-SNE completed after 2049 iterations with final KLD: 2.539768934249878\n",
      "t-SNE completed after 1699 iterations with final KLD: 2.2243263721466064\n",
      "t-SNE completed after 1699 iterations with final KLD: 1.983177900314331\n",
      "t-SNE completed after 4999 iterations with final KLD: 2.975492000579834\n",
      "started new process pool with 8 processes\n",
      "Running single variable tests for ts\n",
      "started new process pool with 8 processes\n",
      "started new process pool with 8 processes\n",
      "t-SNE completed after 4999 iterations with final KLD: 2.1451497077941895\n",
      "t-SNE completed after 4999 iterations with final KLD: 2.0397424697875977\n",
      "t-SNE completed after 4999 iterations with final KLD: 1.9198139905929565\n",
      "t-SNE completed after 4349 iterations with final KLD: 1.7984490394592285\n",
      "started new process pool with 8 processes\n",
      "Running single variable tests for taux\n",
      "started new process pool with 8 processes\n",
      "started new process pool with 8 processes\n",
      "t-SNE completed after 4999 iterations with final KLD: 2.3476152420043945\n",
      "t-SNE completed after 2449 iterations with final KLD: 1.9423353672027588\n",
      "t-SNE completed after 4999 iterations with final KLD: 2.1764309406280518\n",
      "t-SNE completed after 4399 iterations with final KLD: 2.1116554737091064\n",
      "started new process pool with 8 processes\n",
      "Running single variable tests for tauy\n",
      "started new process pool with 8 processes\n",
      "started new process pool with 8 processes\n",
      "t-SNE completed after 4999 iterations with final KLD: 2.3294334411621094\n",
      "t-SNE completed after 2599 iterations with final KLD: 2.0345523357391357\n",
      "t-SNE completed after 2349 iterations with final KLD: 1.921363115310669\n",
      "t-SNE completed after 4999 iterations with final KLD: 2.16682767868042\n",
      "started new process pool with 8 processes\n",
      "Running single variable tests for ps\n",
      "started new process pool with 8 processes\n",
      "started new process pool with 8 processes\n",
      "t-SNE completed after 4999 iterations with final KLD: 2.4967236518859863\n",
      "t-SNE completed after 2799 iterations with final KLD: 2.1368656158447266\n",
      "t-SNE completed after 2549 iterations with final KLD: 1.9737087488174438\n",
      "t-SNE completed after 4999 iterations with final KLD: 2.271644353866577\n",
      "started new process pool with 8 processes\n",
      "Running single variable tests for psl\n",
      "started new process pool with 8 processes\n",
      "started new process pool with 8 processes\n",
      "t-SNE completed after 2049 iterations with final KLD: 1.97920823097229\n",
      "t-SNE completed after 2599 iterations with final KLD: 2.1115522384643555\n",
      "t-SNE completed after 4999 iterations with final KLD: 2.4622366428375244\n",
      "t-SNE completed after 4999 iterations with final KLD: 2.2835946083068848\n",
      "started new process pool with 8 processes\n",
      "Running single variable tests for shflx\n",
      "started new process pool with 8 processes\n",
      "started new process pool with 8 processes\n",
      "t-SNE completed after 4999 iterations with final KLD: 1.7714147567749023\n",
      "t-SNE completed after 4999 iterations with final KLD: 1.6900314092636108\n",
      "t-SNE completed after 3049 iterations with final KLD: 1.5110080242156982\n",
      "t-SNE completed after 4849 iterations with final KLD: 1.597451090812683\n",
      "started new process pool with 8 processes\n",
      "Running single variable tests for cldtot\n",
      "started new process pool with 8 processes\n",
      "started new process pool with 8 processes\n"
     ]
    }
   ],
   "source": [
    "data_vars = list(X_ds.data_vars.keys())\n",
    "TC_k = 20\n",
    "tsne_perplexities = range(20, 100, 20)\n",
    "umap_nns = [10, 20, 50]\n",
    "umap_mds = np.arange(0.1, 1.0, 0.2)\n",
    "res_dict = dict()\n",
    "for (i, (var_name, X_var_3d)) in enumerate(zip(data_vars, X_arr.values)):\n",
    "    print('Running single variable tests for {}'.format(var_name))\n",
    "    # Flatten spatial dimensions\n",
    "    n_t, n_lat, n_lon = X_var_3d.shape[0], X_var_3d.shape[1], X_var_3d.shape[2]\n",
    "    X_var = X_var_3d.reshape((n_t, n_lat*n_lon))\n",
    "    assert(X_var.shape == (n_t, n_lat*n_lon))\n",
    "    res = []\n",
    "    # PCA\n",
    "    X_var_pc = pca(X_var, n_components=2)\n",
    "    tc_pc = trust_cont_score(X_var, X_var_pc, k=TC_k)\n",
    "    res.append((X_var_pc, tc_pc, \"PCA\"))\n",
    "    # Random projections\n",
    "    X_var_rp, tc_rp = run_rp_with_tc_cv(X_var, k=TC_k)\n",
    "    res.append((X_var_rp, tc_rp, \"RP\"))\n",
    "    # t-SNE\n",
    "    X_var_tsne, tc_tsne = run_tsne_with_tc_cv(X_var, tsne_perplexities, k=TC_k)\n",
    "    res.append((X_var_tsne, tc_tsne, \"t-SNE\"))\n",
    "    # UMAP\n",
    "    X_var_umap, tc_umap = run_umap_with_tc_cv(X_var, umap_nns, umap_mds, k=TC_k)\n",
    "    res.append((X_var_umap, tc_umap, \"UMAP\"))\n",
    "    # Store results for variable\n",
    "    res_dict[var_name] = res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_plots_per_var = 4\n",
    "plt.figure(figsize=(25,45))\n",
    "for (i, (var_name, maps)) in enumerate(res_dict.items()):\n",
    "    for (j, (X_2d, tc, name)) in enumerate(maps):\n",
    "        plt.subplot(len(data_vars), n_plots_per_var, i*n_plots_per_var + j + 1)\n",
    "        plot_mapping_vs_y_2d(X_2d, ys, \"{}, {} 2D (TC={:.3f})\".format(var_name, name, tc))\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
