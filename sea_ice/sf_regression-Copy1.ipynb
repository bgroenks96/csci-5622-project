{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left\">\n",
    "<img src=\"http://project.inria.fr/saclaycds/files/2017/02/logoUPSayPlusCDS_990.png\" width=\"800px\">\n",
    "</div>\n",
    "\n",
    "# [RAMP](https://www.ramp.studio/problems/storm_forecast_hackathon) on Tropical Storm Intensity Forecast (from reanalysis data)\n",
    "\n",
    "_Sophie Giffard-Roisin (CU/CNRS), Mo Yang (CNRS), Balazs Kegl (CNRS/CDS), Claire Monteleoni (CU/CNRS), Alexandre Boucaud (CNRS/CDS)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Introduction](#Introduction)\n",
    "2. [The prediction task](#The-prediction-task)\n",
    "2. [Installation of libraries](#Installation-of-libraries) : To do before coming!\n",
    "2. [The data](#The-data)\n",
    "3. [The pipeline](#The-pipeline)\n",
    "4. [Evaluation](#Evaluation)\n",
    "5. [Local testing/exploration](#Testing-the-submission)\n",
    "6. [Submission](#Submitting-to-the-online-challenge:-ramp.studio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The goal of the RAMP is to predict the intensity of tropical and extra-tropical storms (24h forecast) using information from past storms since 1979. The intensity can be measured as the maximum sustained wind over a period of one minute at 10 meters height. This speed, calculated every 6 hours, is usually explained in knots (1kt=0.514 m/s) and is used to define the hurricane category from the [Saffir-Simpson scale](https://en.wikipedia.org/wiki/Saffir–Simpson_scale). Estimating the intensity evolution of a storm is of course crucial for the population.\n",
    "\n",
    "<img src=\"https://github.com/sophiegif/ramp_kit_storm_forecast_new/blob/master/figures_pynb/all_storms_since1979_IBTrRACKS_newcats.png?raw=true\" width=\"70%\">\n",
    "<div style=\"text-align: center\">Database: tropical/extra-tropical storm tracks since 1979. Dots = initial position, color = maximal storm strength according to the Saffir-Simpson scale.</div>\n",
    "\n",
    "Today, the forecasts (track and intensity) are provided by a numerous number of guidance models (1). Dynamical models solve the physical equations governing motions in the atmosphere. Statistical models, in contrast, are based on historical relationships between storm behavior and various other parameters. However, the lack of improvement in intensity forecasting is attributed to the complexity of tropical systems and an incomplete understanding of factors that affect their development. What is mainly still hard to predict is the rapid intensification of hurricanes: in 1992, Andrew went from tropical depression to a category 5 hurricane in 24h. \n",
    "\n",
    "Machine learning (and deep learning) methods have been only scarcely tested, and there is hope in that it can improve storm forecasts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The prediction task\n",
    "\n",
    "<ul class=\"list-unstyled list-inline text-center\">\n",
    "  <li>\n",
    "    <img src=\"https://github.com/sophiegif/ramp_kit_storm_forecast_new/blob/master/figures_pynb/storm_shema3.png?raw=true\" alt= \"image1\" width=\"350\" height=\"350\">\n",
    "    <figcaption>Goal: estimate the 24h-forecast intensity of all storms.</figcaption>\n",
    "  </li>\n",
    "  \n",
    "  <li>\n",
    "    <img src=\"https://github.com/sophiegif/ramp_kit_storm_forecast_new/blob/master/figures_pynb/hurricane_pb.png?raw=true\" alt= \"image2\" width=\"350\" height=\"350\">\n",
    "    <figcaption>Feature data: centered maps of wind, altitude, sst, slp, humidity...</figcaption>\n",
    "  </li>\n",
    "</ul>\n",
    "\n",
    "This challenge proposes to design the best algorithm to predict for a large number of storms the 24h-forecast intensity every 6 hours. The (real) database is composed of more than 3000 extra-tropical and tropical storm tracks, and it also provides the intensity and some local physical information at each timestep (2). Moreover, we also provide some 700-hPa and 1000-hPa feature maps of the neighborhood of the storm (from ERA-interm reanalysis database (3)), that can be viewed as images centered on the current storm location (see right image).\n",
    "\n",
    "The goal is to provide for each time step of each storm (total number of instants = 90 000), the predicted 24h-forecast intensity, so 4 time steps in the future. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "\n",
    "1. National Hurricane Center Forecast Verification website, https://www.nhc.noaa.gov/verification/, updated 04 April 2017.\n",
    "\n",
    "2. Knapp, K. R., M. C. Kruk, D. H. Levinson, H. J. Diamond, and C. J. Neumann, 2010: The International Best Track Archive for Climate Stewardship (IBTrACS): Unifying tropical cyclone best track data. Bulletin of the American Meteorological Society, 91, 363-376 https://www.ncdc.noaa.gov/ibtracs/index.php?name=wmo-data\n",
    "\n",
    "3. Dee, D. P. et al.(2011), The ERA-Interim reanalysis: configuration and performance of the data assimilation system. Q.J.R. Meteorol. Soc., 137: 553–597. https://rmets.onlinelibrary.wiley.com/doi/abs/10.1002/qj.828"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation of libraries\n",
    "\n",
    "To get this notebook running and test your models locally using the `ramp_test_submission`, we recommend that you use the Python distribution from [Anaconda](https://www.anaconda.com/download/) or [Miniconda](https://docs.anaconda.com/docs_oss/conda/install/quick#miniconda-quick-install-requirements). (uncomment the lines before running them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install -y -c conda conda-env     # First install conda-env to ease the creation of virtual envs in conda\n",
    "# !conda env create                        # Uses the local environment.yml to create the 'storm_forecast_2' env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OR** if you have Python already installed but are **not using Anaconda**, you'll want to use `pip` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installation of ramp-workflow\n",
    "\n",
    "For being able to test submissions, you also need to have the `ramp-workflow` package locally. You can install the latest version with pip from github:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/paris-saclay-cds/ramp-workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download data (optional)\n",
    "\n",
    "If the data has not yet been downloaded locally, uncomment the following cell and run it.\n",
    "The starting kit data is 260 MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python download_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utilities ##\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict\n",
    "from typing import Tuple\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from sklearn.preprocessing import Imputer, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.utils.graph_shortest_path import graph_shortest_path\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import multiprocessing as mp\n",
    "import itertools\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "def group_dict(iterable, keyfn, mapfn):\n",
    "    \"\"\"\n",
    "    Groups the iterable using the given key function and returns a dictionary\n",
    "    of keys to groups.\n",
    "    \"\"\"\n",
    "    groups = it.groupby(iterable, key=keyfn)\n",
    "    gdict = dict()\n",
    "    for k, g in groups:\n",
    "        gdict[k] = list(map(mapfn, g))\n",
    "    return gdict\n",
    "\n",
    "def stormid_dict(X_df: pd.DataFrame) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Partitions the storm forecast dataset into separate groups for each storm and\n",
    "    returns the result as a dictionary.\n",
    "    \"\"\"\n",
    "    groups = X_df.groupby(['stormid'])\n",
    "    storm_dict = dict()\n",
    "    for stormid, df in groups:\n",
    "        storm_dict[stormid] = df\n",
    "    return storm_dict\n",
    "\n",
    "def feature_groups(X_df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Partitions X_df into three groups by columns:\n",
    "    1) 0-D features\n",
    "    2) 11x11 z, u, v wind reanalysis data\n",
    "    3) 11x11 sst, slp, humidity, and vorticity reanalysis data\n",
    "    4) All features\n",
    "    \"\"\"\n",
    "    feat_cols = X_df.get(['stormid','instant_t', 'windspeed', 'latitude', 'longitude','hemisphere','Jday_predictor','initial_max_wind','max_wind_change_12h','dist2land'])\n",
    "    nature_cols = pd.get_dummies(X_df.nature, prefix='nature', drop_first=False)\n",
    "    basin_cols = pd.get_dummies(X_df.basin, prefix='basin', drop_first=False)\n",
    "    X_0D = pd.concat([feat_cols, nature_cols, basin_cols], axis=1, sort=False)\n",
    "    X_zuv = X_df.get([col for col in X_df.columns if col.startswith('z_') or col.startswith('u_') or col.startswith('v_')])\n",
    "    X_sshv = X_df.get([col for col in X_df.columns if col.startswith('sst') or col.startswith('slp')\n",
    "                   or col.startswith('hum') or col.startswith('vo700')])\n",
    "    X_all = pd.concat([X_0D, X_zuv, X_sshv], axis = 1)\n",
    "    X_0D_zuv = pd.concat([X_0D, X_zuv], axis = 1)\n",
    "    X_0D_sshv = pd.concat([X_0D, X_sshv], axis = 1)\n",
    "    \n",
    "    return X_0D, X_0D_zuv, X_0D_sshv, X_all\n",
    "\n",
    "def trust_cont_score(X, X_map, k=10, alpha=0.5, impute_strategy='median'):\n",
    "    \"\"\"\n",
    "    Computes the \"trustworthiness\" and \"continuity\" [1] of X_map with respect to X.\n",
    "    This is a port and extension of the implementation provided by Van der Maaten [2].\n",
    "    \n",
    "    Parameters:\n",
    "    X     : the data in its original representation\n",
    "    X_map : the lower dimensional representation of the data to be evaluated\n",
    "    k     : parameter that determines the size of the neighborhood for the T&C measure\n",
    "    alpha : mixing parameter in [0,1] that determines the weight given to trustworthiness vs. continuity; higher values will give more\n",
    "            weight to trustworthiness, lower values to continuity.\n",
    "    \n",
    "    [1] Kaski S, Nikkilä J, Oja M, Venna J, Törönen P, Castrén E. Trustworthiness and metrics in visualizing similarity of gene expression. BMC bioinformatics. 2003 Dec;4(1):48.\n",
    "    [2] Maaten L. Learning a parametric embedding by preserving local structure. InArtificial Intelligence and Statistics 2009 Apr 15 (pp. 384-391).\n",
    "    \"\"\"\n",
    "    # Impute X values\n",
    "    X = Imputer(strategy=impute_strategy).fit_transform(X)\n",
    "    # Compute pairwise distance matrices\n",
    "    D_h = pairwise_distances(X, X, metric='euclidean')\n",
    "    D_l = pairwise_distances(X_map, X_map, metric='euclidean')\n",
    "    # Compute neighborhood indices\n",
    "    ind_h = np.argsort(D_h, axis=1)\n",
    "    ind_l = np.argsort(D_l, axis=1)\n",
    "    # Compute trustworthiness\n",
    "    N = X.shape[0]\n",
    "    T = 0\n",
    "    C = 0\n",
    "    t_ranks = np.zeros((k, 1))\n",
    "    c_ranks = np.zeros((k, 1))\n",
    "    for i in range(N):\n",
    "        for j in range(k):\n",
    "            t_ranks[j] = np.where(ind_h[i,:] == ind_l[i, j+1])\n",
    "            c_ranks[j] = np.where(ind_l[i,:] == ind_h[i, j+1])\n",
    "        t_ranks -= k\n",
    "        c_ranks -= k\n",
    "        T += np.sum(t_ranks[np.where(t_ranks > 0)])\n",
    "        C += np.sum(c_ranks[np.where(c_ranks > 0)])\n",
    "    S = (2 / (N * k * (2 * N - 3 * k - 1)))\n",
    "    T = 1.0 - S*T\n",
    "    C = 1.0 - S*C\n",
    "    return alpha*T + (1.0-alpha)*C\n",
    "\n",
    "def sammon_stress(X, X_m, impute_strategy='median'):\n",
    "    X = Imputer(strategy=impute_strategy).fit_transform(X)\n",
    "    Dx = pairwise_distances(X, X, metric='euclidean')\n",
    "    Dy = pairwise_distances(X_m, X_m, metric='euclidean')\n",
    "    # Sammon Stress computes sums over indices where i < j\n",
    "    # We can interpet this as being the upper triangle of each matrix, from the k=1 diagonal\n",
    "    Dx_ut = np.triu(Dx, k=1)\n",
    "    Dy_ut = np.triu(Dy, k=1)\n",
    "    # Compute Sammon Stress, S\n",
    "    S = (1 / np.sum(Dx_ut))*np.sum(np.square(Dx_ut - Dy_ut) / (Dx_ut + np.ones(Dx.shape)))\n",
    "    return S\n",
    "    \n",
    "    \n",
    "def residual_variance(X, X_m, n_neighbors=20):\n",
    "    kng_h = kneighbors_graph(X, n_neighbors=n_neighbors, mode='distance', n_jobs=mp.cpu_count()).toarray()\n",
    "    D_h = graph_shortest_path(kng_h, method='D', directed=False)\n",
    "    #D_h = pairwise_distances(X, X, metric='euclidean')\n",
    "    #D_l = kneighbors_graph(X_m, n_neighbors=50, mode='distance').toarray()\n",
    "    D_l = pairwise_distances(X_m, X_m, metric='euclidean')\n",
    "    r,_ = spearmanr(D_h.flatten(), D_l.flatten())\n",
    "    return 1 - r**2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel(fn, params, n_jobs=mp.cpu_count()):\n",
    "    pool = mp.Pool(n_jobs)\n",
    "    print('started new process pool with {} processes'.format(n_jobs))\n",
    "    try:\n",
    "        res = pool.map(fn, params)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "    except:\n",
    "        print('process pool interrupted, shutting down')\n",
    "        pool.terminate()\n",
    "        pool.join()\n",
    "        raise\n",
    "    return res\n",
    "\n",
    "mplog = mp.get_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mapping_vs_intensity_2d(X_map, ys, title=\"\", xlabel=\"\", ylabel=\"\", instant_labels: pd.DataFrame = None):\n",
    "    plt.scatter(X_map[:,0], X_map[:,1], c=ys, cmap='copper')\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.colorbar().set_label(\"Storm intensity\")\n",
    "    if instant_labels is not None:\n",
    "        for (i, xy) in enumerate(X_map):\n",
    "            if i % 2 == 0:\n",
    "                plt.annotate(instant_labels.values[i], xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(X_df, n_components=2):\n",
    "    imputer = Imputer(strategy='median')\n",
    "    scaler = StandardScaler()\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca_pipeline = Pipeline([('med_imputer', imputer),('scaler', scaler),('pca',pca)])\n",
    "    X_pc = pca_pipeline.fit_transform(X_df)    \n",
    "    pca_comp_feat_ratios = np.square(pca.components_)\n",
    "    feat_names = X_df.columns\n",
    "    pc_feat_contrib = pd.DataFrame(pca_comp_feat_ratios, columns=feat_names)\n",
    "    return X_pc #, pc_feat_contrib, pca.components_, pca.explained_variance_ratio_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_projection(X_df, n_components='auto', eps=0.1):\n",
    "    imputer = Imputer(strategy='median')\n",
    "    scaler = StandardScaler()\n",
    "    proj = SparseRandomProjection(n_components=n_components, eps=eps)\n",
    "    proj_pipeline = Pipeline([('med_imputer', imputer),('scaler', scaler),('proj', proj)])\n",
    "    X_rp = proj_pipeline.fit_transform(X_df)\n",
    "    return X_rp #, proj.n_components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne(X_df, n_components=2, n_iter=5000, perplexity=30, learning_rate=100, init='pca'):\n",
    "    imputer = Imputer(strategy='median')\n",
    "    scaler = StandardScaler()\n",
    "    tsne = TSNE(n_components=n_components, perplexity=perplexity, learning_rate=learning_rate, init=init, n_iter=n_iter)\n",
    "    tsne_pipeline = Pipeline([('med_imputer', imputer),('scaler', scaler),('tsne', tsne)])\n",
    "    X_tsne = tsne_pipeline.fit_transform(X_df)\n",
    "    print(\"t-SNE completed after {} iterations with final KLD: {}\".format(tsne.n_iter_, tsne.kl_divergence_))\n",
    "    return X_tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def umap(X_df, n_components=2, y=None, n_neighbors=5, min_dist=0.1, metric='correlation'):\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    imputer = Imputer(strategy='median')\n",
    "    scaler = StandardScaler()\n",
    "    umap = UMAP(n_components=n_components, n_neighbors=n_neighbors, min_dist=min_dist, metric=metric)\n",
    "    umap_pipeline = Pipeline([('med_imputer', imputer),('scaler', scaler),('umap', umap)])\n",
    "    X_umap = umap_pipeline.fit_transform(X_df, y)\n",
    "    warnings.resetwarnings()\n",
    "    return X_umap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data\n",
    "\n",
    "The 3000 storms have been separated in a train set, a test set and a local starting kit (train+test sets). The data from `download_data.py` (local starting kit) includes only 1/4 storms of the total database; and the train set on which your code will run on the platform has another half. They are disjoined. \n",
    "\n",
    "Let's have a look at the local train data (only the first rows are plotted)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from problem import get_train_data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 100\n",
    "pd.options.display.max_columns = 100\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# RMSE will be in the same units as the output variable(windspeed in knots)\n",
    "# Data Exploration: the training set has 15777 training examples with 859 features each.\n",
    "data_train, y_true = get_train_data()\n",
    "X_0D, X_0D_zuv, X_0D_sshv, X_all = feature_groups(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy of their cross-validation function: \n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "def get_cv(X, y, num_splits):\n",
    "    group = np.array(X['stormid'])\n",
    "    X, y, group = shuffle(X, y, group, random_state=3)\n",
    "    gkf = GroupKFold(n_splits=num_splits).split(X, y, group)\n",
    "    return gkf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stormid</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1979152N11265</td>\n",
       "      <td>-11.368664</td>\n",
       "      <td>2.252614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1979152N11265</td>\n",
       "      <td>-9.471066</td>\n",
       "      <td>-0.677273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1979152N11265</td>\n",
       "      <td>-11.327783</td>\n",
       "      <td>2.578541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1979152N11265</td>\n",
       "      <td>-9.364247</td>\n",
       "      <td>0.408132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1979152N11265</td>\n",
       "      <td>-11.661928</td>\n",
       "      <td>2.437675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1979152N11265</td>\n",
       "      <td>-10.149296</td>\n",
       "      <td>-1.203223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1979152N11265</td>\n",
       "      <td>-10.009797</td>\n",
       "      <td>0.640239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1979152N11265</td>\n",
       "      <td>-7.120567</td>\n",
       "      <td>-1.484484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1979152N11265</td>\n",
       "      <td>-9.626284</td>\n",
       "      <td>1.270390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1979152N11265</td>\n",
       "      <td>-7.426718</td>\n",
       "      <td>-1.094641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1979152N11265</td>\n",
       "      <td>-7.649788</td>\n",
       "      <td>2.506732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1979152N11265</td>\n",
       "      <td>-4.956765</td>\n",
       "      <td>0.355648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1979152N11265</td>\n",
       "      <td>-9.092579</td>\n",
       "      <td>2.919142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1979191N22264</td>\n",
       "      <td>-6.879639</td>\n",
       "      <td>11.184702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1979191N22264</td>\n",
       "      <td>-10.828461</td>\n",
       "      <td>9.396421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1979191N22264</td>\n",
       "      <td>-8.174858</td>\n",
       "      <td>6.606540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1979191N22264</td>\n",
       "      <td>-6.686737</td>\n",
       "      <td>6.813224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1979191N22264</td>\n",
       "      <td>-3.664360</td>\n",
       "      <td>3.476398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1979191N22264</td>\n",
       "      <td>-6.273378</td>\n",
       "      <td>1.595225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1979191N22264</td>\n",
       "      <td>-3.966650</td>\n",
       "      <td>-2.551688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1979191N22264</td>\n",
       "      <td>-3.270659</td>\n",
       "      <td>-0.686791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1979191N22264</td>\n",
       "      <td>-0.036183</td>\n",
       "      <td>-1.176518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1979191N22264</td>\n",
       "      <td>-2.900758</td>\n",
       "      <td>2.104360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1979191N22264</td>\n",
       "      <td>-2.372805</td>\n",
       "      <td>1.663610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1979191N22264</td>\n",
       "      <td>1.890370</td>\n",
       "      <td>4.052512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1979191N22264</td>\n",
       "      <td>4.398656</td>\n",
       "      <td>6.921130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1979191N22264</td>\n",
       "      <td>-1.157016</td>\n",
       "      <td>10.292920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1979191N22264</td>\n",
       "      <td>-0.233173</td>\n",
       "      <td>8.979716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1979191N22264</td>\n",
       "      <td>4.914991</td>\n",
       "      <td>8.926471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1979191N22264</td>\n",
       "      <td>5.370788</td>\n",
       "      <td>9.745817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1979191N22264</td>\n",
       "      <td>0.383421</td>\n",
       "      <td>8.587626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1979191N22264</td>\n",
       "      <td>0.886339</td>\n",
       "      <td>6.448014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1979191N22264</td>\n",
       "      <td>4.501262</td>\n",
       "      <td>6.864055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1979191N22264</td>\n",
       "      <td>3.715739</td>\n",
       "      <td>8.528786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1979191N22264</td>\n",
       "      <td>-1.033907</td>\n",
       "      <td>9.454550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1979191N22264</td>\n",
       "      <td>1.778386</td>\n",
       "      <td>8.913476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1979191N22264</td>\n",
       "      <td>2.212269</td>\n",
       "      <td>10.802144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1979191N22264</td>\n",
       "      <td>1.085551</td>\n",
       "      <td>11.434294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1979192N30280</td>\n",
       "      <td>-1.057435</td>\n",
       "      <td>13.361290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1979192N30280</td>\n",
       "      <td>-2.940967</td>\n",
       "      <td>14.568909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1979192N30280</td>\n",
       "      <td>-0.716633</td>\n",
       "      <td>12.673653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1979192N30280</td>\n",
       "      <td>0.134480</td>\n",
       "      <td>13.324597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1979192N30280</td>\n",
       "      <td>3.789703</td>\n",
       "      <td>14.580910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1979192N30280</td>\n",
       "      <td>0.284395</td>\n",
       "      <td>16.498278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1979192N30280</td>\n",
       "      <td>-1.967395</td>\n",
       "      <td>18.294674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1979192N30280</td>\n",
       "      <td>-0.970484</td>\n",
       "      <td>19.534300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1979192N30280</td>\n",
       "      <td>0.021338</td>\n",
       "      <td>20.147919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1979192N30280</td>\n",
       "      <td>-0.349296</td>\n",
       "      <td>19.359960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1979241N11335</td>\n",
       "      <td>-3.654283</td>\n",
       "      <td>1.907637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1979241N11335</td>\n",
       "      <td>-5.757329</td>\n",
       "      <td>6.045148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15727</th>\n",
       "      <td>2016323N13279</td>\n",
       "      <td>-2.622974</td>\n",
       "      <td>1.588903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15728</th>\n",
       "      <td>2016323N13279</td>\n",
       "      <td>-2.804977</td>\n",
       "      <td>2.028196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15729</th>\n",
       "      <td>2016323N13279</td>\n",
       "      <td>-2.649678</td>\n",
       "      <td>2.636040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15730</th>\n",
       "      <td>2016323N13279</td>\n",
       "      <td>-4.234346</td>\n",
       "      <td>0.824828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15731</th>\n",
       "      <td>2016323N13279</td>\n",
       "      <td>-3.897787</td>\n",
       "      <td>0.616690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15732</th>\n",
       "      <td>2016323N13279</td>\n",
       "      <td>-4.891383</td>\n",
       "      <td>1.692766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15733</th>\n",
       "      <td>2016323N13279</td>\n",
       "      <td>-3.875626</td>\n",
       "      <td>1.709267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15734</th>\n",
       "      <td>2016323N13279</td>\n",
       "      <td>-5.076464</td>\n",
       "      <td>-0.965803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15735</th>\n",
       "      <td>2016323N13279</td>\n",
       "      <td>-3.723929</td>\n",
       "      <td>-0.269427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15736</th>\n",
       "      <td>2016323N13279</td>\n",
       "      <td>-3.701011</td>\n",
       "      <td>0.283340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15737</th>\n",
       "      <td>2016323N13279</td>\n",
       "      <td>-2.526306</td>\n",
       "      <td>1.144810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15738</th>\n",
       "      <td>2016323N13279</td>\n",
       "      <td>-3.183821</td>\n",
       "      <td>0.819705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15739</th>\n",
       "      <td>2016323N13279</td>\n",
       "      <td>-2.670816</td>\n",
       "      <td>1.520783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15740</th>\n",
       "      <td>2016323N13279</td>\n",
       "      <td>-3.748267</td>\n",
       "      <td>3.295871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15741</th>\n",
       "      <td>2016323N13279</td>\n",
       "      <td>-3.392116</td>\n",
       "      <td>5.004964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15742</th>\n",
       "      <td>2016323N13279</td>\n",
       "      <td>-5.098673</td>\n",
       "      <td>4.277419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15743</th>\n",
       "      <td>2016323N13279</td>\n",
       "      <td>-3.395313</td>\n",
       "      <td>2.716406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15744</th>\n",
       "      <td>2016323N13279</td>\n",
       "      <td>-4.238845</td>\n",
       "      <td>5.319476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15745</th>\n",
       "      <td>2016323N13279</td>\n",
       "      <td>-4.424425</td>\n",
       "      <td>5.522723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15746</th>\n",
       "      <td>2017114S08137</td>\n",
       "      <td>-10.460972</td>\n",
       "      <td>8.157699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15747</th>\n",
       "      <td>2017114S08137</td>\n",
       "      <td>-9.899401</td>\n",
       "      <td>2.296600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15748</th>\n",
       "      <td>2017114S08137</td>\n",
       "      <td>-12.209877</td>\n",
       "      <td>7.582972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15749</th>\n",
       "      <td>2017114S08137</td>\n",
       "      <td>-10.226893</td>\n",
       "      <td>3.738607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15750</th>\n",
       "      <td>2017114S08137</td>\n",
       "      <td>-12.687749</td>\n",
       "      <td>7.809170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15751</th>\n",
       "      <td>2017114S08137</td>\n",
       "      <td>-10.169924</td>\n",
       "      <td>1.774905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15752</th>\n",
       "      <td>2017114S08137</td>\n",
       "      <td>-11.659687</td>\n",
       "      <td>5.762686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15753</th>\n",
       "      <td>2017114S08137</td>\n",
       "      <td>-10.272373</td>\n",
       "      <td>2.468092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15754</th>\n",
       "      <td>2017114S08137</td>\n",
       "      <td>-11.778329</td>\n",
       "      <td>7.025935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15755</th>\n",
       "      <td>2017114S08137</td>\n",
       "      <td>-10.360892</td>\n",
       "      <td>1.164345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15756</th>\n",
       "      <td>2017114S08137</td>\n",
       "      <td>-11.804598</td>\n",
       "      <td>5.068649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15757</th>\n",
       "      <td>2017114S08137</td>\n",
       "      <td>-10.157083</td>\n",
       "      <td>4.114744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15758</th>\n",
       "      <td>2017114S08137</td>\n",
       "      <td>-11.919261</td>\n",
       "      <td>9.481547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15759</th>\n",
       "      <td>2017114S08137</td>\n",
       "      <td>-11.174776</td>\n",
       "      <td>3.528153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15760</th>\n",
       "      <td>2017114S08137</td>\n",
       "      <td>-12.872307</td>\n",
       "      <td>8.396741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15761</th>\n",
       "      <td>2017114S08137</td>\n",
       "      <td>-11.711938</td>\n",
       "      <td>7.537682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15762</th>\n",
       "      <td>2017114S08137</td>\n",
       "      <td>-13.829912</td>\n",
       "      <td>10.919139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15763</th>\n",
       "      <td>2017114S08137</td>\n",
       "      <td>-11.444435</td>\n",
       "      <td>8.362177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15764</th>\n",
       "      <td>2017114S08137</td>\n",
       "      <td>-12.008813</td>\n",
       "      <td>11.034733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15765</th>\n",
       "      <td>2017114S08137</td>\n",
       "      <td>-10.130545</td>\n",
       "      <td>7.255167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15766</th>\n",
       "      <td>2017114S08137</td>\n",
       "      <td>-12.122210</td>\n",
       "      <td>11.918860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15767</th>\n",
       "      <td>2017161N13119</td>\n",
       "      <td>-15.689685</td>\n",
       "      <td>-1.091099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15768</th>\n",
       "      <td>2017161N13119</td>\n",
       "      <td>-16.738972</td>\n",
       "      <td>-3.303041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15769</th>\n",
       "      <td>2017161N13119</td>\n",
       "      <td>-16.409642</td>\n",
       "      <td>-2.519886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15770</th>\n",
       "      <td>2017161N13119</td>\n",
       "      <td>-15.998733</td>\n",
       "      <td>-2.983007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15771</th>\n",
       "      <td>2017161N13119</td>\n",
       "      <td>-15.751970</td>\n",
       "      <td>-1.947664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15772</th>\n",
       "      <td>2017161N13119</td>\n",
       "      <td>-15.955869</td>\n",
       "      <td>-4.682482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15773</th>\n",
       "      <td>2017161N13119</td>\n",
       "      <td>-14.190847</td>\n",
       "      <td>-5.585417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15774</th>\n",
       "      <td>2017161N13119</td>\n",
       "      <td>-12.765021</td>\n",
       "      <td>-5.594391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15775</th>\n",
       "      <td>2017161N13119</td>\n",
       "      <td>-11.620434</td>\n",
       "      <td>-5.519461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15776</th>\n",
       "      <td>2017161N13119</td>\n",
       "      <td>-11.896162</td>\n",
       "      <td>-9.408074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15777 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             stormid          0          1\n",
       "0      1979152N11265 -11.368664   2.252614\n",
       "1      1979152N11265  -9.471066  -0.677273\n",
       "2      1979152N11265 -11.327783   2.578541\n",
       "3      1979152N11265  -9.364247   0.408132\n",
       "4      1979152N11265 -11.661928   2.437675\n",
       "5      1979152N11265 -10.149296  -1.203223\n",
       "6      1979152N11265 -10.009797   0.640239\n",
       "7      1979152N11265  -7.120567  -1.484484\n",
       "8      1979152N11265  -9.626284   1.270390\n",
       "9      1979152N11265  -7.426718  -1.094641\n",
       "10     1979152N11265  -7.649788   2.506732\n",
       "11     1979152N11265  -4.956765   0.355648\n",
       "12     1979152N11265  -9.092579   2.919142\n",
       "13     1979191N22264  -6.879639  11.184702\n",
       "14     1979191N22264 -10.828461   9.396421\n",
       "15     1979191N22264  -8.174858   6.606540\n",
       "16     1979191N22264  -6.686737   6.813224\n",
       "17     1979191N22264  -3.664360   3.476398\n",
       "18     1979191N22264  -6.273378   1.595225\n",
       "19     1979191N22264  -3.966650  -2.551688\n",
       "20     1979191N22264  -3.270659  -0.686791\n",
       "21     1979191N22264  -0.036183  -1.176518\n",
       "22     1979191N22264  -2.900758   2.104360\n",
       "23     1979191N22264  -2.372805   1.663610\n",
       "24     1979191N22264   1.890370   4.052512\n",
       "25     1979191N22264   4.398656   6.921130\n",
       "26     1979191N22264  -1.157016  10.292920\n",
       "27     1979191N22264  -0.233173   8.979716\n",
       "28     1979191N22264   4.914991   8.926471\n",
       "29     1979191N22264   5.370788   9.745817\n",
       "30     1979191N22264   0.383421   8.587626\n",
       "31     1979191N22264   0.886339   6.448014\n",
       "32     1979191N22264   4.501262   6.864055\n",
       "33     1979191N22264   3.715739   8.528786\n",
       "34     1979191N22264  -1.033907   9.454550\n",
       "35     1979191N22264   1.778386   8.913476\n",
       "36     1979191N22264   2.212269  10.802144\n",
       "37     1979191N22264   1.085551  11.434294\n",
       "38     1979192N30280  -1.057435  13.361290\n",
       "39     1979192N30280  -2.940967  14.568909\n",
       "40     1979192N30280  -0.716633  12.673653\n",
       "41     1979192N30280   0.134480  13.324597\n",
       "42     1979192N30280   3.789703  14.580910\n",
       "43     1979192N30280   0.284395  16.498278\n",
       "44     1979192N30280  -1.967395  18.294674\n",
       "45     1979192N30280  -0.970484  19.534300\n",
       "46     1979192N30280   0.021338  20.147919\n",
       "47     1979192N30280  -0.349296  19.359960\n",
       "48     1979241N11335  -3.654283   1.907637\n",
       "49     1979241N11335  -5.757329   6.045148\n",
       "...              ...        ...        ...\n",
       "15727  2016323N13279  -2.622974   1.588903\n",
       "15728  2016323N13279  -2.804977   2.028196\n",
       "15729  2016323N13279  -2.649678   2.636040\n",
       "15730  2016323N13279  -4.234346   0.824828\n",
       "15731  2016323N13279  -3.897787   0.616690\n",
       "15732  2016323N13279  -4.891383   1.692766\n",
       "15733  2016323N13279  -3.875626   1.709267\n",
       "15734  2016323N13279  -5.076464  -0.965803\n",
       "15735  2016323N13279  -3.723929  -0.269427\n",
       "15736  2016323N13279  -3.701011   0.283340\n",
       "15737  2016323N13279  -2.526306   1.144810\n",
       "15738  2016323N13279  -3.183821   0.819705\n",
       "15739  2016323N13279  -2.670816   1.520783\n",
       "15740  2016323N13279  -3.748267   3.295871\n",
       "15741  2016323N13279  -3.392116   5.004964\n",
       "15742  2016323N13279  -5.098673   4.277419\n",
       "15743  2016323N13279  -3.395313   2.716406\n",
       "15744  2016323N13279  -4.238845   5.319476\n",
       "15745  2016323N13279  -4.424425   5.522723\n",
       "15746  2017114S08137 -10.460972   8.157699\n",
       "15747  2017114S08137  -9.899401   2.296600\n",
       "15748  2017114S08137 -12.209877   7.582972\n",
       "15749  2017114S08137 -10.226893   3.738607\n",
       "15750  2017114S08137 -12.687749   7.809170\n",
       "15751  2017114S08137 -10.169924   1.774905\n",
       "15752  2017114S08137 -11.659687   5.762686\n",
       "15753  2017114S08137 -10.272373   2.468092\n",
       "15754  2017114S08137 -11.778329   7.025935\n",
       "15755  2017114S08137 -10.360892   1.164345\n",
       "15756  2017114S08137 -11.804598   5.068649\n",
       "15757  2017114S08137 -10.157083   4.114744\n",
       "15758  2017114S08137 -11.919261   9.481547\n",
       "15759  2017114S08137 -11.174776   3.528153\n",
       "15760  2017114S08137 -12.872307   8.396741\n",
       "15761  2017114S08137 -11.711938   7.537682\n",
       "15762  2017114S08137 -13.829912  10.919139\n",
       "15763  2017114S08137 -11.444435   8.362177\n",
       "15764  2017114S08137 -12.008813  11.034733\n",
       "15765  2017114S08137 -10.130545   7.255167\n",
       "15766  2017114S08137 -12.122210  11.918860\n",
       "15767  2017161N13119 -15.689685  -1.091099\n",
       "15768  2017161N13119 -16.738972  -3.303041\n",
       "15769  2017161N13119 -16.409642  -2.519886\n",
       "15770  2017161N13119 -15.998733  -2.983007\n",
       "15771  2017161N13119 -15.751970  -1.947664\n",
       "15772  2017161N13119 -15.955869  -4.682482\n",
       "15773  2017161N13119 -14.190847  -5.585417\n",
       "15774  2017161N13119 -12.765021  -5.594391\n",
       "15775  2017161N13119 -11.620434  -5.519461\n",
       "15776  2017161N13119 -11.896162  -9.408074\n",
       "\n",
       "[15777 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pca = pca(X_0D, 2)\n",
    "X_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Results for Everything: (must change X_df in each function)\n",
    "\n",
    "pca_dict = {}\n",
    "max_depth = [5,10,20,25]\n",
    "max_depth_tree = [5,10,15,20,25,50,100]\n",
    "max_features = ['sqrt', 'log2', None]\n",
    "n_estimators = [10,25,50,100,200]\n",
    "alpha_lasso = [.00001, .0001, .001, .01, .1, 1]\n",
    "alpha_ridge = [1, 2, 5, 10, 25, 50, 100, 200, 500, 1000]\n",
    "\n",
    "params_forest_boost = list(itertools.product(max_depth, max_features, n_estimators))\n",
    "params_tree = list(itertools.product(max_depth, max_features))\n",
    "\n",
    "results_forest = parallel(run_forest, params_forest_boost)\n",
    "results_tree = parallel(run_tree, params_tree)\n",
    "results_boost = parallel(run_boost, params_forest_boost)\n",
    "\n",
    "pca_dict['forest_regressor'] = dict(results_forest)\n",
    "\n",
    "pca_dict['tree_regressor'] = dict(results_tree)\n",
    "\n",
    "pca_dict['boost_regressor'] = dict(results_boost)\n",
    "\n",
    "pca_dict['ridge_regressor'] = ridge(X_0D_zuv, y_true, alpha_ridge)\n",
    "\n",
    "pca_dict['lasso_regressor'] = lasso(X_0D_zuv, y_true, alpha_lasso)\n",
    "\n",
    "print(pca_dict)\n",
    "#np.save('storm_forecast_zero_d_zuv_tuning.npy', zero_0d_zuv_dict)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started new process pool with 12 processes\n",
      "started new process pool with 12 processes\n",
      "started new process pool with 12 processes\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# Generate Results for Everything: (must change X_df in each function)\n",
    "zero_0d_zuv_dict = {}\n",
    "max_depth = [5,10,20,25]\n",
    "max_depth_tree = [5,10,15,20,25,50,100]\n",
    "max_features = ['sqrt', 'log2', None]\n",
    "n_estimators = [10,25,50,100,200]\n",
    "alpha_lasso = [.00001, .0001, .001, .01, .1, 1]\n",
    "alpha_ridge = [1, 2, 5, 10, 25, 50, 100, 200, 500, 1000]\n",
    "\n",
    "params_forest_boost = list(itertools.product(max_depth, max_features, n_estimators))\n",
    "params_tree = list(itertools.product(max_depth, max_features))\n",
    "\n",
    "results_forest2 = parallel(run_forest, params_forest_boost)\n",
    "results_tree2 = parallel(run_tree, params_tree)\n",
    "results_boost2 = parallel(run_boost, params_forest_boost)\n",
    "\n",
    "zero_0d_zuv_dict['forest_regressor'] = dict(results_forest2)\n",
    "\n",
    "zero_0d_zuv_dict['tree_regressor'] = dict(results_tree2)\n",
    "\n",
    "zero_0d_zuv_dict['boost_regressor'] = dict(results_boost2)\n",
    "\n",
    "zero_0d_zuv_dict['ridge_regressor'] = ridge(X_0D_zuv, y_true, alpha_ridge)\n",
    "\n",
    "zero_0d_zuv_dict['lasso_regressor'] = lasso(X_0D_zuv, y_true, alpha_lasso)\n",
    "\n",
    "print(zero_0d_zuv_dict)\n",
    "#np.save('storm_forecast_zero_d_zuv_tuning.npy', zero_0d_zuv_dict)\n",
    "'''\n",
    "0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started new process pool with 12 processes\n",
      "Saved!\n",
      "started new process pool with 12 processes\n",
      "Saved!\n",
      "started new process pool with 12 processes\n",
      "Saved!\n",
      "started new process pool with 12 processes\n",
      "Saved!\n",
      "{'X_0D': [((0.01, 0.0001), 27.243328399806316), ((0.01, 0.001), 27.096007526271688), ((0.01, 0.01), 26.083204006998987), ((0.01, 0.1), 25.92087292373086), ((0.01, 1.0), 26.999871197547442), ((0.01, 10.0), 27.2440324709632), ((0.01, 100.0), 27.257221316978672), ((0.1, 0.0001), 27.09001658967749), ((0.1, 0.001), 25.660937270730468), ((0.1, 0.01), 20.85404394279725), ((0.1, 0.1), 20.567592056667273), ((0.1, 1.0), 25.24842331218722), ((0.1, 10.0), 27.09718725534535), ((0.1, 100.0), 27.230405638910973), ((1.0, 0.0001), 25.61151118799768), ((1.0, 0.001), 19.754588171704583), ((1.0, 0.01), 16.41439196964381), ((1.0, 0.1), 15.597830580066976), ((1.0, 1.0), 20.058082672622426), ((1.0, 10.0), 25.887955680956146), ((1.0, 100.0), 26.97395195403758), ((10.0, 0.0001), 19.660596480820058), ((10.0, 0.001), 16.82168007991266), ((10.0, 0.01), 15.734650700355697), ((10.0, 0.1), 14.503111312243675), ((10.0, 1.0), 14.0324350125818), ((10.0, 10.0), 21.015335902701096), ((10.0, 100.0), 25.698433935351037), ((100.0, 0.0001), 16.916228972905277), ((100.0, 0.001), 16.520172090431707), ((100.0, 0.01), 15.230825038156114), ((100.0, 0.1), 13.82237290917782), ((100.0, 1.0), 11.74512241762449), ((100.0, 10.0), 16.628895628118435), ((100.0, 100.0), 23.860108930948325)], 'X_0D_zuv': [((0.01, 0.0001), 27.1566134605375), ((0.01, 0.001), 26.67638626461615), ((0.01, 0.01), 26.96751318528937), ((0.01, 0.1), 27.256915341524532), ((0.01, 1.0), 27.25898646177), ((0.01, 10.0), 27.25898712229493), ((0.01, 100.0), 27.258987122294926), ((0.1, 0.0001), 26.363737039310937), ((0.1, 0.001), 25.074849576008994), ((0.1, 0.01), 25.43283444596078), ((0.1, 0.1), 27.227325746753944), ((0.1, 1.0), 27.24802212437065), ((0.1, 10.0), 27.248028724085078), ((0.1, 100.0), 27.248028724085078), ((1.0, 0.0001), 24.53550587056271), ((1.0, 0.001), 19.342050782861033), ((1.0, 0.01), 20.050705171072703), ((1.0, 0.1), 26.94270407154751), ((1.0, 1.0), 27.146650313408465), ((1.0, 10.0), 27.146715306100866), ((1.0, 100.0), 27.14671530610087), ((10.0, 0.0001), 18.519470631831133), ((10.0, 0.001), 14.988792933695175), ((10.0, 0.01), 13.52838526359176), ((10.0, 0.1), 25.257062718732993), ((10.0, 1.0), 27.001205680065492), ((10.0, 10.0), 27.001755954560487), ((10.0, 100.0), 27.001755954560487), ((100.0, 0.0001), 15.600693148492503), ((100.0, 0.001), 13.834743028363318), ((100.0, 0.01), 9.297232111255596), ((100.0, 0.1), 22.170919265526493), ((100.0, 1.0), 26.846357122057757), ((100.0, 10.0), 26.848312497848216), ((100.0, 100.0), 26.848312497848216)], 'X_0D_sshv': [((0.01, 0.0001), 27.07020332467333), ((0.01, 0.001), 26.49952695658373), ((0.01, 0.01), 27.164184034160783), ((0.01, 0.1), 27.258968327468615), ((0.01, 1.0), 27.25898712229487), ((0.01, 10.0), 27.258987122294926), ((0.01, 100.0), 27.258987122294926), ((0.1, 0.0001), 26.076067802572247), ((0.1, 0.001), 24.535648903259858), ((0.1, 0.01), 26.402351286859638), ((0.1, 0.1), 27.247840306707815), ((0.1, 1.0), 27.248028724084485), ((0.1, 10.0), 27.248028724085078), ((0.1, 100.0), 27.248028724085078), ((1.0, 0.0001), 23.737974289016506), ((1.0, 0.001), 19.118671917863562), ((1.0, 0.01), 23.22696483928863), ((1.0, 0.1), 27.1448251242822), ((1.0, 1.0), 27.14671530609461), ((1.0, 10.0), 27.14671530610087), ((1.0, 100.0), 27.14671530610087), ((10.0, 0.0001), 18.234724161669615), ((10.0, 0.001), 14.479537667902159), ((10.0, 0.01), 17.60373411935975), ((10.0, 0.1), 26.983877477396916), ((10.0, 1.0), 27.00175595449156), ((10.0, 10.0), 27.001755954560487), ((10.0, 100.0), 27.001755954560487), ((100.0, 0.0001), 15.10172524113521), ((100.0, 0.001), 13.207505159240991), ((100.0, 0.01), 14.963344856010174), ((100.0, 0.1), 26.7912303977119), ((100.0, 1.0), 26.848312497700658), ((100.0, 10.0), 26.848312497848216), ((100.0, 100.0), 26.848312497848216)], 'X_all': [((0.01, 0.0001), 27.008174945220887), ((0.01, 0.001), 26.552464720173962), ((0.01, 0.01), 27.239335927967932), ((0.01, 0.1), 27.258983819179225), ((0.01, 1.0), 27.258987122294926), ((0.01, 10.0), 27.258987122294926), ((0.01, 100.0), 27.258987122294926), ((0.1, 0.0001), 25.93139495217719), ((0.1, 0.001), 24.67435478800664), ((0.1, 0.01), 27.05377230868354), ((0.1, 0.1), 27.24799561896016), ((0.1, 1.0), 27.248028724085078), ((0.1, 10.0), 27.248028724085078), ((0.1, 100.0), 27.248028724085078), ((1.0, 0.0001), 23.26230456063529), ((1.0, 0.001), 19.25126188617382), ((1.0, 0.01), 25.647306083540965), ((1.0, 0.1), 27.146383774739572), ((1.0, 1.0), 27.146715306100866), ((1.0, 10.0), 27.14671530610087), ((1.0, 100.0), 27.14671530610087), ((10.0, 0.0001), 17.75147010686356), ((10.0, 0.001), 13.621326635269101), ((10.0, 0.01), 20.280868080657825), ((10.0, 0.1), 26.998623840673005), ((10.0, 1.0), 27.001755954560476), ((10.0, 10.0), 27.001755954560487), ((10.0, 100.0), 27.001755954560487), ((100.0, 0.0001), 14.385623551555447), ((100.0, 0.001), 11.198257603987004), ((100.0, 0.01), 16.714211662357194), ((100.0, 0.1), 26.837049017144665), ((100.0, 1.0), 26.848312497848188), ((100.0, 10.0), 26.848312497848216), ((100.0, 100.0), 26.848312497848216)]}\n"
     ]
    }
   ],
   "source": [
    "# Generate Results for Everything: (must change X_df in each function)\n",
    "\n",
    "svr_dict = {}\n",
    "C = np.logspace(-2, 2, num=5) \n",
    "gamma = np.logspace(-4, 2, num=7)\n",
    "\n",
    "params_svr = list(itertools.product(C, gamma))\n",
    "\n",
    "def run_svr(p):\n",
    "    c, g = p\n",
    "    return svr(X_0D, y_true, C = c, gamma = g)\n",
    "results_svr1 = parallel(run_svr, params_svr)\n",
    "svr_dict['X_0D'] = results_svr1\n",
    "np.save('storm_forecast_svr_tuning.npy', svr_dict)\n",
    "print('Saved!')\n",
    "\n",
    "def run_svr(p):\n",
    "    c, g = p\n",
    "    return svr(X_0D_zuv, y_true, C = c, gamma = g)\n",
    "results_svr2 = parallel(run_svr, params_svr)\n",
    "svr_dict['X_0D_zuv'] = results_svr2\n",
    "np.save('storm_forecast_svr_tuning.npy', svr_dict)\n",
    "print('Saved!')\n",
    "\n",
    "def run_svr(p):\n",
    "    c, g = p\n",
    "    return svr(X_0D_sshv, y_true, C = c, gamma = g)\n",
    "results_svr3 = parallel(run_svr, params_svr)\n",
    "svr_dict['X_0D_sshv'] = results_svr3\n",
    "np.save('storm_forecast_svr_tuning.npy', svr_dict)\n",
    "print('Saved!')\n",
    "\n",
    "def run_svr(p):\n",
    "    c, g = p\n",
    "    return svr(X_all, y_true, C = c, gamma = g)\n",
    "results_svr4 = parallel(run_svr, params_svr)\n",
    "svr_dict['X_all'] = results_svr4\n",
    "np.save('storm_forecast_svr_tuning.npy', svr_dict)\n",
    "print('Saved!')\n",
    "\n",
    "print(svr_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homemade Forest Parameter Tuning \n",
    "def forest(X_df, y_df, max_depth, max_features, n_estimators, n_splits = 5):\n",
    "    mse_sum = 0\n",
    "    for train_index, test_index in get_cv(X_df,y_df,n_splits):\n",
    "        forest_regressor = Pipeline([('imputer', Imputer(strategy='median')), \n",
    "            ('scale', StandardScaler()),       \n",
    "            ('regressor', RandomForestRegressor(max_depth = max_depth, max_features = max_features, n_estimators = n_estimators))])\n",
    "        drop = X_df.drop(columns = ['stormid'])\n",
    "\n",
    "        X_train, X_test = drop.iloc[train_index,:], drop.iloc[test_index,:]\n",
    "        y_train, y_test = y_df[train_index], y_df[test_index]\n",
    "\n",
    "        forest_regressor.fit(X_train,y_train)\n",
    "        y_pred = forest_regressor.predict(X_test)\n",
    "        mse_sum += mean_squared_error(y_test, y_pred)\n",
    "        \n",
    "    return ((max_depth, max_features, n_estimators),np.sqrt(mse_sum/n_splits))\n",
    "\n",
    "def run_forest(p):\n",
    "    d, f, n = p\n",
    "    return forest(X_0D_zuv, y_true, max_depth=d, max_features=f, n_estimators=n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started new process pool with 12 processes\n",
      "started new process pool with 12 processes\n",
      "started new process pool with 12 processes\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "{'forest_regressor': {(5, 'sqrt', 10): 17.41505097083192, (5, 'sqrt', 25): 17.57600564260812, (5, 'sqrt', 50): 17.418217393669508, (5, 'sqrt', 100): 17.391244257822596, (5, 'sqrt', 250): 17.263172287885936, (5, 'sqrt', 500): 17.2519530747091, (5, 'log2', 10): 17.41505097083192, (5, 'log2', 25): 17.57600564260812, (5, 'log2', 50): 17.418217393669508, (5, 'log2', 100): 17.391244257822596, (5, 'log2', 250): 17.263172287885936, (5, 'log2', 500): 17.2519530747091, (5, None, 10): 15.531966281617958, (5, None, 25): 15.52322425859352, (5, None, 50): 15.495562032567733, (5, None, 100): 15.47850971848783, (5, None, 250): 15.468644943766844, (5, None, 500): 15.485014667469407, (10, 'sqrt', 10): 14.172701561797307, (10, 'sqrt', 25): 13.933482493838005, (10, 'sqrt', 50): 13.850762571592373, (10, 'sqrt', 100): 13.802312435494564, (10, 'sqrt', 250): 13.806369416673764, (10, 'sqrt', 500): 13.801773222510917, (10, 'log2', 10): 14.108000747985104, (10, 'log2', 25): 13.95389612301635, (10, 'log2', 50): 13.868476554511368, (10, 'log2', 100): 13.82520725400644, (10, 'log2', 250): 13.83498407512651, (10, 'log2', 500): 13.797664681210184, (10, None, 10): 13.494281090337985, (10, None, 25): 13.361051267886596, (10, None, 50): 13.269163828974126, (10, None, 100): 13.237744308183226, (10, None, 250): 13.227449998267673, (10, None, 500): 13.212619699004538, (15, 'sqrt', 10): 12.428233401377106, (15, 'sqrt', 25): 12.097915394278965, (15, 'sqrt', 50): 11.905560573162777, (15, 'sqrt', 100): 11.909275570459767, (15, 'sqrt', 250): 11.837880728466416, (15, 'sqrt', 500): 11.817521479807821, (15, 'log2', 10): 12.378157448013582, (15, 'log2', 25): 11.975792759881564, (15, 'log2', 50): 11.941642219387912, (15, 'log2', 100): 11.867662198848295, (15, 'log2', 250): 11.829708004115806, (15, 'log2', 500): 11.813087660232483, (15, None, 10): 12.71159508030578, (15, None, 25): 12.297798845495075, (15, None, 50): 12.193953988691927, (15, None, 100): 12.157566485722198, (15, None, 250): 12.11699120974591, (15, None, 500): 12.091730486452528, (20, 'sqrt', 10): 11.900087262931827, (20, 'sqrt', 25): 11.409340384974527, (20, 'sqrt', 50): 11.217587720974924, (20, 'sqrt', 100): 11.155175062412221, (20, 'sqrt', 250): 11.123041659410546, (20, 'sqrt', 500): 11.09961091765332, (20, 'log2', 10): 11.84518695157379, (20, 'log2', 25): 11.384710173279068, (20, 'log2', 50): 11.252098665565759, (20, 'log2', 100): 11.180031404166945, (20, 'log2', 250): 11.12354313138684, (20, 'log2', 500): 11.106667138968863, (20, None, 10): 12.504587057376286, (20, None, 25): 12.007132038168146, (20, None, 50): 11.925379772566973, (20, None, 100): 11.880522197251988, (20, None, 250): 11.824142459959974, (20, None, 500): 11.801551192639177, (25, 'sqrt', 10): 11.815012874544898, (25, 'sqrt', 25): 11.338282396623452, (25, 'sqrt', 50): 11.106768062315126, (25, 'sqrt', 100): 11.054317603777694, (25, 'sqrt', 250): 11.013452186835089, (25, 'sqrt', 500): 10.961161704397576, (25, 'log2', 10): 11.895420073619578, (25, 'log2', 25): 11.35389403722803, (25, 'log2', 50): 11.106842652619806, (25, 'log2', 100): 11.025785771703502, (25, 'log2', 250): 10.955828686812055, (25, 'log2', 500): 10.958931071291634, (25, None, 10): 12.595058178227944, (25, None, 25): 12.0473037143361, (25, None, 50): 11.916852109140915, (25, None, 100): 11.843827404211902, (25, None, 250): 11.788075221146647, (25, None, 500): 11.768973748169289}, 'tree_regressor': {(5, 'sqrt'): 21.58080340437444, (5, 'log2'): 21.58080340437444, (5, None): 15.941608658636543, (10, 'sqrt'): 16.72551575685888, (10, 'log2'): 16.72551575685888, (10, None): 15.191513428863713, (15, 'sqrt'): 16.363618727686486, (15, 'log2'): 16.363618727686486, (15, None): 15.834214920390927, (20, 'sqrt'): 15.608224577932042, (20, 'log2'): 15.608224577932042, (20, None): 16.136097288640297, (25, 'sqrt'): 16.150408561515363, (25, 'log2'): 16.150408561515363, (25, None): 16.13375888163831}, 'boost_regressor': {(5, 'sqrt', 10): 19.2666289845167, (5, 'sqrt', 25): 15.47665087037957, (5, 'sqrt', 50): 13.991259561061879, (5, 'sqrt', 100): 13.349294466903375, (5, 'sqrt', 250): 12.637931681206357, (5, 'sqrt', 500): 12.03191131047471, (5, 'log2', 10): 19.2666289845167, (5, 'log2', 25): 15.47665087037957, (5, 'log2', 50): 13.991259561061879, (5, 'log2', 100): 13.349294466903375, (5, 'log2', 250): 12.637931681206357, (5, 'log2', 500): 12.03191131047471, (5, None, 10): 17.150653445477428, (5, None, 25): 14.391095010958928, (5, None, 50): 13.514204733538424, (5, None, 100): 13.043312855269534, (5, None, 250): 12.335223508816005, (5, None, 500): 11.723804607777298, (10, 'sqrt', 10): 16.290538808134635, (10, 'sqrt', 25): 12.651459158374957, (10, 'sqrt', 50): 11.52869067071204, (10, 'sqrt', 100): 10.690412664320787, (10, 'sqrt', 250): 9.970699176169704, (10, 'sqrt', 500): 9.760365952268485, (10, 'log2', 10): 16.315735827042868, (10, 'log2', 25): 12.561480796941625, (10, 'log2', 50): 11.410461962321147, (10, 'log2', 100): 10.754596958184882, (10, 'log2', 250): 10.030249781444411, (10, 'log2', 500): 9.778621567741625, (10, None, 10): 15.543650217490264, (10, None, 25): 12.567383610450062, (10, None, 50): 11.669071146355579, (10, None, 100): 11.207301717100261, (10, None, 250): 10.788175785624784, (10, None, 500): 10.650474879061397, (15, 'sqrt', 10): 14.727268827122446, (15, 'sqrt', 25): 11.22571385022774, (15, 'sqrt', 50): 10.556100880816588, (15, 'sqrt', 100): 10.301973759780378, (15, 'sqrt', 250): 10.314460443982226, (15, 'sqrt', 500): 10.213500006051355, (15, 'log2', 10): 14.776342566821432, (15, 'log2', 25): 11.153959438129798, (15, 'log2', 50): 10.556100880816588, (15, 'log2', 100): 10.301973759780378, (15, 'log2', 250): 10.363302760959193, (15, 'log2', 500): 10.20868191508443, (15, None, 10): 15.197129224870693, (15, None, 25): 13.038514106478644, (15, None, 50): 12.77916502327349, (15, None, 100): 12.731633839040294, (15, None, 250): 12.717120287969765, (15, None, 500): 12.745784638280355, (20, 'sqrt', 10): 14.318500464267796, (20, 'sqrt', 25): 11.109204235428324, (20, 'sqrt', 50): 10.664473933587676, (20, 'sqrt', 100): 10.554135670128096, (20, 'sqrt', 250): 10.628018117057225, (20, 'sqrt', 500): 10.658072843541675, (20, 'log2', 10): 14.247317810125622, (20, 'log2', 25): 11.006909340310308, (20, 'log2', 50): 10.589705947289593, (20, 'log2', 100): 10.601700666856349, (20, 'log2', 250): 10.6216629964741, (20, 'log2', 500): 10.572768250290485, (20, None, 10): 15.624432009970324, (20, None, 25): 14.42226165563532, (20, None, 50): 14.601757297669838, (20, None, 100): 14.630442275886768, (20, None, 250): 14.61973470037511, (20, None, 500): 14.603687698460929, (25, 'sqrt', 10): 14.214077562745851, (25, 'sqrt', 25): 10.962634810260743, (25, 'sqrt', 50): 10.775632716880107, (25, 'sqrt', 100): 10.635734436840355, (25, 'sqrt', 250): 10.736711778200505, (25, 'sqrt', 500): 10.579392577418465, (25, 'log2', 10): 14.168369038078696, (25, 'log2', 25): 11.007866421964993, (25, 'log2', 50): 10.742764677249355, (25, 'log2', 100): 10.599088018677685, (25, 'log2', 250): 10.65124946007849, (25, 'log2', 500): 10.649715726404446, (25, None, 10): 15.782588898455934, (25, None, 25): 14.998780467705508, (25, None, 50): 15.331691806855078, (25, None, 100): 15.388782811743992, (25, None, 250): 15.377103836956193, (25, None, 500): 15.36579126632879}, 'ridge_regressor': {1: 15.932610211764992, 2: 15.9234680667562, 5: 15.91116653109693, 10: 15.900433703868568, 25: 15.883631041744387, 50: 15.870656199718354, 100: 15.860701486365963, 200: 15.85851237392054, 500: 15.88367327821326, 1000: 15.956487622326991}, 'lasso_regressor': {1e-05: 15.937967423038764, 0.0001: 15.935991929768106, 0.001: 15.918340950537434, 0.01: 15.857215895397399, 0.1: 15.924706857547713, 1: 16.519797532785294}}\n"
     ]
    }
   ],
   "source": [
    "max_depth = [5,10,15,20,25]\n",
    "max_features = ['sqrt', 'log2', None]\n",
    "n_estimators = [10,25,50, 100,250,500]\n",
    "\n",
    "params = list(itertools.product(max_depth, max_features, n_estimators))\n",
    "\n",
    "def run_forest(p):\n",
    "    d, f, n = p\n",
    "    return forest(X_0D, y_true, max_depth=d, max_features=f, n_estimators=n)\n",
    "\n",
    "results = parallel(run_forest, params)\n",
    "zero_d_dict['forest_regressor'] = dict(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homemade Tree Parameter Tuning \n",
    "def tree(X_df, y_df, max_depth, max_features, n_splits =5):\n",
    "    mse_sum = 0\n",
    "    for train_index, test_index in get_cv(X_df,y_df,n_splits):\n",
    "        tree_regressor = Pipeline([('imputer', Imputer(strategy='median')), \n",
    "            ('scale', StandardScaler()),\n",
    "            ('regressor', DecisionTreeRegressor(max_depth = max_depth, max_features = max_features))])\n",
    "        drop = X_df.drop(columns = ['stormid'])\n",
    "        X_train, X_test = drop.iloc[train_index,:], drop.iloc[test_index,:]\n",
    "        y_train, y_test = y_true[train_index], y_true[test_index]\n",
    "        tree_regressor.fit(X_train,y_train)\n",
    "        y_pred = tree_regressor.predict(X_test)\n",
    "        mse_sum += mean_squared_error(y_test, y_pred)\n",
    "        \n",
    "    return ((max_depth, max_features),np.sqrt(mse_sum/n_splits))\n",
    "\n",
    "def run_tree(p):\n",
    "    d, f = p\n",
    "    return tree(X_0D_zuv, y_true, max_depth=d, max_features=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started new process pool with 12 processes\n"
     ]
    }
   ],
   "source": [
    "# Generate Results\n",
    "max_depth = [5,10,15,20,25,50,100]\n",
    "max_features = [None, 'sqrt', 'log2']\n",
    "\n",
    "params = list(itertools.product(max_depth, max_features))\n",
    "\n",
    "results_tree = parallel(run_tree, params)\n",
    "zero_d_dict['tree_regressor'] = dict(results_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homemade SVR Parameter Tuning \n",
    "def svr(X_df, y_df, C, gamma, n_splits = 5):\n",
    "    mse_sum = 0\n",
    "    for train_index, test_index in get_cv(X_df,y_df,n_splits):\n",
    "        rbf_regressor = Pipeline([('imputer', Imputer(strategy='median')),\n",
    "            ('scale', StandardScaler()),\n",
    "            ('regressor', SVR(kernel = 'rbf', C = C, gamma = gamma))])\n",
    "        drop = X_df.drop(columns = ['stormid'])\n",
    "        \n",
    "        X_train, X_test = drop.iloc[train_index,:], drop.iloc[test_index,:]\n",
    "        y_train, y_test = y_true[train_index], y_true[test_index]\n",
    "        rbf_regressor.fit(X_train,y_train)\n",
    "        y_pred = rbf_regressor.predict(X_test)\n",
    "        mse_sum += mean_squared_error(y_test, y_pred)\n",
    "        \n",
    "    return ((C, gamma),np.sqrt(mse_sum/n_splits))\n",
    "\n",
    "def run_svr(p):\n",
    "    c, g = p\n",
    "    return svr(X_0D_zuv, y_true, C = c, gamma = g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started new process pool with 12 processes\n",
      "process pool interrupted, shutting down\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-43d3d0644556>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msvr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_0D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtest_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_svr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_all\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-58-7ffa0e43068f>\u001b[0m in \u001b[0;36mparallel\u001b[0;34m(fn, params, n_jobs)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'started new process pool with {} processes'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         '''\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Generate Reults\n",
    "C = np.logspace(-3, 3, num=7)\n",
    "gamma = np.logspace(-5, 2, num=8)\n",
    "\n",
    "params = list(itertools.product(C, gamma))\n",
    "\n",
    "results_rbf = parallel(run_svr, params)\n",
    "zero_d_dict['rbf_regressor'] = dict(results_rbf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homemade Boosted Forest Parameter Tuning \n",
    "def boost(X_df, y_df, max_depth, max_features, n_estimators, n_splits = 5):\n",
    "    mse_sum = 0\n",
    "    for train_index, test_index in get_cv(X_df,y_df,5):\n",
    "        forest_regressor = Pipeline([('imputer', Imputer(strategy='median')), \n",
    "            ('scale', StandardScaler()),       \n",
    "            ('regressor', GradientBoostingRegressor(max_depth = max_depth, max_features = max_features, n_estimators = n_estimators))])\n",
    "        drop = X_df.drop(columns = ['stormid'])\n",
    "\n",
    "        X_train, X_test = drop.iloc[train_index,:], drop.iloc[test_index,:]\n",
    "        y_train, y_test = y_df[train_index], y_df[test_index]\n",
    "\n",
    "        forest_regressor.fit(X_train,y_train)\n",
    "        y_pred = forest_regressor.predict(X_test)\n",
    "        mse_sum += mean_squared_error(y_test, y_pred)\n",
    "        \n",
    "    return ((max_depth, max_features, n_estimators),np.sqrt(mse_sum/n_splits))\n",
    "\n",
    "def run_boost(p):\n",
    "    d, f, n = p\n",
    "    return boost(X_0D_zuv, y_true, max_depth=d, max_features=f, n_estimators=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started new process pool with 12 processes\n"
     ]
    }
   ],
   "source": [
    "# Generate Results\n",
    "max_depth = [5,10,15,20,25]\n",
    "max_features = ['sqrt', 'log2', None]\n",
    "n_estimators = [10,25,50,100,250,500]\n",
    "\n",
    "params = list(itertools.product(max_depth, max_features, n_estimators))\n",
    "\n",
    "results_boost = parallel(run_boost, params)\n",
    "zero_d_dict['boost_regressor'] = dict(results_boost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homemade Lasso Parameter Tuning \n",
    "def lasso(X_df, y_df, alpha, n_splits = 5):\n",
    "    lasso_dict = {}\n",
    "    for a in alpha:\n",
    "        mse_sum = 0\n",
    "        for train_index, test_index in get_cv(X_df,y_df,n_splits):\n",
    "            lasso_regressor = Pipeline([('imputer', Imputer(strategy='median')),\n",
    "                ('scale', StandardScaler()),\n",
    "                ('regressor', Lasso(alpha = a))])\n",
    "            drop = X_df.drop(columns = ['stormid'])\n",
    "\n",
    "            X_train, X_test = drop.iloc[train_index,:], drop.iloc[test_index,:]\n",
    "            y_train, y_test = y_true[train_index], y_true[test_index]\n",
    "\n",
    "            lasso_regressor.fit(X_train,y_train)\n",
    "            y_pred = lasso_regressor.predict(X_test)\n",
    "            mse_sum += mean_squared_error(y_test, y_pred)\n",
    "\n",
    "        lasso_dict[a] = np.sqrt(mse_sum/n_splits)\n",
    "    return lasso_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Results\n",
    "alpha = [.00001, .0001, .001, .01, .1, 1]\n",
    "zero_d_dict['lasso_regressor'] = lasso(X_0D, y_true, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homemade Ridge Parameter Tuning \n",
    "def ridge(X_df, y_df, alpha, n_splits = 5):\n",
    "    ridge_dict = {}\n",
    "    for a in alpha:\n",
    "        mse_sum = 0\n",
    "        for train_index, test_index in get_cv(X_df,y_df,n_splits):\n",
    "            ridge_regressor = Pipeline([('imputer', Imputer(strategy='median')),\n",
    "                ('scale', StandardScaler()),\n",
    "                ('regressor', Ridge(alpha = a))])\n",
    "            drop = X_df.drop(columns = ['stormid'])\n",
    "\n",
    "            X_train, X_test = drop.iloc[train_index,:], drop.iloc[test_index,:]\n",
    "            y_train, y_test = y_true[train_index], y_true[test_index]\n",
    "\n",
    "            ridge_regressor.fit(X_train,y_train)\n",
    "            y_pred = ridge_regressor.predict(X_test)\n",
    "            mse_sum += mean_squared_error(y_test, y_pred)\n",
    "\n",
    "        ridge_dict[a] = np.sqrt(mse_sum/n_splits)\n",
    "    return ridge_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = [1, 2, 5, 10, 25, 50, 100, 200, 500, 1000]\n",
    "zero_d_dict['ridge_regressor'] = ridge(X_0D, y_true, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ridge_regressor': {1: 16.6271535325568, 2: 16.627150575724325, 5: 16.627145639511117, 10: 16.62715036124462, 25: 16.6272585695048, 50: 16.62773358299904, 100: 16.62966375531627, 200: 16.63667575138488, 500: 16.673449608004624, 1000: 16.76053834539504}, 'lasso_regressor': {1e-05: 16.627157187430107, 0.0001: 16.62715754334873, 0.001: 16.62716334419993, 0.01: 16.627455739611218, 0.1: 16.65497769119576, 1: 17.051883574861765}, 'tree_regressor': {(5, None): 15.941608658636543, (5, 'sqrt'): 21.141863700287818, (5, 'log2'): 21.141863700287818, (10, None): 15.24781333802893, (10, 'sqrt'): 17.399240807878297, (10, 'log2'): 17.399240807878297, (15, None): 15.758919697561016, (15, 'sqrt'): 15.829055480627154, (15, 'log2'): 15.829055480627154, (20, None): 15.998055685081027, (20, 'sqrt'): 15.660803992592191, (20, 'log2'): 15.660803992592191, (25, None): 16.175074486227732, (25, 'sqrt'): 16.34489918411493, (25, 'log2'): 16.34489918411493, (50, None): 16.10410767288366, (50, 'sqrt'): 16.576701576142604, (50, 'log2'): 16.576701576142604, (100, None): 16.10410767288366, (100, 'sqrt'): 16.576701576142604, (100, 'log2'): 16.576701576142604}, 'boost_regressor': {(5, 'sqrt', 10): 19.00683992897514, (5, 'sqrt', 25): 15.442236168404527, (5, 'sqrt', 50): 14.086251923263664, (5, 'sqrt', 100): 13.348736438320527, (5, 'sqrt', 250): 12.62511897030908, (5, 'sqrt', 500): 12.034868456463647, (5, 'log2', 10): 19.00683992897514, (5, 'log2', 25): 15.442236168404527, (5, 'log2', 50): 14.086251923263664, (5, 'log2', 100): 13.348736438320527, (5, 'log2', 250): 12.62511897030908, (5, 'log2', 500): 12.034868456463647, (5, None, 10): 17.150739868196585, (5, None, 25): 14.390866028108503, (5, None, 50): 13.513103329374596, (5, None, 100): 13.043202445191755, (5, None, 250): 12.339092225542988, (5, None, 500): 11.72747890773129, (10, 'sqrt', 10): 16.369068277262983, (10, 'sqrt', 25): 12.693084193421827, (10, 'sqrt', 50): 11.453100707400699, (10, 'sqrt', 100): 10.740400875963422, (10, 'sqrt', 250): 10.007017294208481, (10, 'sqrt', 500): 9.772681767969575, (10, 'log2', 10): 16.340689078792373, (10, 'log2', 25): 12.664679723452418, (10, 'log2', 50): 11.405909186062651, (10, 'log2', 100): 10.796995863280777, (10, 'log2', 250): 9.986260532697228, (10, 'log2', 500): 9.744595411349081, (10, None, 10): 15.55328749182691, (10, None, 25): 12.594653496635217, (10, None, 50): 11.664449373410465, (10, None, 100): 11.155842986317365, (10, None, 250): 10.789049513802807, (10, None, 500): 10.69115916211471, (15, 'sqrt', 10): 14.7050224005368, (15, 'sqrt', 25): 11.212227608957697, (15, 'sqrt', 50): 10.518288622483814, (15, 'sqrt', 100): 10.266326098759397, (15, 'sqrt', 250): 10.351060208659913, (15, 'sqrt', 500): 10.335366631748142, (15, 'log2', 10): 14.739225681723173, (15, 'log2', 25): 11.239564113333586, (15, 'log2', 50): 10.518288622483814, (15, 'log2', 100): 10.266326098759397, (15, 'log2', 250): 10.2916195059102, (15, 'log2', 500): 10.199604527518819, (15, None, 10): 15.172871760122426, (15, None, 25): 12.982863019687674, (15, None, 50): 12.804414754579813, (15, None, 100): 12.721271622385952, (15, None, 250): 12.717872685509754, (15, None, 500): 12.727439452858391, (20, 'sqrt', 10): 14.340411285034843, (20, 'sqrt', 25): 11.061940402354814, (20, 'sqrt', 50): 10.612818357531864, (20, 'sqrt', 100): 10.599020780120824, (20, 'sqrt', 250): 10.594923873332014, (20, 'sqrt', 500): 10.677575169563529, (20, 'log2', 10): 14.299770467108575, (20, 'log2', 25): 10.99926361185862, (20, 'log2', 50): 10.611242191377622, (20, 'log2', 100): 10.640743389901706, (20, 'log2', 250): 10.613040295966917, (20, 'log2', 500): 10.65115471656446, (20, None, 10): 15.625114900820998, (20, None, 25): 14.445879644244524, (20, None, 50): 14.61982563605548, (20, None, 100): 14.609426146079038, (20, None, 250): 14.612497269263084, (20, None, 500): 14.668681750520442, (25, 'sqrt', 10): 14.24393674858707, (25, 'sqrt', 25): 10.994202992109907, (25, 'sqrt', 50): 10.73435847576166, (25, 'sqrt', 100): 10.64383544954568, (25, 'sqrt', 250): 10.62813081220283, (25, 'sqrt', 500): 10.668259943657919, (25, 'log2', 10): 14.15837468262506, (25, 'log2', 25): 11.00257947480385, (25, 'log2', 50): 10.737397253374999, (25, 'log2', 100): 10.67371159177355, (25, 'log2', 250): 10.60395726741342, (25, 'log2', 500): 10.70155125650311, (25, None, 10): 15.755796141432082, (25, None, 25): 14.994064901004949, (25, None, 50): 15.335612173921556, (25, None, 100): 15.396472955775469, (25, None, 250): 15.365242052685073, (25, None, 500): 15.385239476464186}, 'forest_regressor': {(5, 'sqrt', 10): 17.41505097083192, (5, 'sqrt', 25): 17.57600564260812, (5, 'sqrt', 50): 17.418217393669508, (5, 'sqrt', 100): 17.391244257822596, (5, 'sqrt', 250): 17.263172287885936, (5, 'sqrt', 500): 17.2519530747091, (5, 'log2', 10): 17.41505097083192, (5, 'log2', 25): 17.57600564260812, (5, 'log2', 50): 17.418217393669508, (5, 'log2', 100): 17.391244257822596, (5, 'log2', 250): 17.263172287885936, (5, 'log2', 500): 17.2519530747091, (5, None, 10): 15.531966281617958, (5, None, 25): 15.52322425859352, (5, None, 50): 15.495562032567733, (5, None, 100): 15.47850971848783, (5, None, 250): 15.468644943766844, (5, None, 500): 15.485014667469407, (10, 'sqrt', 10): 14.172701561797307, (10, 'sqrt', 25): 13.933482493838005, (10, 'sqrt', 50): 13.850762571592373, (10, 'sqrt', 100): 13.802312435494564, (10, 'sqrt', 250): 13.806369416673764, (10, 'sqrt', 500): 13.801773222510917, (10, 'log2', 10): 14.108000747985104, (10, 'log2', 25): 13.95389612301635, (10, 'log2', 50): 13.868476554511368, (10, 'log2', 100): 13.82520725400644, (10, 'log2', 250): 13.83498407512651, (10, 'log2', 500): 13.797664681210184, (10, None, 10): 13.494281090337985, (10, None, 25): 13.361051267886596, (10, None, 50): 13.269163828974126, (10, None, 100): 13.237744308183226, (10, None, 250): 13.227449998267673, (10, None, 500): 13.212619699004538, (15, 'sqrt', 10): 12.428233401377106, (15, 'sqrt', 25): 12.097915394278965, (15, 'sqrt', 50): 11.905560573162777, (15, 'sqrt', 100): 11.909275570459767, (15, 'sqrt', 250): 11.837880728466416, (15, 'sqrt', 500): 11.817521479807821, (15, 'log2', 10): 12.378157448013582, (15, 'log2', 25): 11.975792759881564, (15, 'log2', 50): 11.941642219387912, (15, 'log2', 100): 11.867662198848295, (15, 'log2', 250): 11.829708004115806, (15, 'log2', 500): 11.813087660232483, (15, None, 10): 12.71159508030578, (15, None, 25): 12.297798845495075, (15, None, 50): 12.193953988691927, (15, None, 100): 12.157566485722198, (15, None, 250): 12.11699120974591, (15, None, 500): 12.091730486452528, (20, 'sqrt', 10): 11.900087262931827, (20, 'sqrt', 25): 11.409340384974527, (20, 'sqrt', 50): 11.217587720974924, (20, 'sqrt', 100): 11.155175062412221, (20, 'sqrt', 250): 11.123041659410546, (20, 'sqrt', 500): 11.09961091765332, (20, 'log2', 10): 11.84518695157379, (20, 'log2', 25): 11.384710173279068, (20, 'log2', 50): 11.252098665565759, (20, 'log2', 100): 11.180031404166945, (20, 'log2', 250): 11.12354313138684, (20, 'log2', 500): 11.106667138968863, (20, None, 10): 12.504587057376286, (20, None, 25): 12.007132038168146, (20, None, 50): 11.925379772566973, (20, None, 100): 11.880522197251988, (20, None, 250): 11.824142459959974, (20, None, 500): 11.801551192639177, (25, 'sqrt', 10): 11.815012874544898, (25, 'sqrt', 25): 11.338282396623452, (25, 'sqrt', 50): 11.106768062315126, (25, 'sqrt', 100): 11.054317603777694, (25, 'sqrt', 250): 11.013452186835089, (25, 'sqrt', 500): 10.961161704397576, (25, 'log2', 10): 11.895420073619578, (25, 'log2', 25): 11.35389403722803, (25, 'log2', 50): 11.106842652619806, (25, 'log2', 100): 11.025785771703502, (25, 'log2', 250): 10.955828686812055, (25, 'log2', 500): 10.958931071291634, (25, None, 10): 12.595058178227944, (25, None, 25): 12.0473037143361, (25, None, 50): 11.916852109140915, (25, None, 100): 11.843827404211902, (25, None, 250): 11.788075221146647, (25, None, 500): 11.768973748169289}}\n"
     ]
    }
   ],
   "source": [
    "zero_d_dict['tree_regressor'] = dict(results_tree)\n",
    "zero_d_dict['boost_regressor'] = dict(results_boost)\n",
    "zero_d_dict['forest_regressor'] = dict(results_forest)\n",
    "print(zero_d_dict)\n",
    "np.save('storm_forecast_zero_d_tuning.npy', zero_d_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the data is a list of time instants (one every 6h). The first storm will result in x lines beginning with its stormid and the corresponding time step, with all the associated features on the same row. Then the time steps from the second storm will be below, and so on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of storms in the local training set: {}'.format( len(set(data_train['stormid'])) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of time steps in the local training set: {}'.format(y_train.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 0D features from track data\n",
    "\n",
    "A set of simple features has been extracted for each storm at each time point: \n",
    "\n",
    "- latitude, longitude: in degrees\n",
    "- windspeed: current (max) windspeed (knots) \n",
    "- hemisphere:  South=0, North=1\n",
    "- Jday predictor:  Gaussian function of (Julian day of storm init - peak day of the hurricane season), see (1)\n",
    "- initial_max_wind: initial (max) windspeed of the storm \n",
    "- max_wind_change_12h: last 12h (max) windspeed change\n",
    "- basin = based on the present location: \n",
    "       0 = NA - North Atlantic / 1 = SA - South Atlantic    / 2 = WP - West Pacific       / 3 = EP - East Pacific /\n",
    "       4 = SP - South Pacific  / 5 = NI - North Indian      / 6 = SI - South Indian       / 7 = AS - Arabian Sea /\n",
    "       8 = BB - Bay of Bengal  / 9 = EA - Eastern Australia / 10 = WA - Western Australia / 11 = CP - Central Pacific\n",
    "       12 = CS - Carribbean Sea/ 13 = GM - Gulf of Mexico   / 14 = MM - Missing\n",
    "- nature = nature of the storm  \n",
    "       0 = TS - Tropical / 1 = SS - Subtropical / 2 = ET - Extratropical / 3 = DS - Disturbance /\n",
    "       4 = MX - Mix of conflicting reports / 5 = NR - Not Reported / 6 = MM - Missing / 7 =  - Missing\n",
    "- dist2land = current distance to the land (km)\n",
    "\n",
    "\n",
    "(1) DeMaria, Mark, et al. \"Further improvements to the statistical hurricane intensity prediction scheme (SHIPS).\" Weather and Forecasting 20.4 (2005): 531-543. https://journals.ametsoc.org/doi/full/10.1175/WAF862.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15777, 28)\n",
      "(15777,)\n"
     ]
    }
   ],
   "source": [
    "# Simple Regression With Only 0D Features:\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(test.shape)\n",
    "print(y_true.shape)\n",
    "# Data Exploration: There are now 11832 training examples and 3945 test examples\n",
    "X_train, X_test, y_train, y_test = train_test_split(test, y_true)\n",
    "\n",
    "#X_train = X_train.loc[:,'latitude':'dist2land']\n",
    "#X_test = X_test.loc[:,'latitude':'dist2land']\n",
    "#X_train = preprocessing.scale(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([    1,     2,     3, ..., 15771, 15774, 15775]), array([    0,     4,     6, ..., 15772, 15773, 15776]))\n",
      "(array([    0,     4,     6, ..., 15772, 15773, 15776]), array([    1,     2,     3, ..., 15771, 15774, 15775]))\n",
      "RandomizedSearchCV(cv=5, error_score='raise',\n",
      "          estimator=Pipeline(memory=None,\n",
      "     steps=[('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('regressor', Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=None,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False))]),\n",
      "          fit_params=None, iid=True, n_iter=2, n_jobs=None,\n",
      "          param_distributions={'regressor__alpha': [1, 10]},\n",
      "          pre_dispatch='2*n_jobs', random_state=1, refit=True,\n",
      "          return_train_score='warn', scoring='neg_mean_squared_error',\n",
      "          verbose=0)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-8bd560a4a373>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrbf_regressor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_distributions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrid_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'neg_mean_squared_error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;31m#print(grid.best_score_)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m#grid.best_params_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    638\u001b[0m                                   error_score=self.error_score)\n\u001b[1;32m    639\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[0;32m--> 640\u001b[0;31m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \"\"\"\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    211\u001b[0m                 Xt, fitted_transformer = fit_transform_one_cached(\n\u001b[1;32m    212\u001b[0m                     \u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                     **fit_params_steps[name])\n\u001b[0m\u001b[1;32m    214\u001b[0m                 \u001b[0;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;31m# transformer. This is necessary when loading the transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, weight, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    579\u001b[0m                        **fit_params):\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/imputation.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    164\u001b[0m                                                    \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m                                                    \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissing_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m                                                    self.axis)\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/imputation.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[0;34m(self, X, strategy, missing_values, axis)\u001b[0m\n\u001b[1;32m    271\u001b[0m                 masked_X.mask = np.logical_or(masked_X.mask,\n\u001b[1;32m    272\u001b[0m                                               np.isnan(X))\n\u001b[0;32m--> 273\u001b[0;31m             \u001b[0mmedian_masked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasked_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m             \u001b[0;31m# Avoid the warning \"Warning: converting a masked element to nan.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0mmedian\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmedian_masked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/ma/extras.py\u001b[0m in \u001b[0;36mmedian\u001b[0;34m(a, axis, out, overwrite_input, keepdims)\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m     r, k = _ureduce(a, func=_median, axis=axis, out=out,\n\u001b[0;32m--> 694\u001b[0;31m                     overwrite_input=overwrite_input)\n\u001b[0m\u001b[1;32m    695\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_ureduce\u001b[0;34m(a, func, **kwargs)\u001b[0m\n\u001b[1;32m   3248\u001b[0m         \u001b[0mkeepdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3250\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3251\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/ma/extras.py\u001b[0m in \u001b[0;36m_median\u001b[0;34m(a, axis, out, overwrite_input)\u001b[0m\n\u001b[1;32m    713\u001b[0m             \u001b[0masorted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m         \u001b[0masorted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/ma/core.py\u001b[0m in \u001b[0;36msort\u001b[0;34m(a, axis, kind, order, endwith, fill_value)\u001b[0m\n\u001b[1;32m   6710\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6711\u001b[0m         a.sort(axis=axis, kind=kind, order=order,\n\u001b[0;32m-> 6712\u001b[0;31m                endwith=endwith, fill_value=fill_value)\n\u001b[0m\u001b[1;32m   6713\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6714\u001b[0m         \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/ma/core.py\u001b[0m in \u001b[0;36msort\u001b[0;34m(self, axis, kind, order, endwith, fill_value)\u001b[0m\n\u001b[1;32m   5560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5561\u001b[0m         sidx = self.argsort(axis=axis, kind=kind, order=order,\n\u001b[0;32m-> 5562\u001b[0;31m                             fill_value=fill_value, endwith=endwith)\n\u001b[0m\u001b[1;32m   5563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5564\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake_along_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msidx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/ma/core.py\u001b[0m in \u001b[0;36margsort\u001b[0;34m(self, axis, kind, order, endwith, fill_value)\u001b[0m\n\u001b[1;32m   5407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5408\u001b[0m         \u001b[0mfilled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5409\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfilled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5411\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0margmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# SVR with RBF Kernel:\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "rbf_regressor = Pipeline([('scale', StandardScaler()),\n",
    "            ('imputer', Imputer(strategy='median')),\n",
    "            ('regressor', Lasso())])\n",
    "\n",
    "'''\n",
    "rbf_regressor.fit(X_train,y_train)\n",
    "y_pred = rbf_regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(mse)\n",
    "'''\n",
    "\n",
    "\n",
    "#grid_list = {\"regressor__C\": np.logspace(-3, 3, num=7), \"regressor__gamma\": np.logspace(-5, 2, num=8)}\n",
    "grid_list = { \n",
    "    'regressor__alpha': [1, 10]\n",
    "}\n",
    "folds = get_cv(data_train, y_true)\n",
    "for f in folds:\n",
    "    print(f)\n",
    "grid = RandomizedSearchCV(estimator=rbf_regressor, param_distributions=grid_list, n_jobs = None, random_state = 1, n_iter=2, scoring = 'neg_mean_squared_error', cv = 5)\n",
    "print(grid)\n",
    "grid.fit(tmp,y_true)\n",
    "#print(grid.best_score_)\n",
    "#grid.best_params_\n",
    "\n",
    "'''\n",
    "param_tuning['rbf_regressor'] = (grid.best_score_,grid.best_params_)\n",
    "np.save('storm_forecast_parameter_tuning.npy', param_tuning)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "poly_regressor = Pipeline([('scale', StandardScaler()),\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('regressor', SVR('poly'))])\n",
    "\n",
    "poly_regressor.fit(X_train,y_train)\n",
    "y_pred = poly_regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(mse)\n",
    "\n",
    "\n",
    "grid_list = {\"regressor__C\": np.logspace(-3, 3, num=7), \"regressor__gamma\": np.logspace(-5, 2, num=8)}\n",
    "grid = RandomizedSearchCV(estimator=poly_regressor, param_distributions=grid_list, n_jobs = -1, random_state = 1, n_iter=50, scoring = 'neg_mean_squared_error', cv = 5)\n",
    "grid.fit(X_train,y_train)\n",
    "print(grid.best_score_)\n",
    "grid.best_params_\n",
    "\n",
    "param_tuning['poly_regressor'] = (grid.best_score_,grid.best_params_)\n",
    "np.save('storm_forecast_parameter_tuning.npy', param_tuning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264.26308266441345\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [15777, 3945]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-bda75d46fe3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtree_regressor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_distributions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrid_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'neg_mean_squared_error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_cv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m#param_tuning['tree_regressor'] = (grid.best_score_,grid.best_params_)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mrefit_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'score'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 616\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    617\u001b[0m         \u001b[0mn_splits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_n_splits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m         \u001b[0;31m# Regenerate parameter iterable for each fit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 204\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [15777, 3945]"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "tree_regressor = Pipeline([('scale', StandardScaler()),\n",
    "            ('imputer', Imputer(strategy='median')),\n",
    "            ('regressor', DecisionTreeRegressor())])\n",
    "\n",
    "tree_regressor.fit(X_train,y_train)\n",
    "y_pred = tree_regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(mse)\n",
    "\n",
    "\n",
    "grid_list = {\"regressor__max_depth\": [None,5,10,15,20,25],'regressor__max_features': [None, 'sqrt', 'log2']}\n",
    "grid = RandomizedSearchCV(estimator=tree_regressor, param_distributions=grid_list, n_jobs = -1, random_state = 1, n_iter=10, scoring = 'neg_mean_squared_error', cv = get_cv(test,y_true))\n",
    "\n",
    "grid.fit(tmp,y_pred)\n",
    "\n",
    "#param_tuning['tree_regressor'] = (grid.best_score_,grid.best_params_)\n",
    "#np.save('storm_forecast_parameter_tuning.npy', param_tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "forest_regressor = Pipeline([('scale', StandardScaler()),\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('regressor', RandomForestRegressor())])\n",
    "\n",
    "forest_regressor.fit(X_train,y_train)\n",
    "y_pred = forest_regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(mse)\n",
    "\n",
    "\n",
    "grid_list = { \n",
    "    'regressor__n_estimators': [10,25,50,100,250,500,1000],\n",
    "    'regressor__max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'regressor__max_depth' : [None,5,10,15,20,25,50,75,100],\n",
    "}\n",
    "grid = RandomizedSearchCV(estimator=forest_regressor, param_distributions=grid_list, n_jobs = -1, random_state = 1, n_iter=50, scoring = 'neg_mean_squared_error', cv = 5)\n",
    "grid.fit(X_train,y_train)\n",
    "\n",
    "param_tuning['forest_regressor'] = (grid.best_score_,grid.best_params_)\n",
    "np.save('storm_forecast_parameter_tuning_basin+nature.npy', param_tuning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(param_tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "boost_regressor = Pipeline([('scale', StandardScaler()),\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('regressor', GradientBoostingRegressor())])\n",
    "\n",
    "boost_regressor.fit(X_train,y_train)\n",
    "y_pred = boost_regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(mse)\n",
    "\n",
    "grid_list = { \n",
    "    'regressor__n_estimators': [10,25,50,100,250,500,1000],\n",
    "    'regressor__max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'regressor__max_depth' : [None,3,5,10,15,20,25,50,75,100],\n",
    "}\n",
    "grid = RandomizedSearchCV(estimator=forest_regressor, param_distributions=grid_list, n_jobs = -1, random_state = 1, n_iter=50, scoring = 'neg_mean_squared_error', cv = 5)\n",
    "grid.fit(X_train,y_train)\n",
    "\n",
    "param_tuning['boost_regressor'] = (grid.best_score_,grid.best_params_)\n",
    "np.save('storm_forecast_parameter_tuning_basin+nature.npy', param_tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "elastic_regressor = Pipeline([('scale', StandardScaler()),\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('regressor', ElasticNet())])\n",
    "\n",
    "elastic_regressor.fit(X_train,y_train)\n",
    "y_pred = elasticregressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "[1, 50, 100, 200, 1000]\n",
    "\n",
    "ridge_regressor = Pipeline([('scale', StandardScaler()),\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('regressor', Ridge())])\n",
    "\n",
    "ridge_regressor.fit(X_train,y_train)\n",
    "y_pred = ridge_regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(mse)\n",
    "\n",
    "grid_list = { \n",
    "    'regressor__alpha': [1, 2, 5, 10, 25, 50, 100, 200, 500, 1000]\n",
    "}\n",
    "grid = RandomizedSearchCV(estimator=ridge_regressor, param_distributions=grid_list, n_jobs = -1, random_state = 1, n_iter=50, scoring = 'neg_mean_squared_error', cv = 5)\n",
    "grid.fit(X_train,y_train)\n",
    "\n",
    "param_tuning['ridge_regressor'] = (grid.best_score_,grid.best_params_)\n",
    "np.save('storm_forecast_parameter_tuning.npy', param_tuning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "lasso_regressor = Pipeline([('scale', StandardScaler()),\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('regressor', Lasso())])\n",
    "\n",
    "lasso_regressor.fit(X_train,y_train)\n",
    "y_pred = lasso_regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(mse)\n",
    "\n",
    "grid_list = { \n",
    "    'regressor__alpha': [.00001, .0001, .001, .01, .1, 1]\n",
    "}\n",
    "grid = RandomizedSearchCV(estimator=lasso_regressor, param_distributions=grid_list, n_jobs = -1, random_state = 1, n_iter=50, scoring = 'neg_mean_squared_error', cv = 5)\n",
    "grid.fit(X_train,y_train)\n",
    "\n",
    "param_tuning['lasso_regressor'] = (grid.best_score_,grid.best_params_)\n",
    "np.save('storm_forecast_parameter_tuning.npy', param_tuning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "regressor = Pipeline([('scale', StandardScaler()),\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('regressor', RandomForestRegressor())])\n",
    "\n",
    "regressor.fit(X_train,y_train)\n",
    "y_pred = regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(mse)\n",
    "\n",
    "grid_list = { \n",
    "    'n_estimators': [50, 100, 200, 300, 400, 500],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth' : [5,10,20],\n",
    "    'criterion' :['gini', 'entropy']\n",
    "}\n",
    "\n",
    "grid = RandomizedSearchCV(estimator=regressor, param_distributions=grid_list, n_jobs = -1, random_state = 1, n_iter=100, scoring = 'neg_mean_squared_error', cv = 5)\n",
    "print(grid)\n",
    "grid.fit(X_train,y_train)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "grid_list = {\"C\": [1,2,3],\n",
    "             \"gamma\": [.1,.2,.3]}\n",
    "\n",
    "regressor = Pipeline([('scale', StandardScaler()),\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('regressor', SVR(kernel = 'rbf', gamma = .1, C = 1))])\n",
    "regressor.fit(X_train,y_train)\n",
    "y_pred = regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(mse)\n",
    "\n",
    "grid_list = {\"regressor__C\": np.logspace(-2, 2, num=5), \"regressor__gamma\": np.logspace(-4, 0, num=5)}\n",
    "\n",
    "grid = RandomizedSearchCV(estimator=regressor, param_distributions=grid_list, n_jobs = -1, random_state = 1, n_iter=100, scoring = 'neg_mean_squared_error', cv = 5)\n",
    "print(grid)\n",
    "grid.fit(X_train,y_train)\n",
    "print(grid.best_score_)\n",
    "\n",
    "#print(grid.best_estimator_)\n",
    "#print(grid.best_estimator_.regressor__gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. The reanalysis data\n",
    "\n",
    "At each time step, we extracted 7 grids (11x11 pixels) of meteorological parameters centered on the current storm location. Their choice is based on the forecast literature, on personal experience and on known hypothesis of storm strengthening.\n",
    "\n",
    "#### a) 25x25 degree z, u and v at 700hPa-level\n",
    "First, we provide 3 maps of 25 x 25 degrees (lat/long) at 700hPa-level pressure: the altitude `z`, the u-wind `u` (positive if wind from the West) and the v-wind `v` (positive if wind from the South). These grids are subsampled to 11x11 pixels (1 pixel ~=2 degrees).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "sample_id=20 # sample number plotted - you can change it to see other storms and other instants\n",
    "grid_l=11 # size of all 2D-grids (in pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_25x25=['z','u','v']\n",
    "plt.figure(figsize=(10,4))\n",
    "for p,param in enumerate(params_25x25):\n",
    "    image=np.zeros([grid_l,grid_l])\n",
    "    for i in range(grid_l):\n",
    "         for j in range(grid_l):\n",
    "            image[i,j]=data_train[param+'_'+str(i)+'_'+str(j)][sample_id]\n",
    "    plt.subplot(1,3,p+1)\n",
    "    plt.imshow(np.array(image),extent=[-12,12,-12,12],\n",
    "               interpolation='nearest', origin='lower', cmap='seismic')\n",
    "    plt.xlabel('param '+param)\n",
    "t=plt.suptitle('Example of 700-hPa level maps 25x25 degrees, centered in the storm location.'\n",
    "         +'\\n (altitude, u-wind and v-wind)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) 11x11 degree sst, slp, humidity at 1000hPa,  and vorticity at 700hPa\n",
    "We provide some more localized maps of 11 x 11 degrees (lat/long) at the surface: the sea surface temperature `sst`, surface level pressure `slp`, the relative humidity `hum` at 1000hPa (near surface). These grids are sampled to 11x11 pixels (1 pixel = 1 degree). We also provide the vorticity at 700hPa `vo700`. \n",
    "\n",
    "NB: `sst` is only defined on the sea, so land has NaNs values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_11x11=['sst','slp','hum','vo700']\n",
    "plt.figure(figsize=(10,3))\n",
    "for p,param in enumerate(params_11x11):\n",
    "    image=np.zeros([grid_l,grid_l])\n",
    "    for i in range(grid_l):\n",
    "         for j in range(grid_l):\n",
    "            image[i,j]=data_train[param+'_'+str(i)+'_'+str(j)][sample_id]\n",
    "    plt.subplot(1,4,p+1)\n",
    "    plt.imshow(np.array(image),extent=[-5,5,-5,5],\n",
    "               interpolation='nearest', origin='lower', cmap='seismic')\n",
    "    plt.xlabel('param '+param)\n",
    "t=plt.suptitle('Example of 11x11 degrees maps, centered in the storm location.'\n",
    "         +'\\n (surf. temp., surf. pressure, 1000hPa humidity and 700hPa vorticity)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/sophiegif/ramp_kit_storm_forecast_new/blob/master/figures_pynb/pipeline.png?raw=true\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For submitting at the [RAMP site](http://ramp.studio), you will have to write two classes, saved in a specific file:   \n",
    "\n",
    "* a class `FeatureExtractor` in a `feature_extractor.py` file.\n",
    "* a class `Regressor` in a `regressor.py` file.\n",
    "\n",
    "You can look at the simple examples provided in /submissions:\n",
    "- starting_kit : using only the track data\n",
    "- starting_kit_pressure_map : using both track data and image data\n",
    "\n",
    "### Using data from previous time steps\n",
    "Of course, you can use the data from previous time steps, e.g., for the prediction of the intensity of storm S at t=3 you can use data from S at t=\\[0:2\\]. However, it is completely forbidden (and we check it!) to use future data like S at t=4,.. This is illustrated in the figure below, where the estimation of the 24h-forecast of the time instant 2 (red line) can use blue but not red features.\n",
    "\n",
    "- `illegal_lookahead`: this simple submission illustrates the error you will have if you are illegally looking ahead time of the same storm.\n",
    "- `legal_lookbefore` : this simple submission illustrates how to use information from previous time steps of the same storm.\n",
    "\n",
    "<img src=\"https://github.com/sophiegif/ramp_kit_storm_forecast_new/blob/master/figures_pynb/illegal_lookahead.png?raw=true\" width=\"70%\">\n",
    "<div style=\"text-align: center\">Data from previous steps are allowed, but data from future steps are forbidden.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "The framework is evaluated with a cross-validation approach. The metric used is the RMSE (root mean square error) in knots across all storm time instants. We also made visible three other metrics: `mae` is the mean absolute error, in knots. `mae_hurr` is the MAE using only time instants corresponding to hurricanes (windspeed>64 knots), while `rel_mae_hurr` is the relative RMSE on hurricanes. These metrics are interesting because the current forecasting practice is to exclude all other stages of development (e.g., extratropical, tropical wave...), see [this page](https://www.nhc.noaa.gov/verification/verify5.shtml?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the submission\n",
    "You can test locally our pipeline using `ramp_test_submission` command line (`-h` will give you all infos). For that, open a terminal in your `storm_forecast/` folder and type on a terminal `ramp_test_submission --submission starting_kit`. You can then copy a submission example in `submissions/<YOUR_SUBMISSION_NAME>/`and modify its codes as you want. Finally, test it on your computer with `ramp_test_submission --submission <YOUR_SUBMISSION_NAME>`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get to see the train and test scores, and no errors, then you can submit your model to the ramp.studio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some warnings when building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "\n",
    " <ul>\n",
    "  <li>If you want to use the features from previous time steps in your learning (for example using LSTMs), you will have to use the 'stormid' and the 'instant_t' columns. Moreover, you will have to handle separetly the first time steps, which are not provided with past data.</li>\n",
    "  <li>The intensity value to predict is the max windspeed. However, this value was measured empirically with a precision of ~5knots. </li>\n",
    "</ul> \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting to the online challenge: ramp.studio\n",
    "\n",
    "Once you have found a good model, you can submit them to [ramp.studio](http://www.ramp.studio) to enter the online challenge. First, if it is your first time using the RAMP platform, [sign up](http://www.ramp.studio/sign_up), otherwise [log in](http://www.ramp.studio/login). Then sign up to the event [storm_forecast_CI2018](http://www.ramp.studio/events/storm_forecast_CI2018). Sign up for the event. Both signups are controled by RAMP administrators, so there **can be a delay between asking for signup and being able to submit**.\n",
    "\n",
    "Once your signup request is accepted, you can go to your [sandbox](http://www.ramp.studio/events/storm_forecast_CI2018/sandbox) and copy-paste (or upload) [`feature_extractor.py`](/edit/submissions/starting_kit/feature_extractor.py) and [`classifier.py`](/edit/submissions/starting_kit/classifier.py). Save it, rename it, then submit it. The submission is trained and tested on our backend in the similar way as `ramp_test_submission` does it locally. While your submission is waiting in the queue and being trained, you can find it in the \"New submissions (pending training)\" table in [my submissions](http://www.ramp.studio/events/storm_forecast_CI2018/my_submissions). Once it is trained your submission shows up on the [public leaderboard](http://www.ramp.studio/events/storm_forecast_CI2018/leaderboard). \n",
    "If there is an error (despite having tested your submission locally with `ramp_test_submission`), it will show up in the \"Failed submissions\" table in [my submissions](http://www.ramp.studio/events/storm_forecast_CI2018/my_submissions). You can click on the error to see part of the trace.\n",
    "\n",
    "After submission, do not forget to give credit to the previous submissions you reused or integrated into your submission.\n",
    "\n",
    "The data set we use on the backend is usually different from what you find in the starting kit, so the score may be different.\n",
    "\n",
    "The official score in this RAMP (the first score column after \"historical contributivity\" on the [leaderboard](http://www.ramp.studio/events/storm_forecast_CI2018/leaderboard)) is the RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More information\n",
    "\n",
    "You can find more information in the [README](https://github.com/paris-saclay-cds/ramp-workflow/blob/master/README.md) of the [ramp-workflow library](https://github.com/paris-saclay-cds/ramp-workflow)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contact\n",
    "\n",
    "Don't hesitate to [contact us](mailto:admin@ramp.studio?subject=Storm forecast CI2018 ramp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homemade Forest Parameter Tuning. Attempt at Multiprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from multiprocessing import Pool\n",
    "import itertools\n",
    "\n",
    "\n",
    "def forest_search(max_depth, max_features, n_estimators):\n",
    "    \n",
    "    cart_prod = list(itertools.product(max_depth,max_features,n_estimators))\n",
    "    n_splits = 5\n",
    "    forest_dict = {}\n",
    "\n",
    "\n",
    "    for param_combo in cart_prod:\n",
    "        mse_sum = 0\n",
    "        for train_index, test_index in get_cv(all_features,y_true,n_splits):\n",
    "            forest_regressor = Pipeline([('imputer', Imputer(strategy='median')), \n",
    "                ('scale', StandardScaler()),       \n",
    "                ('regressor', RandomForestRegressor(max_depth = param_combo[0], max_features = param_combo[1], n_estimators = param_combo[2]))])\n",
    "            drop = all_features.drop(columns = ['stormid'])\n",
    "\n",
    "            X_train, X_test = drop.iloc[train_index,:], drop.iloc[test_index,:]\n",
    "            y_train, y_test = y_true[train_index], y_true[test_index]\n",
    "\n",
    "            forest_regressor.fit(X_train,y_train)\n",
    "            y_pred = forest_regressor.predict(X_test)\n",
    "            mse_sum += mean_squared_error(y_test, y_pred)\n",
    "\n",
    "        forest_dict[param_combo] = np.sqrt(mse_sum/n_splits)\n",
    "        print('Elements in Dict:', len(forest_dict))\n",
    "    return forest_dict\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    p = Pool(6)\n",
    "\n",
    "    max_depth = [5,10,15,20,25]\n",
    "    max_features = ['sqrt', 'log2', None]\n",
    "    n_estimators = [10,25,50,100,250,500]\n",
    "\n",
    "    print(type(max_depth))\n",
    "    forest_dict = p.starmap(forest_search, zip([5,10,15,20,25],['sqrt', 'log2', None], [10,25,50,100,250,500]))\n",
    "    p.close()\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "class Regressor(BaseEstimator):\n",
    "    def __init__(self, kernel = 'rbf', gamma = .1, C = 1):\n",
    "        self.reg = Pipeline([('scale', StandardScaler()),\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('regressor', SVR(kernel = kernel, gamma = gamma, C = C))                 \n",
    "        ])\n",
    "        self.kernel = kernel\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.reg.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        print(\"Kernel:\", self.kernel)\n",
    "        return self.reg.predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import itertools\n",
    "\n",
    "# This actually decreases performance:\n",
    "'''\n",
    "data_train['previous_windspeed'] = data_train['windspeed'].shift(1)\n",
    "data_train['previous_windspeed'].loc[data_train['stormid'] != data_train['stormid'].shift(1)] = np.nan\n",
    "data_train['previous_windspeed'].loc[data_train['previous_windspeed'] == np.nan] = data_train['windspeed']\n",
    "data_train['previous_windspeed'].fillna(method='bfill', inplace = True)\n",
    "'''\n",
    "\n",
    "# This is an illegal lookahead???:\n",
    "'''\n",
    "data_train['previous_y'] = y_true\n",
    "data_train['previous_y'] = data_train['previous_y'].shift(1)\n",
    "data_train['previous_y'].loc[data_train['stormid'] != data_train['stormid'].shift(1)] = np.nan\n",
    "data_train['previous_y'].fillna(method='bfill', inplace = True)\n",
    "'''\n",
    "df = \n",
    "all_features = pd.concat([data_train.loc[:,'stormid'], data_train.loc[:,'latitude':'max_wind_change_12h'],pd.get_dummies(data_train.nature, prefix = 'nature'), pd.get_dummies(data_train.basin, prefix = 'basin'), extra_dummy, data_train.loc[:,'dist2land':] ], axis = 1)\n",
    "\n",
    "# Manually add dummy variables for the basins:\n",
    "extra_dummy = pd.DataFrame(data = np.zeros((len(data_train),8)), columns = ['basin_7', 'basin_8', 'basin_9', 'basin_10', 'basin_11', 'basin_12', 'basin_13', 'basin_14'])\n",
    "\n",
    "# For \"illegal lookahead\" features:\n",
    "#zero_d = pd.concat([data_train.loc[:,'stormid'], data_train.loc[:,'latitude':'max_wind_change_12h'], data_train.loc[:,'dist2land'], pd.get_dummies(data_train.nature, prefix = 'nature'), pd.get_dummies(data_train.basin, prefix = 'basin'), extra_dummy, data_train.loc[:,'previous_y']], axis = 1)\n",
    "\n",
    "# For basin and nature features:\n",
    "zero_d = pd.concat([data_train.loc[:,'stormid'], data_train.loc[:,'latitude':'max_wind_change_12h'], data_train.loc[:,'dist2land'], pd.get_dummies(data_train.nature, prefix = 'nature'), pd.get_dummies(data_train.basin, prefix = 'basin'), extra_dummy], axis = 1)\n",
    "\n",
    "# For basin, nature, and previous windspeed features:\n",
    "#zero_d = pd.concat([data_train.loc[:,'stormid'], data_train.loc[:,'latitude':'max_wind_change_12h'], data_train.loc[:,'dist2land'], pd.get_dummies(data_train.nature, prefix = 'nature'), pd.get_dummies(data_train.basin, prefix = 'basin'), extra_dummy, data_train.loc[:,'previous_windspeed']], axis = 1)\n",
    "\n",
    "# For nature feature:\n",
    "#zero_d = pd.concat([data_train.loc[:,'latitude':'max_wind_change_12h'], data_train.loc[:,'dist2land'], pd.get_dummies(data_train.nature, prefix = 'nature')], axis = 1)\n",
    "\n",
    "all_features = pd.concat([data_train.loc[:,'stormid'], data_train.loc[:,'latitude':'max_wind_change_12h'],pd.get_dummies(data_train.nature, prefix = 'nature'), pd.get_dummies(data_train.basin, prefix = 'basin'), extra_dummy, data_train.loc[:,'dist2land':] ], axis = 1)\n",
    "\n",
    "zero_d_uvz = pd.concat([zero_d, data_train[[col for col in data_train.columns if col.startswith(('z_','u_','v_'))]]], axis = 1)\n",
    "\n",
    "zero_d_sst= pd.concat([zero_d, data_train[[col for col in data_train.columns if col.startswith(('sst_','slp_','hum_'))]]], axis = 1)\n",
    "#print(all_features.loc[:,'sst_0_0':'sst_10_10'])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
