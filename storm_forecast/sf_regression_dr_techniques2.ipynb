{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left\">\n",
    "<img src=\"http://project.inria.fr/saclaycds/files/2017/02/logoUPSayPlusCDS_990.png\" width=\"800px\">\n",
    "</div>\n",
    "\n",
    "# [RAMP](https://www.ramp.studio/problems/storm_forecast_hackathon) on Tropical Storm Intensity Forecast (from reanalysis data)\n",
    "\n",
    "_Sophie Giffard-Roisin (CU/CNRS), Mo Yang (CNRS), Balazs Kegl (CNRS/CDS), Claire Monteleoni (CU/CNRS), Alexandre Boucaud (CNRS/CDS)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Introduction](#Introduction)\n",
    "2. [The prediction task](#The-prediction-task)\n",
    "2. [Installation of libraries](#Installation-of-libraries) : To do before coming!\n",
    "2. [The data](#The-data)\n",
    "3. [The pipeline](#The-pipeline)\n",
    "4. [Evaluation](#Evaluation)\n",
    "5. [Local testing/exploration](#Testing-the-submission)\n",
    "6. [Submission](#Submitting-to-the-online-challenge:-ramp.studio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The goal of the RAMP is to predict the intensity of tropical and extra-tropical storms (24h forecast) using information from past storms since 1979. The intensity can be measured as the maximum sustained wind over a period of one minute at 10 meters height. This speed, calculated every 6 hours, is usually explained in knots (1kt=0.514 m/s) and is used to define the hurricane category from the [Saffir-Simpson scale](https://en.wikipedia.org/wiki/Saffir–Simpson_scale). Estimating the intensity evolution of a storm is of course crucial for the population.\n",
    "\n",
    "<img src=\"https://github.com/sophiegif/ramp_kit_storm_forecast_new/blob/master/figures_pynb/all_storms_since1979_IBTrRACKS_newcats.png?raw=true\" width=\"70%\">\n",
    "<div style=\"text-align: center\">Database: tropical/extra-tropical storm tracks since 1979. Dots = initial position, color = maximal storm strength according to the Saffir-Simpson scale.</div>\n",
    "\n",
    "Today, the forecasts (track and intensity) are provided by a numerous number of guidance models (1). Dynamical models solve the physical equations governing motions in the atmosphere. Statistical models, in contrast, are based on historical relationships between storm behavior and various other parameters. However, the lack of improvement in intensity forecasting is attributed to the complexity of tropical systems and an incomplete understanding of factors that affect their development. What is mainly still hard to predict is the rapid intensification of hurricanes: in 1992, Andrew went from tropical depression to a category 5 hurricane in 24h. \n",
    "\n",
    "Machine learning (and deep learning) methods have been only scarcely tested, and there is hope in that it can improve storm forecasts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The prediction task\n",
    "\n",
    "<ul class=\"list-unstyled list-inline text-center\">\n",
    "  <li>\n",
    "    <img src=\"https://github.com/sophiegif/ramp_kit_storm_forecast_new/blob/master/figures_pynb/storm_shema3.png?raw=true\" alt= \"image1\" width=\"350\" height=\"350\">\n",
    "    <figcaption>Goal: estimate the 24h-forecast intensity of all storms.</figcaption>\n",
    "  </li>\n",
    "  \n",
    "  <li>\n",
    "    <img src=\"https://github.com/sophiegif/ramp_kit_storm_forecast_new/blob/master/figures_pynb/hurricane_pb.png?raw=true\" alt= \"image2\" width=\"350\" height=\"350\">\n",
    "    <figcaption>Feature data: centered maps of wind, altitude, sst, slp, humidity...</figcaption>\n",
    "  </li>\n",
    "</ul>\n",
    "\n",
    "This challenge proposes to design the best algorithm to predict for a large number of storms the 24h-forecast intensity every 6 hours. The (real) database is composed of more than 3000 extra-tropical and tropical storm tracks, and it also provides the intensity and some local physical information at each timestep (2). Moreover, we also provide some 700-hPa and 1000-hPa feature maps of the neighborhood of the storm (from ERA-interm reanalysis database (3)), that can be viewed as images centered on the current storm location (see right image).\n",
    "\n",
    "The goal is to provide for each time step of each storm (total number of instants = 90 000), the predicted 24h-forecast intensity, so 4 time steps in the future. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "\n",
    "1. National Hurricane Center Forecast Verification website, https://www.nhc.noaa.gov/verification/, updated 04 April 2017.\n",
    "\n",
    "2. Knapp, K. R., M. C. Kruk, D. H. Levinson, H. J. Diamond, and C. J. Neumann, 2010: The International Best Track Archive for Climate Stewardship (IBTrACS): Unifying tropical cyclone best track data. Bulletin of the American Meteorological Society, 91, 363-376 https://www.ncdc.noaa.gov/ibtracs/index.php?name=wmo-data\n",
    "\n",
    "3. Dee, D. P. et al.(2011), The ERA-Interim reanalysis: configuration and performance of the data assimilation system. Q.J.R. Meteorol. Soc., 137: 553–597. https://rmets.onlinelibrary.wiley.com/doi/abs/10.1002/qj.828"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation of libraries\n",
    "\n",
    "To get this notebook running and test your models locally using the `ramp_test_submission`, we recommend that you use the Python distribution from [Anaconda](https://www.anaconda.com/download/) or [Miniconda](https://docs.anaconda.com/docs_oss/conda/install/quick#miniconda-quick-install-requirements). (uncomment the lines before running them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install -y -c conda conda-env     # First install conda-env to ease the creation of virtual envs in conda\n",
    "# !conda env create                        # Uses the local environment.yml to create the 'storm_forecast_2' env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OR** if you have Python already installed but are **not using Anaconda**, you'll want to use `pip` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installation of ramp-workflow\n",
    "\n",
    "For being able to test submissions, you also need to have the `ramp-workflow` package locally. You can install the latest version with pip from github:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/paris-saclay-cds/ramp-workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download data (optional)\n",
    "\n",
    "If the data has not yet been downloaded locally, uncomment the following cell and run it.\n",
    "The starting kit data is 260 MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python download_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scikit-learn version is 0.19.2.\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utilities ##\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict\n",
    "from typing import Tuple\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from sklearn.preprocessing import Imputer, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.utils.graph_shortest_path import graph_shortest_path\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import multiprocessing as mp\n",
    "import itertools\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "def group_dict(iterable, keyfn, mapfn):\n",
    "    \"\"\"\n",
    "    Groups the iterable using the given key function and returns a dictionary\n",
    "    of keys to groups.\n",
    "    \"\"\"\n",
    "    groups = it.groupby(iterable, key=keyfn)\n",
    "    gdict = dict()\n",
    "    for k, g in groups:\n",
    "        gdict[k] = list(map(mapfn, g))\n",
    "    return gdict\n",
    "\n",
    "def stormid_dict(X_df: pd.DataFrame) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Partitions the storm forecast dataset into separate groups for each storm and\n",
    "    returns the result as a dictionary.\n",
    "    \"\"\"\n",
    "    groups = X_df.groupby(['stormid'])\n",
    "    storm_dict = dict()\n",
    "    for stormid, df in groups:\n",
    "        storm_dict[stormid] = df\n",
    "    return storm_dict\n",
    "\n",
    "def feature_groups(X_df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Partitions X_df into three groups by columns:\n",
    "    1) 0-D features\n",
    "    2) 11x11 z, u, v wind reanalysis data\n",
    "    3) 11x11 sst, slp, humidity, and vorticity reanalysis data\n",
    "    4) All features\n",
    "    \"\"\"\n",
    "    feat_cols = X_df.get(['stormid','instant_t', 'windspeed', 'latitude', 'longitude','hemisphere','Jday_predictor','initial_max_wind','max_wind_change_12h','dist2land'])\n",
    "    nature_cols = pd.get_dummies(X_df.nature, prefix='nature', drop_first=False)\n",
    "    basin_cols = pd.get_dummies(X_df.basin, prefix='basin', drop_first=False)\n",
    "    X_0D = pd.concat([feat_cols, nature_cols, basin_cols], axis=1, sort=False)\n",
    "    X_zuv = X_df.get([col for col in X_df.columns if col.startswith('z_') or col.startswith('u_') or col.startswith('v_')])\n",
    "    X_sshv = X_df.get([col for col in X_df.columns if col.startswith('sst') or col.startswith('slp')\n",
    "                   or col.startswith('hum') or col.startswith('vo700')])\n",
    "    X_all = pd.concat([X_0D, X_zuv, X_sshv], axis = 1)\n",
    "    X_0D_zuv = pd.concat([X_0D, X_zuv], axis = 1)\n",
    "    X_0D_sshv = pd.concat([X_0D, X_sshv], axis = 1)\n",
    "    \n",
    "    return X_0D, X_0D_zuv, X_0D_sshv, X_all\n",
    "\n",
    "def trust_cont_score(X, X_map, k=10, alpha=0.5, impute_strategy='median'):\n",
    "    \"\"\"\n",
    "    Computes the \"trustworthiness\" and \"continuity\" [1] of X_map with respect to X.\n",
    "    This is a port and extension of the implementation provided by Van der Maaten [2].\n",
    "    \n",
    "    Parameters:\n",
    "    X     : the data in its original representation\n",
    "    X_map : the lower dimensional representation of the data to be evaluated\n",
    "    k     : parameter that determines the size of the neighborhood for the T&C measure\n",
    "    alpha : mixing parameter in [0,1] that determines the weight given to trustworthiness vs. continuity; higher values will give more\n",
    "            weight to trustworthiness, lower values to continuity.\n",
    "    \n",
    "    [1] Kaski S, Nikkilä J, Oja M, Venna J, Törönen P, Castrén E. Trustworthiness and metrics in visualizing similarity of gene expression. BMC bioinformatics. 2003 Dec;4(1):48.\n",
    "    [2] Maaten L. Learning a parametric embedding by preserving local structure. InArtificial Intelligence and Statistics 2009 Apr 15 (pp. 384-391).\n",
    "    \"\"\"\n",
    "    # Impute X values\n",
    "    X = Imputer(strategy=impute_strategy).fit_transform(X)\n",
    "    # Compute pairwise distance matrices\n",
    "    D_h = pairwise_distances(X, X, metric='euclidean')\n",
    "    D_l = pairwise_distances(X_map, X_map, metric='euclidean')\n",
    "    # Compute neighborhood indices\n",
    "    ind_h = np.argsort(D_h, axis=1)\n",
    "    ind_l = np.argsort(D_l, axis=1)\n",
    "    # Compute trustworthiness\n",
    "    N = X.shape[0]\n",
    "    T = 0\n",
    "    C = 0\n",
    "    t_ranks = np.zeros((k, 1))\n",
    "    c_ranks = np.zeros((k, 1))\n",
    "    for i in range(N):\n",
    "        for j in range(k):\n",
    "            t_ranks[j] = np.where(ind_h[i,:] == ind_l[i, j+1])\n",
    "            c_ranks[j] = np.where(ind_l[i,:] == ind_h[i, j+1])\n",
    "        t_ranks -= k\n",
    "        c_ranks -= k\n",
    "        T += np.sum(t_ranks[np.where(t_ranks > 0)])\n",
    "        C += np.sum(c_ranks[np.where(c_ranks > 0)])\n",
    "    S = (2 / (N * k * (2 * N - 3 * k - 1)))\n",
    "    T = 1.0 - S*T\n",
    "    C = 1.0 - S*C\n",
    "    return alpha*T + (1.0-alpha)*C\n",
    "\n",
    "def sammon_stress(X, X_m, impute_strategy='median'):\n",
    "    X = Imputer(strategy=impute_strategy).fit_transform(X)\n",
    "    Dx = pairwise_distances(X, X, metric='euclidean')\n",
    "    Dy = pairwise_distances(X_m, X_m, metric='euclidean')\n",
    "    # Sammon Stress computes sums over indices where i < j\n",
    "    # We can interpet this as being the upper triangle of each matrix, from the k=1 diagonal\n",
    "    Dx_ut = np.triu(Dx, k=1)\n",
    "    Dy_ut = np.triu(Dy, k=1)\n",
    "    # Compute Sammon Stress, S\n",
    "    S = (1 / np.sum(Dx_ut))*np.sum(np.square(Dx_ut - Dy_ut) / (Dx_ut + np.ones(Dx.shape)))\n",
    "    return S\n",
    "    \n",
    "    \n",
    "def residual_variance(X, X_m, n_neighbors=20):\n",
    "    kng_h = kneighbors_graph(X, n_neighbors=n_neighbors, mode='distance', n_jobs=mp.cpu_count()).toarray()\n",
    "    D_h = graph_shortest_path(kng_h, method='D', directed=False)\n",
    "    #D_h = pairwise_distances(X, X, metric='euclidean')\n",
    "    #D_l = kneighbors_graph(X_m, n_neighbors=50, mode='distance').toarray()\n",
    "    D_l = pairwise_distances(X_m, X_m, metric='euclidean')\n",
    "    r,_ = spearmanr(D_h.flatten(), D_l.flatten())\n",
    "    return 1 - r**2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel(fn, params, n_jobs=mp.cpu_count()):\n",
    "    pool = mp.Pool(n_jobs)\n",
    "    print('started new process pool with {} processes'.format(n_jobs))\n",
    "    try:\n",
    "        res = pool.map(fn, params)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "    except:\n",
    "        print('process pool interrupted, shutting down')\n",
    "        pool.terminate()\n",
    "        pool.join()\n",
    "        raise\n",
    "    return res\n",
    "\n",
    "mplog = mp.get_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mapping_vs_intensity_2d(X_map, ys, title=\"\", xlabel=\"\", ylabel=\"\", instant_labels: pd.DataFrame = None):\n",
    "    plt.scatter(X_map[:,0], X_map[:,1], c=ys, cmap='copper')\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.colorbar().set_label(\"Storm intensity\")\n",
    "    if instant_labels is not None:\n",
    "        for (i, xy) in enumerate(X_map):\n",
    "            if i % 2 == 0:\n",
    "                plt.annotate(instant_labels.values[i], xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(X_df, n_components=2):\n",
    "    X_drop = X_df.drop(columns = ['stormid'])\n",
    "    imputer = Imputer(strategy='median')\n",
    "    scaler = StandardScaler()\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca_pipeline = Pipeline([('med_imputer', imputer),('scaler', scaler),('pca',pca)])\n",
    "    X_pc = pca_pipeline.fit_transform(X_drop)    \n",
    "    pca_comp_feat_ratios = np.square(pca.components_)\n",
    "    feat_names = X_drop.columns\n",
    "    pc_feat_contrib = pd.DataFrame(pca_comp_feat_ratios, columns=feat_names)\n",
    "    return pd.concat([X_df['stormid'],pd.DataFrame(X_pc)], axis = 1) #, pc_feat_contrib, pca.components_, pca.explained_variance_ratio_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_projection(X_df, n_components='auto', eps=0.1):\n",
    "    X_drop = X_df.drop(columns = ['stormid'])\n",
    "    imputer = Imputer(strategy='median')\n",
    "    scaler = StandardScaler()\n",
    "    proj = SparseRandomProjection(n_components=n_components, eps=eps)\n",
    "    proj_pipeline = Pipeline([('med_imputer', imputer),('scaler', scaler),('proj', proj)])\n",
    "    X_rp = proj_pipeline.fit_transform(X_drop)\n",
    "    return pd.concat([X_df['stormid'],pd.DataFrame(X_rp)], axis = 1) #, proj.n_components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne(X_df, n_components=2, n_iter=5000, perplexity=30, learning_rate=100, init='pca'):\n",
    "    imputer = Imputer(strategy='median')\n",
    "    scaler = StandardScaler()\n",
    "    tsne = TSNE(n_components=n_components, perplexity=perplexity, learning_rate=learning_rate, init=init, n_iter=n_iter)\n",
    "    tsne_pipeline = Pipeline([('med_imputer', imputer),('scaler', scaler),('tsne', tsne)])\n",
    "    X_tsne = tsne_pipeline.fit_transform(X_df)\n",
    "    print(\"t-SNE completed after {} iterations with final KLD: {}\".format(tsne.n_iter_, tsne.kl_divergence_))\n",
    "    return X_tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def umap(X_df, n_components=2, y=None, n_neighbors=5, min_dist=0.1, metric='correlation'):\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    imputer = Imputer(strategy='median')\n",
    "    scaler = StandardScaler()\n",
    "    umap = UMAP(n_components=n_components, n_neighbors=n_neighbors, min_dist=min_dist, metric=metric)\n",
    "    umap_pipeline = Pipeline([('med_imputer', imputer),('scaler', scaler),('umap', umap)])\n",
    "    X_umap = umap_pipeline.fit_transform(X_df, y)\n",
    "    warnings.resetwarnings()\n",
    "    return X_umap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data\n",
    "\n",
    "The 3000 storms have been separated in a train set, a test set and a local starting kit (train+test sets). The data from `download_data.py` (local starting kit) includes only 1/4 storms of the total database; and the train set on which your code will run on the platform has another half. They are disjoined. \n",
    "\n",
    "Let's have a look at the local train data (only the first rows are plotted)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from problem import get_train_data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 100\n",
    "pd.options.display.max_columns = 100\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "zero_d_dict = {}\n",
    "# RMSE will be in the same units as the output variable(windspeed in knots)\n",
    "# Data Exploration: the training set has 15777 training examples with 859 features each.\n",
    "data_train, y_true = get_train_data()\n",
    "X_0D, X_0D_zuv, X_0D_sshv, X_all = feature_groups(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy of their cross-validation function: \n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "def get_cv(X, y, num_splits):\n",
    "    group = np.array(X['stormid'])\n",
    "    X, y, group = shuffle(X, y, group, random_state=3)\n",
    "    gkf = GroupKFold(n_splits=num_splits).split(X, y, group)\n",
    "    return gkf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15777, 464)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_rp = rand_projection(X_all, eps = .5)\n",
    "np.shape(X_rp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Results for Everything: (must change X_df in each function)\n",
    "total_rp_dict = {}\n",
    "\n",
    "for eps in [0.1, 0.25, 0.5, 0.75, 0.99]\n",
    "    rp_dict = {}\n",
    "    \n",
    "    X_rp = rand_projection(X_all,eps)\n",
    "\n",
    "    max_depth = [5,10,20,25]\n",
    "    max_depth_tree = [5,10,15,20,25,50,100]\n",
    "    max_features = ['sqrt', 'log2', None]\n",
    "    n_estimators = [10,25,50,100,200]\n",
    "    alpha_lasso = [.00001, .0001, .001, .01, .1, 1]\n",
    "    alpha_ridge = [1, 2, 5, 10, 25, 50, 100, 200, 500, 1000]\n",
    "    C = np.logspace(-2, 2, num=5) \n",
    "    gamma = np.logspace(-4, 2, num=7)\n",
    "\n",
    "    params_forest_boost = list(itertools.product(max_depth, max_features, n_estimators))\n",
    "    params_tree = list(itertools.product(max_depth, max_features))\n",
    "    params_svr = list(itertools.product(C, gamma))\n",
    "\n",
    "    results_forest = parallel(run_forest, params_forest_boost)\n",
    "    results_tree = parallel(run_tree, params_tree)\n",
    "    results_boost = parallel(run_boost, params_forest_boost)\n",
    "    results_svr = parallel(run_svr, params_svr)\n",
    "\n",
    "    rp_dict['forest_regressor'] = dict(results_forest)\n",
    "    total_rp_dict[eps] = rp_dict\n",
    "    np.save('storm_forecast_rp_tuning.npy', total_rp_dict)\n",
    "    print(\"Saved!\")\n",
    "    \n",
    "    rp_dict['tree_regressor'] = dict(results_tree)\n",
    "    total_rp_dict[eps] = rp_dict\n",
    "    np.save('storm_forecast_rp_tuning.npy', total_rp_dict)\n",
    "    print(\"Saved!\")\n",
    "    \n",
    "    rp_dict['boost_regressor'] = dict(results_boost)\n",
    "    total_rp_dict[eps] = rp_dict\n",
    "    np.save('storm_forecast_rp_tuning.npy',total_rp_dict)\n",
    "    print(\"Saved!\")\n",
    "    \n",
    "    rp_dict['svr_regressor'] = dict(results_svr)\n",
    "    total_rp_dict[eps] = rp_dict\n",
    "    np.save('storm_forecast_rp_tuning.npy', total_rp_dict)\n",
    "    print(\"Saved!\")\n",
    "    \n",
    "    rp_dict['ridge_regressor'] = ridge(X_rp, y_true, alpha_ridge)\n",
    "    total_rp_dict[eps] = rp_dict\n",
    "    np.save('storm_forecast_rp_tuning.npy', total_rp_dict)\n",
    "    print(\"Saved!\")\n",
    "    \n",
    "    rp_dict['lasso_regressor'] = lasso(X_rp, y_true, alpha_lasso)\n",
    "    total_rp_dict[eps] = rp_dict\n",
    "    np.save('storm_forecast_rp_tuning.npy', total_rp_dict)\n",
    "    print(\"Saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Generate Results for Everything: (must change X_df in each function)\n",
    "zero_0d_zuv_dict = {}\n",
    "max_depth = [5,10,20,25]\n",
    "max_depth_tree = [5,10,15,20,25,50,100]\n",
    "max_features = ['sqrt', 'log2', None]\n",
    "n_estimators = [10,25,50,100,200]\n",
    "alpha_lasso = [.00001, .0001, .001, .01, .1, 1]\n",
    "alpha_ridge = [1, 2, 5, 10, 25, 50, 100, 200, 500, 1000]\n",
    "\n",
    "params_forest_boost = list(itertools.product(max_depth, max_features, n_estimators))\n",
    "params_tree = list(itertools.product(max_depth, max_features))\n",
    "\n",
    "results_forest2 = parallel(run_forest, params_forest_boost)\n",
    "results_tree2 = parallel(run_tree, params_tree)\n",
    "results_boost2 = parallel(run_boost, params_forest_boost)\n",
    "\n",
    "zero_0d_zuv_dict['forest_regressor'] = dict(results_forest2)\n",
    "\n",
    "zero_0d_zuv_dict['tree_regressor'] = dict(results_tree2)\n",
    "\n",
    "zero_0d_zuv_dict['boost_regressor'] = dict(results_boost2)\n",
    "\n",
    "zero_0d_zuv_dict['ridge_regressor'] = ridge(X_0D_zuv, y_true, alpha_ridge)\n",
    "\n",
    "zero_0d_zuv_dict['lasso_regressor'] = lasso(X_0D_zuv, y_true, alpha_lasso)\n",
    "\n",
    "print(zero_0d_zuv_dict)\n",
    "#np.save('storm_forecast_zero_d_zuv_tuning.npy', zero_0d_zuv_dict)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homemade Forest Parameter Tuning \n",
    "def forest(X_df, y_df, max_depth, max_features, n_estimators, n_splits = 5):\n",
    "    mse_sum = 0\n",
    "    for train_index, test_index in get_cv(X_df,y_df,n_splits):\n",
    "        forest_regressor = Pipeline([       \n",
    "            ('regressor', RandomForestRegressor(max_depth = max_depth, max_features = max_features, n_estimators = n_estimators))])\n",
    "        drop = X_df.drop(columns = ['stormid'])\n",
    "\n",
    "        X_train, X_test = drop.iloc[train_index,:], drop.iloc[test_index,:]\n",
    "        y_train, y_test = y_df[train_index], y_df[test_index]\n",
    "\n",
    "        forest_regressor.fit(X_train,y_train)\n",
    "        y_pred = forest_regressor.predict(X_test)\n",
    "        mse_sum += mean_squared_error(y_test, y_pred)\n",
    "        \n",
    "    return ((max_depth, max_features, n_estimators),np.sqrt(mse_sum/n_splits))\n",
    "\n",
    "def run_forest(p):\n",
    "    d, f, n = p\n",
    "    return forest(X_rp, y_true, max_depth=d, max_features=f, n_estimators=n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = [5,10,15,20,25]\n",
    "max_features = ['sqrt', 'log2', None]\n",
    "n_estimators = [10,25,50, 100,250,500]\n",
    "\n",
    "results = parallel(run_forest, params)\n",
    "zero_d_dict['forest_regressor'] = dict(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homemade Tree Parameter Tuning \n",
    "def tree(X_df, y_df, max_depth, max_features, n_splits =5):\n",
    "    mse_sum = 0\n",
    "    for train_index, test_index in get_cv(X_df,y_df,n_splits):\n",
    "        tree_regressor = Pipeline([\n",
    "            ('regressor', DecisionTreeRegressor(max_depth = max_depth, max_features = max_features))])\n",
    "        drop = X_df.drop(columns = ['stormid'])\n",
    "        X_train, X_test = drop.iloc[train_index,:], drop.iloc[test_index,:]\n",
    "        y_train, y_test = y_true[train_index], y_true[test_index]\n",
    "        tree_regressor.fit(X_train,y_train)\n",
    "        y_pred = tree_regressor.predict(X_test)\n",
    "        mse_sum += mean_squared_error(y_test, y_pred)\n",
    "        \n",
    "    return ((max_depth, max_features),np.sqrt(mse_sum/n_splits))\n",
    "\n",
    "def run_tree(p):\n",
    "    d, f = p\n",
    "    return tree(X_rp, y_true, max_depth=d, max_features=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Results\n",
    "max_depth = [5,10,15,20,25,50,100]\n",
    "max_features = [None, 'sqrt', 'log2']\n",
    "\n",
    "params = list(itertools.product(max_depth, max_features))\n",
    "\n",
    "results_tree = parallel(run_tree, params)\n",
    "zero_d_dict['tree_regressor'] = dict(results_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homemade SVR Parameter Tuning \n",
    "def svr(X_df, y_df, C, gamma, n_splits = 5):\n",
    "    mse_sum = 0\n",
    "    for train_index, test_index in get_cv(X_df,y_df,n_splits):\n",
    "        rbf_regressor = Pipeline([\n",
    "            ('regressor', SVR(kernel = 'rbf', C = C, gamma = gamma))])\n",
    "        drop = X_df.drop(columns = ['stormid'])\n",
    "        \n",
    "        X_train, X_test = drop.iloc[train_index,:], drop.iloc[test_index,:]\n",
    "        y_train, y_test = y_true[train_index], y_true[test_index]\n",
    "        rbf_regressor.fit(X_train,y_train)\n",
    "        y_pred = rbf_regressor.predict(X_test)\n",
    "        mse_sum += mean_squared_error(y_test, y_pred)\n",
    "        \n",
    "    return ((C, gamma),np.sqrt(mse_sum/n_splits))\n",
    "\n",
    "def run_svr(p):\n",
    "    c, g = p\n",
    "    return svr(X_rp, y_true, C = c, gamma = g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Reults\n",
    "C = np.logspace(-3, 3, num=7)\n",
    "gamma = np.logspace(-5, 2, num=8)\n",
    "\n",
    "params = list(itertools.product(C, gamma))\n",
    "\n",
    "results_rbf = parallel(run_svr, params)\n",
    "zero_d_dict['rbf_regressor'] = dict(results_rbf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homemade Boosted Forest Parameter Tuning \n",
    "def boost(X_df, y_df, max_depth, max_features, n_estimators, n_splits = 5):\n",
    "    mse_sum = 0\n",
    "    for train_index, test_index in get_cv(X_df,y_df,5):\n",
    "        forest_regressor = Pipeline([      \n",
    "            ('regressor', GradientBoostingRegressor(max_depth = max_depth, max_features = max_features, n_estimators = n_estimators))])\n",
    "        drop = X_df.drop(columns = ['stormid'])\n",
    "\n",
    "        X_train, X_test = drop.iloc[train_index,:], drop.iloc[test_index,:]\n",
    "        y_train, y_test = y_df[train_index], y_df[test_index]\n",
    "\n",
    "        forest_regressor.fit(X_train,y_train)\n",
    "        y_pred = forest_regressor.predict(X_test)\n",
    "        mse_sum += mean_squared_error(y_test, y_pred)\n",
    "        \n",
    "    return ((max_depth, max_features, n_estimators),np.sqrt(mse_sum/n_splits))\n",
    "\n",
    "def run_boost(p):\n",
    "    d, f, n = p\n",
    "    return boost(X_rp, y_true, max_depth=d, max_features=f, n_estimators=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Results\n",
    "max_depth = [5,10,15,20,25]\n",
    "max_features = ['sqrt', 'log2', None]\n",
    "n_estimators = [10,25,50,100,250,500]\n",
    "\n",
    "params = list(itertools.product(max_depth, max_features, n_estimators))\n",
    "\n",
    "results_boost = parallel(run_boost, params)\n",
    "zero_d_dict['boost_regressor'] = dict(results_boost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homemade Lasso Parameter Tuning \n",
    "def lasso(X_df, y_df, alpha, n_splits = 5):\n",
    "    lasso_dict = {}\n",
    "    for a in alpha:\n",
    "        mse_sum = 0\n",
    "        for train_index, test_index in get_cv(X_df,y_df,n_splits):\n",
    "            lasso_regressor = Pipeline([\n",
    "                ('regressor', Lasso(alpha = a))])\n",
    "            drop = X_df.drop(columns = ['stormid'])\n",
    "\n",
    "            X_train, X_test = drop.iloc[train_index,:], drop.iloc[test_index,:]\n",
    "            y_train, y_test = y_true[train_index], y_true[test_index]\n",
    "\n",
    "            lasso_regressor.fit(X_train,y_train)\n",
    "            y_pred = lasso_regressor.predict(X_test)\n",
    "            mse_sum += mean_squared_error(y_test, y_pred)\n",
    "\n",
    "        lasso_dict[a] = np.sqrt(mse_sum/n_splits)\n",
    "    return lasso_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Results\n",
    "alpha = [.00001, .0001, .001, .01, .1, 1]\n",
    "zero_d_dict['lasso_regressor'] = lasso(X_0D, y_true, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homemade Ridge Parameter Tuning \n",
    "def ridge(X_df, y_df, alpha, n_splits = 5):\n",
    "    ridge_dict = {}\n",
    "    for a in alpha:\n",
    "        mse_sum = 0\n",
    "        for train_index, test_index in get_cv(X_df,y_df,n_splits):\n",
    "            ridge_regressor = Pipeline([\n",
    "                ('regressor', Ridge(alpha = a))])\n",
    "            drop = X_df.drop(columns = ['stormid'])\n",
    "\n",
    "            X_train, X_test = drop.iloc[train_index,:], drop.iloc[test_index,:]\n",
    "            y_train, y_test = y_true[train_index], y_true[test_index]\n",
    "\n",
    "            ridge_regressor.fit(X_train,y_train)\n",
    "            y_pred = ridge_regressor.predict(X_test)\n",
    "            mse_sum += mean_squared_error(y_test, y_pred)\n",
    "\n",
    "        ridge_dict[a] = np.sqrt(mse_sum/n_splits)\n",
    "    return ridge_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = [1, 2, 5, 10, 25, 50, 100, 200, 500, 1000]\n",
    "zero_d_dict['ridge_regressor'] = ridge(X_0D, y_true, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_d_dict['tree_regressor'] = dict(results_tree)\n",
    "zero_d_dict['boost_regressor'] = dict(results_boost)\n",
    "zero_d_dict['forest_regressor'] = dict(results_forest)\n",
    "print(zero_d_dict)\n",
    "np.save('storm_forecast_zero_d_tuning.npy', zero_d_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the data is a list of time instants (one every 6h). The first storm will result in x lines beginning with its stormid and the corresponding time step, with all the associated features on the same row. Then the time steps from the second storm will be below, and so on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of storms in the local training set: {}'.format( len(set(data_train['stormid'])) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of time steps in the local training set: {}'.format(y_train.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 0D features from track data\n",
    "\n",
    "A set of simple features has been extracted for each storm at each time point: \n",
    "\n",
    "- latitude, longitude: in degrees\n",
    "- windspeed: current (max) windspeed (knots) \n",
    "- hemisphere:  South=0, North=1\n",
    "- Jday predictor:  Gaussian function of (Julian day of storm init - peak day of the hurricane season), see (1)\n",
    "- initial_max_wind: initial (max) windspeed of the storm \n",
    "- max_wind_change_12h: last 12h (max) windspeed change\n",
    "- basin = based on the present location: \n",
    "       0 = NA - North Atlantic / 1 = SA - South Atlantic    / 2 = WP - West Pacific       / 3 = EP - East Pacific /\n",
    "       4 = SP - South Pacific  / 5 = NI - North Indian      / 6 = SI - South Indian       / 7 = AS - Arabian Sea /\n",
    "       8 = BB - Bay of Bengal  / 9 = EA - Eastern Australia / 10 = WA - Western Australia / 11 = CP - Central Pacific\n",
    "       12 = CS - Carribbean Sea/ 13 = GM - Gulf of Mexico   / 14 = MM - Missing\n",
    "- nature = nature of the storm  \n",
    "       0 = TS - Tropical / 1 = SS - Subtropical / 2 = ET - Extratropical / 3 = DS - Disturbance /\n",
    "       4 = MX - Mix of conflicting reports / 5 = NR - Not Reported / 6 = MM - Missing / 7 =  - Missing\n",
    "- dist2land = current distance to the land (km)\n",
    "\n",
    "\n",
    "(1) DeMaria, Mark, et al. \"Further improvements to the statistical hurricane intensity prediction scheme (SHIPS).\" Weather and Forecasting 20.4 (2005): 531-543. https://journals.ametsoc.org/doi/full/10.1175/WAF862.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Regression With Only 0D Features:\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(test.shape)\n",
    "print(y_true.shape)\n",
    "# Data Exploration: There are now 11832 training examples and 3945 test examples\n",
    "X_train, X_test, y_train, y_test = train_test_split(test, y_true)\n",
    "\n",
    "#X_train = X_train.loc[:,'latitude':'dist2land']\n",
    "#X_test = X_test.loc[:,'latitude':'dist2land']\n",
    "#X_train = preprocessing.scale(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVR with RBF Kernel:\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "rbf_regressor = Pipeline([('scale', StandardScaler()),\n",
    "            ('imputer', Imputer(strategy='median')),\n",
    "            ('regressor', Lasso())])\n",
    "\n",
    "'''\n",
    "rbf_regressor.fit(X_train,y_train)\n",
    "y_pred = rbf_regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(mse)\n",
    "'''\n",
    "\n",
    "\n",
    "#grid_list = {\"regressor__C\": np.logspace(-3, 3, num=7), \"regressor__gamma\": np.logspace(-5, 2, num=8)}\n",
    "grid_list = { \n",
    "    'regressor__alpha': [1, 10]\n",
    "}\n",
    "folds = get_cv(data_train, y_true)\n",
    "for f in folds:\n",
    "    print(f)\n",
    "grid = RandomizedSearchCV(estimator=rbf_regressor, param_distributions=grid_list, n_jobs = None, random_state = 1, n_iter=2, scoring = 'neg_mean_squared_error', cv = 5)\n",
    "print(grid)\n",
    "grid.fit(tmp,y_true)\n",
    "#print(grid.best_score_)\n",
    "#grid.best_params_\n",
    "\n",
    "'''\n",
    "param_tuning['rbf_regressor'] = (grid.best_score_,grid.best_params_)\n",
    "np.save('storm_forecast_parameter_tuning.npy', param_tuning)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "poly_regressor = Pipeline([('scale', StandardScaler()),\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('regressor', SVR('poly'))])\n",
    "\n",
    "poly_regressor.fit(X_train,y_train)\n",
    "y_pred = poly_regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(mse)\n",
    "\n",
    "\n",
    "grid_list = {\"regressor__C\": np.logspace(-3, 3, num=7), \"regressor__gamma\": np.logspace(-5, 2, num=8)}\n",
    "grid = RandomizedSearchCV(estimator=poly_regressor, param_distributions=grid_list, n_jobs = -1, random_state = 1, n_iter=50, scoring = 'neg_mean_squared_error', cv = 5)\n",
    "grid.fit(X_train,y_train)\n",
    "print(grid.best_score_)\n",
    "grid.best_params_\n",
    "\n",
    "param_tuning['poly_regressor'] = (grid.best_score_,grid.best_params_)\n",
    "np.save('storm_forecast_parameter_tuning.npy', param_tuning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "tree_regressor = Pipeline([('scale', StandardScaler()),\n",
    "            ('imputer', Imputer(strategy='median')),\n",
    "            ('regressor', DecisionTreeRegressor())])\n",
    "\n",
    "tree_regressor.fit(X_train,y_train)\n",
    "y_pred = tree_regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(mse)\n",
    "\n",
    "\n",
    "grid_list = {\"regressor__max_depth\": [None,5,10,15,20,25],'regressor__max_features': [None, 'sqrt', 'log2']}\n",
    "grid = RandomizedSearchCV(estimator=tree_regressor, param_distributions=grid_list, n_jobs = -1, random_state = 1, n_iter=10, scoring = 'neg_mean_squared_error', cv = get_cv(test,y_true))\n",
    "\n",
    "grid.fit(tmp,y_pred)\n",
    "\n",
    "#param_tuning['tree_regressor'] = (grid.best_score_,grid.best_params_)\n",
    "#np.save('storm_forecast_parameter_tuning.npy', param_tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "forest_regressor = Pipeline([('scale', StandardScaler()),\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('regressor', RandomForestRegressor())])\n",
    "\n",
    "forest_regressor.fit(X_train,y_train)\n",
    "y_pred = forest_regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(mse)\n",
    "\n",
    "\n",
    "grid_list = { \n",
    "    'regressor__n_estimators': [10,25,50,100,250,500,1000],\n",
    "    'regressor__max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'regressor__max_depth' : [None,5,10,15,20,25,50,75,100],\n",
    "}\n",
    "grid = RandomizedSearchCV(estimator=forest_regressor, param_distributions=grid_list, n_jobs = -1, random_state = 1, n_iter=50, scoring = 'neg_mean_squared_error', cv = 5)\n",
    "grid.fit(X_train,y_train)\n",
    "\n",
    "param_tuning['forest_regressor'] = (grid.best_score_,grid.best_params_)\n",
    "np.save('storm_forecast_parameter_tuning_basin+nature.npy', param_tuning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(param_tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "boost_regressor = Pipeline([('scale', StandardScaler()),\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('regressor', GradientBoostingRegressor())])\n",
    "\n",
    "boost_regressor.fit(X_train,y_train)\n",
    "y_pred = boost_regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(mse)\n",
    "\n",
    "grid_list = { \n",
    "    'regressor__n_estimators': [10,25,50,100,250,500,1000],\n",
    "    'regressor__max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'regressor__max_depth' : [None,3,5,10,15,20,25,50,75,100],\n",
    "}\n",
    "grid = RandomizedSearchCV(estimator=forest_regressor, param_distributions=grid_list, n_jobs = -1, random_state = 1, n_iter=50, scoring = 'neg_mean_squared_error', cv = 5)\n",
    "grid.fit(X_train,y_train)\n",
    "\n",
    "param_tuning['boost_regressor'] = (grid.best_score_,grid.best_params_)\n",
    "np.save('storm_forecast_parameter_tuning_basin+nature.npy', param_tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "elastic_regressor = Pipeline([('scale', StandardScaler()),\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('regressor', ElasticNet())])\n",
    "\n",
    "elastic_regressor.fit(X_train,y_train)\n",
    "y_pred = elasticregressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "[1, 50, 100, 200, 1000]\n",
    "\n",
    "ridge_regressor = Pipeline([('scale', StandardScaler()),\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('regressor', Ridge())])\n",
    "\n",
    "ridge_regressor.fit(X_train,y_train)\n",
    "y_pred = ridge_regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(mse)\n",
    "\n",
    "grid_list = { \n",
    "    'regressor__alpha': [1, 2, 5, 10, 25, 50, 100, 200, 500, 1000]\n",
    "}\n",
    "grid = RandomizedSearchCV(estimator=ridge_regressor, param_distributions=grid_list, n_jobs = -1, random_state = 1, n_iter=50, scoring = 'neg_mean_squared_error', cv = 5)\n",
    "grid.fit(X_train,y_train)\n",
    "\n",
    "param_tuning['ridge_regressor'] = (grid.best_score_,grid.best_params_)\n",
    "np.save('storm_forecast_parameter_tuning.npy', param_tuning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "lasso_regressor = Pipeline([('scale', StandardScaler()),\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('regressor', Lasso())])\n",
    "\n",
    "lasso_regressor.fit(X_train,y_train)\n",
    "y_pred = lasso_regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(mse)\n",
    "\n",
    "grid_list = { \n",
    "    'regressor__alpha': [.00001, .0001, .001, .01, .1, 1]\n",
    "}\n",
    "grid = RandomizedSearchCV(estimator=lasso_regressor, param_distributions=grid_list, n_jobs = -1, random_state = 1, n_iter=50, scoring = 'neg_mean_squared_error', cv = 5)\n",
    "grid.fit(X_train,y_train)\n",
    "\n",
    "param_tuning['lasso_regressor'] = (grid.best_score_,grid.best_params_)\n",
    "np.save('storm_forecast_parameter_tuning.npy', param_tuning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "regressor = Pipeline([('scale', StandardScaler()),\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('regressor', RandomForestRegressor())])\n",
    "\n",
    "regressor.fit(X_train,y_train)\n",
    "y_pred = regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(mse)\n",
    "\n",
    "grid_list = { \n",
    "    'n_estimators': [50, 100, 200, 300, 400, 500],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth' : [5,10,20],\n",
    "    'criterion' :['gini', 'entropy']\n",
    "}\n",
    "\n",
    "grid = RandomizedSearchCV(estimator=regressor, param_distributions=grid_list, n_jobs = -1, random_state = 1, n_iter=100, scoring = 'neg_mean_squared_error', cv = 5)\n",
    "print(grid)\n",
    "grid.fit(X_train,y_train)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "grid_list = {\"C\": [1,2,3],\n",
    "             \"gamma\": [.1,.2,.3]}\n",
    "\n",
    "regressor = Pipeline([('scale', StandardScaler()),\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('regressor', SVR(kernel = 'rbf', gamma = .1, C = 1))])\n",
    "regressor.fit(X_train,y_train)\n",
    "y_pred = regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(mse)\n",
    "\n",
    "grid_list = {\"regressor__C\": np.logspace(-2, 2, num=5), \"regressor__gamma\": np.logspace(-4, 0, num=5)}\n",
    "\n",
    "grid = RandomizedSearchCV(estimator=regressor, param_distributions=grid_list, n_jobs = -1, random_state = 1, n_iter=100, scoring = 'neg_mean_squared_error', cv = 5)\n",
    "print(grid)\n",
    "grid.fit(X_train,y_train)\n",
    "print(grid.best_score_)\n",
    "\n",
    "#print(grid.best_estimator_)\n",
    "#print(grid.best_estimator_.regressor__gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. The reanalysis data\n",
    "\n",
    "At each time step, we extracted 7 grids (11x11 pixels) of meteorological parameters centered on the current storm location. Their choice is based on the forecast literature, on personal experience and on known hypothesis of storm strengthening.\n",
    "\n",
    "#### a) 25x25 degree z, u and v at 700hPa-level\n",
    "First, we provide 3 maps of 25 x 25 degrees (lat/long) at 700hPa-level pressure: the altitude `z`, the u-wind `u` (positive if wind from the West) and the v-wind `v` (positive if wind from the South). These grids are subsampled to 11x11 pixels (1 pixel ~=2 degrees).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "sample_id=20 # sample number plotted - you can change it to see other storms and other instants\n",
    "grid_l=11 # size of all 2D-grids (in pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_25x25=['z','u','v']\n",
    "plt.figure(figsize=(10,4))\n",
    "for p,param in enumerate(params_25x25):\n",
    "    image=np.zeros([grid_l,grid_l])\n",
    "    for i in range(grid_l):\n",
    "         for j in range(grid_l):\n",
    "            image[i,j]=data_train[param+'_'+str(i)+'_'+str(j)][sample_id]\n",
    "    plt.subplot(1,3,p+1)\n",
    "    plt.imshow(np.array(image),extent=[-12,12,-12,12],\n",
    "               interpolation='nearest', origin='lower', cmap='seismic')\n",
    "    plt.xlabel('param '+param)\n",
    "t=plt.suptitle('Example of 700-hPa level maps 25x25 degrees, centered in the storm location.'\n",
    "         +'\\n (altitude, u-wind and v-wind)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) 11x11 degree sst, slp, humidity at 1000hPa,  and vorticity at 700hPa\n",
    "We provide some more localized maps of 11 x 11 degrees (lat/long) at the surface: the sea surface temperature `sst`, surface level pressure `slp`, the relative humidity `hum` at 1000hPa (near surface). These grids are sampled to 11x11 pixels (1 pixel = 1 degree). We also provide the vorticity at 700hPa `vo700`. \n",
    "\n",
    "NB: `sst` is only defined on the sea, so land has NaNs values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_11x11=['sst','slp','hum','vo700']\n",
    "plt.figure(figsize=(10,3))\n",
    "for p,param in enumerate(params_11x11):\n",
    "    image=np.zeros([grid_l,grid_l])\n",
    "    for i in range(grid_l):\n",
    "         for j in range(grid_l):\n",
    "            image[i,j]=data_train[param+'_'+str(i)+'_'+str(j)][sample_id]\n",
    "    plt.subplot(1,4,p+1)\n",
    "    plt.imshow(np.array(image),extent=[-5,5,-5,5],\n",
    "               interpolation='nearest', origin='lower', cmap='seismic')\n",
    "    plt.xlabel('param '+param)\n",
    "t=plt.suptitle('Example of 11x11 degrees maps, centered in the storm location.'\n",
    "         +'\\n (surf. temp., surf. pressure, 1000hPa humidity and 700hPa vorticity)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/sophiegif/ramp_kit_storm_forecast_new/blob/master/figures_pynb/pipeline.png?raw=true\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For submitting at the [RAMP site](http://ramp.studio), you will have to write two classes, saved in a specific file:   \n",
    "\n",
    "* a class `FeatureExtractor` in a `feature_extractor.py` file.\n",
    "* a class `Regressor` in a `regressor.py` file.\n",
    "\n",
    "You can look at the simple examples provided in /submissions:\n",
    "- starting_kit : using only the track data\n",
    "- starting_kit_pressure_map : using both track data and image data\n",
    "\n",
    "### Using data from previous time steps\n",
    "Of course, you can use the data from previous time steps, e.g., for the prediction of the intensity of storm S at t=3 you can use data from S at t=\\[0:2\\]. However, it is completely forbidden (and we check it!) to use future data like S at t=4,.. This is illustrated in the figure below, where the estimation of the 24h-forecast of the time instant 2 (red line) can use blue but not red features.\n",
    "\n",
    "- `illegal_lookahead`: this simple submission illustrates the error you will have if you are illegally looking ahead time of the same storm.\n",
    "- `legal_lookbefore` : this simple submission illustrates how to use information from previous time steps of the same storm.\n",
    "\n",
    "<img src=\"https://github.com/sophiegif/ramp_kit_storm_forecast_new/blob/master/figures_pynb/illegal_lookahead.png?raw=true\" width=\"70%\">\n",
    "<div style=\"text-align: center\">Data from previous steps are allowed, but data from future steps are forbidden.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "The framework is evaluated with a cross-validation approach. The metric used is the RMSE (root mean square error) in knots across all storm time instants. We also made visible three other metrics: `mae` is the mean absolute error, in knots. `mae_hurr` is the MAE using only time instants corresponding to hurricanes (windspeed>64 knots), while `rel_mae_hurr` is the relative RMSE on hurricanes. These metrics are interesting because the current forecasting practice is to exclude all other stages of development (e.g., extratropical, tropical wave...), see [this page](https://www.nhc.noaa.gov/verification/verify5.shtml?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the submission\n",
    "You can test locally our pipeline using `ramp_test_submission` command line (`-h` will give you all infos). For that, open a terminal in your `storm_forecast/` folder and type on a terminal `ramp_test_submission --submission starting_kit`. You can then copy a submission example in `submissions/<YOUR_SUBMISSION_NAME>/`and modify its codes as you want. Finally, test it on your computer with `ramp_test_submission --submission <YOUR_SUBMISSION_NAME>`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get to see the train and test scores, and no errors, then you can submit your model to the ramp.studio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some warnings when building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "\n",
    " <ul>\n",
    "  <li>If you want to use the features from previous time steps in your learning (for example using LSTMs), you will have to use the 'stormid' and the 'instant_t' columns. Moreover, you will have to handle separetly the first time steps, which are not provided with past data.</li>\n",
    "  <li>The intensity value to predict is the max windspeed. However, this value was measured empirically with a precision of ~5knots. </li>\n",
    "</ul> \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting to the online challenge: ramp.studio\n",
    "\n",
    "Once you have found a good model, you can submit them to [ramp.studio](http://www.ramp.studio) to enter the online challenge. First, if it is your first time using the RAMP platform, [sign up](http://www.ramp.studio/sign_up), otherwise [log in](http://www.ramp.studio/login). Then sign up to the event [storm_forecast_CI2018](http://www.ramp.studio/events/storm_forecast_CI2018). Sign up for the event. Both signups are controled by RAMP administrators, so there **can be a delay between asking for signup and being able to submit**.\n",
    "\n",
    "Once your signup request is accepted, you can go to your [sandbox](http://www.ramp.studio/events/storm_forecast_CI2018/sandbox) and copy-paste (or upload) [`feature_extractor.py`](/edit/submissions/starting_kit/feature_extractor.py) and [`classifier.py`](/edit/submissions/starting_kit/classifier.py). Save it, rename it, then submit it. The submission is trained and tested on our backend in the similar way as `ramp_test_submission` does it locally. While your submission is waiting in the queue and being trained, you can find it in the \"New submissions (pending training)\" table in [my submissions](http://www.ramp.studio/events/storm_forecast_CI2018/my_submissions). Once it is trained your submission shows up on the [public leaderboard](http://www.ramp.studio/events/storm_forecast_CI2018/leaderboard). \n",
    "If there is an error (despite having tested your submission locally with `ramp_test_submission`), it will show up in the \"Failed submissions\" table in [my submissions](http://www.ramp.studio/events/storm_forecast_CI2018/my_submissions). You can click on the error to see part of the trace.\n",
    "\n",
    "After submission, do not forget to give credit to the previous submissions you reused or integrated into your submission.\n",
    "\n",
    "The data set we use on the backend is usually different from what you find in the starting kit, so the score may be different.\n",
    "\n",
    "The official score in this RAMP (the first score column after \"historical contributivity\" on the [leaderboard](http://www.ramp.studio/events/storm_forecast_CI2018/leaderboard)) is the RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More information\n",
    "\n",
    "You can find more information in the [README](https://github.com/paris-saclay-cds/ramp-workflow/blob/master/README.md) of the [ramp-workflow library](https://github.com/paris-saclay-cds/ramp-workflow)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contact\n",
    "\n",
    "Don't hesitate to [contact us](mailto:admin@ramp.studio?subject=Storm forecast CI2018 ramp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homemade Forest Parameter Tuning. Attempt at Multiprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from multiprocessing import Pool\n",
    "import itertools\n",
    "\n",
    "\n",
    "def forest_search(max_depth, max_features, n_estimators):\n",
    "    \n",
    "    cart_prod = list(itertools.product(max_depth,max_features,n_estimators))\n",
    "    n_splits = 5\n",
    "    forest_dict = {}\n",
    "\n",
    "\n",
    "    for param_combo in cart_prod:\n",
    "        mse_sum = 0\n",
    "        for train_index, test_index in get_cv(all_features,y_true,n_splits):\n",
    "            forest_regressor = Pipeline([('imputer', Imputer(strategy='median')), \n",
    "                ('scale', StandardScaler()),       \n",
    "                ('regressor', RandomForestRegressor(max_depth = param_combo[0], max_features = param_combo[1], n_estimators = param_combo[2]))])\n",
    "            drop = all_features.drop(columns = ['stormid'])\n",
    "\n",
    "            X_train, X_test = drop.iloc[train_index,:], drop.iloc[test_index,:]\n",
    "            y_train, y_test = y_true[train_index], y_true[test_index]\n",
    "\n",
    "            forest_regressor.fit(X_train,y_train)\n",
    "            y_pred = forest_regressor.predict(X_test)\n",
    "            mse_sum += mean_squared_error(y_test, y_pred)\n",
    "\n",
    "        forest_dict[param_combo] = np.sqrt(mse_sum/n_splits)\n",
    "        print('Elements in Dict:', len(forest_dict))\n",
    "    return forest_dict\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    p = Pool(6)\n",
    "\n",
    "    max_depth = [5,10,15,20,25]\n",
    "    max_features = ['sqrt', 'log2', None]\n",
    "    n_estimators = [10,25,50,100,250,500]\n",
    "\n",
    "    print(type(max_depth))\n",
    "    forest_dict = p.starmap(forest_search, zip([5,10,15,20,25],['sqrt', 'log2', None], [10,25,50,100,250,500]))\n",
    "    p.close()\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "class Regressor(BaseEstimator):\n",
    "    def __init__(self, kernel = 'rbf', gamma = .1, C = 1):\n",
    "        self.reg = Pipeline([('scale', StandardScaler()),\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('regressor', SVR(kernel = kernel, gamma = gamma, C = C))                 \n",
    "        ])\n",
    "        self.kernel = kernel\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.reg.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        print(\"Kernel:\", self.kernel)\n",
    "        return self.reg.predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import itertools\n",
    "\n",
    "# This actually decreases performance:\n",
    "'''\n",
    "data_train['previous_windspeed'] = data_train['windspeed'].shift(1)\n",
    "data_train['previous_windspeed'].loc[data_train['stormid'] != data_train['stormid'].shift(1)] = np.nan\n",
    "data_train['previous_windspeed'].loc[data_train['previous_windspeed'] == np.nan] = data_train['windspeed']\n",
    "data_train['previous_windspeed'].fillna(method='bfill', inplace = True)\n",
    "'''\n",
    "\n",
    "# This is an illegal lookahead???:\n",
    "'''\n",
    "data_train['previous_y'] = y_true\n",
    "data_train['previous_y'] = data_train['previous_y'].shift(1)\n",
    "data_train['previous_y'].loc[data_train['stormid'] != data_train['stormid'].shift(1)] = np.nan\n",
    "data_train['previous_y'].fillna(method='bfill', inplace = True)\n",
    "'''\n",
    "df = \n",
    "all_features = pd.concat([data_train.loc[:,'stormid'], data_train.loc[:,'latitude':'max_wind_change_12h'],pd.get_dummies(data_train.nature, prefix = 'nature'), pd.get_dummies(data_train.basin, prefix = 'basin'), extra_dummy, data_train.loc[:,'dist2land':] ], axis = 1)\n",
    "\n",
    "# Manually add dummy variables for the basins:\n",
    "extra_dummy = pd.DataFrame(data = np.zeros((len(data_train),8)), columns = ['basin_7', 'basin_8', 'basin_9', 'basin_10', 'basin_11', 'basin_12', 'basin_13', 'basin_14'])\n",
    "\n",
    "# For \"illegal lookahead\" features:\n",
    "#zero_d = pd.concat([data_train.loc[:,'stormid'], data_train.loc[:,'latitude':'max_wind_change_12h'], data_train.loc[:,'dist2land'], pd.get_dummies(data_train.nature, prefix = 'nature'), pd.get_dummies(data_train.basin, prefix = 'basin'), extra_dummy, data_train.loc[:,'previous_y']], axis = 1)\n",
    "\n",
    "# For basin and nature features:\n",
    "zero_d = pd.concat([data_train.loc[:,'stormid'], data_train.loc[:,'latitude':'max_wind_change_12h'], data_train.loc[:,'dist2land'], pd.get_dummies(data_train.nature, prefix = 'nature'), pd.get_dummies(data_train.basin, prefix = 'basin'), extra_dummy], axis = 1)\n",
    "\n",
    "# For basin, nature, and previous windspeed features:\n",
    "#zero_d = pd.concat([data_train.loc[:,'stormid'], data_train.loc[:,'latitude':'max_wind_change_12h'], data_train.loc[:,'dist2land'], pd.get_dummies(data_train.nature, prefix = 'nature'), pd.get_dummies(data_train.basin, prefix = 'basin'), extra_dummy, data_train.loc[:,'previous_windspeed']], axis = 1)\n",
    "\n",
    "# For nature feature:\n",
    "#zero_d = pd.concat([data_train.loc[:,'latitude':'max_wind_change_12h'], data_train.loc[:,'dist2land'], pd.get_dummies(data_train.nature, prefix = 'nature')], axis = 1)\n",
    "\n",
    "all_features = pd.concat([data_train.loc[:,'stormid'], data_train.loc[:,'latitude':'max_wind_change_12h'],pd.get_dummies(data_train.nature, prefix = 'nature'), pd.get_dummies(data_train.basin, prefix = 'basin'), extra_dummy, data_train.loc[:,'dist2land':] ], axis = 1)\n",
    "\n",
    "zero_d_uvz = pd.concat([zero_d, data_train[[col for col in data_train.columns if col.startswith(('z_','u_','v_'))]]], axis = 1)\n",
    "\n",
    "zero_d_sst= pd.concat([zero_d, data_train[[col for col in data_train.columns if col.startswith(('sst_','slp_','hum_'))]]], axis = 1)\n",
    "#print(all_features.loc[:,'sst_0_0':'sst_10_10'])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
