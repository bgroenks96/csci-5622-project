{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utilities ##\n",
    "\n",
    "import rampwf as rw\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict\n",
    "from typing import Tuple\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from sklearn.preprocessing import Imputer, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.utils.graph_shortest_path import graph_shortest_path\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import multiprocessing as mp\n",
    "import itertools\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "\n",
    "def parallel(fn, params, n_jobs=mp.cpu_count()):\n",
    "    pool = mp.Pool(n_jobs)\n",
    "    print('started new process pool with {} processes'.format(n_jobs))\n",
    "    try:\n",
    "        res = pool.map(fn, params)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "    except:\n",
    "        print('process pool interrupted, shutting down')\n",
    "        pool.terminate()\n",
    "        pool.join()\n",
    "        raise\n",
    "    return res\n",
    "\n",
    "mplog = mp.get_logger()\n",
    "\n",
    "def pca(X_df, n_components=2):\n",
    "    imputer = Imputer(strategy='median')\n",
    "    scaler = StandardScaler()\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca_pipeline = Pipeline([('med_imputer', imputer),('scaler', scaler),('pca',pca)])\n",
    "    X_pc = pca_pipeline.fit_transform(X_df)    \n",
    "    pca_comp_feat_ratios = np.square(pca.components_)\n",
    "    feat_names = X_df.columns\n",
    "    pc_feat_contrib = pd.DataFrame(pca_comp_feat_ratios, columns=feat_names)\n",
    "    return X_pc #, pc_feat_contrib, pca.components_, pca.explained_variance_ratio_\n",
    "\n",
    "def rand_projection(X_df, n_components='auto', eps=0.1):\n",
    "    imputer = Imputer(strategy='median')\n",
    "    scaler = StandardScaler()\n",
    "    proj = SparseRandomProjection(n_components=n_components, eps=eps)\n",
    "    proj_pipeline = Pipeline([('med_imputer', imputer),('scaler', scaler),('proj', proj)])\n",
    "    X_rp = proj_pipeline.fit_transform(X_df)\n",
    "    return X_rp #, proj.n_components_\n",
    "\n",
    "def umap(X_df, n_components=2, y=None, n_neighbors=5, min_dist=0.1, metric='correlation'):\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    imputer = Imputer(strategy='median')\n",
    "    scaler = StandardScaler()\n",
    "    umap = UMAP(n_components=n_components, n_neighbors=n_neighbors, min_dist=min_dist, metric=metric)\n",
    "    umap_pipeline = Pipeline([('med_imputer', imputer),('scaler', scaler),('umap', umap)])\n",
    "    X_umap = umap_pipeline.fit_transform(X_df, y)\n",
    "    warnings.resetwarnings()\n",
    "    return X_umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation Generator\n",
    "\n",
    "class TimeSeries(object):\n",
    "    def __init__(self, n_cv=8, cv_block_size=0.5, period=12, unit='',\n",
    "                 unit_2=None):\n",
    "        self.n_cv = n_cv\n",
    "        self.cv_block_size = cv_block_size\n",
    "        self.period = period\n",
    "        self.unit = unit\n",
    "        self.unit_2 = unit_2\n",
    "\n",
    "    def get_cv(self, X, y):\n",
    "        n = len(y)\n",
    "        block_size = int(\n",
    "            n * self.cv_block_size / self.n_cv / self.period) * self.period\n",
    "        n_common_block = n - block_size * self.n_cv\n",
    "        n_validation = n - n_common_block\n",
    "        #if self.unit_2 is None:\n",
    "            #print('length of common block: {} {}s'.format(\n",
    "             #   n_common_block, self.unit))\n",
    "            #print('length of validation block: {} {}s'.format(\n",
    "             #   n_validation, self.unit))\n",
    "            #print('length of each cv block: {} {}s'.format(\n",
    "             #  block_size, self.unit))\n",
    "        #else:\n",
    "            #print('length of common block: {} {}s = {} {}s'.format(\n",
    "             #   n_common_block, self.unit, n_common_block / self.period,\n",
    "             #   self.unit_2))\n",
    "            #print('length of validation block: {} {}s = {} {}s'.format(\n",
    "             #   n_validation, self.unit, n_validation / self.period,\n",
    "             #   self. unit_2))\n",
    "            #print('length of each cv block: {} {}s = {} {}s'.format(\n",
    "             #   block_size, self.unit, block_size / self.period, self.unit_2))\n",
    "        for i in range(self.n_cv):\n",
    "            train_is = np.arange(0, n_common_block + i * block_size)\n",
    "            test_is = np.arange(n_common_block + i * block_size, n)\n",
    "            yield (train_is, test_is)\n",
    "            \n",
    "cv = TimeSeries(n_cv=8, cv_block_size=0.5, period=12, unit='month', unit_2='year')\n",
    "\n",
    "#for train_is, test_is in cv.get_cv(X_df,y_df):\n",
    "#    y = X_df.iloc[train_is,:]\n",
    "#    print(y)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homemade Forest Parameter Tuning \n",
    "def forest(X_df, y_df, cv, max_depth, max_features, n_estimators, n_components=None):\n",
    "    mse_sum = 0\n",
    "    for train_index, test_index in cv.get_cv(X_df,y_df):\n",
    "        forest_regressor = Pipeline([('imputer', Imputer(strategy='median')), \n",
    "            ('scale', StandardScaler()), \n",
    "            #('dim_red', PCA(n_components = n_components)),       \n",
    "            ('regressor', RandomForestRegressor(max_depth = max_depth, max_features = max_features, n_estimators = n_estimators))])\n",
    "\n",
    "        X_train, X_test = X_df.iloc[train_index,:], X_df.iloc[test_index,:]\n",
    "        y_train, y_test = y_df.iloc[train_index], y_df.iloc[test_index]\n",
    "        y_train, y_test = y_train.values.reshape(y_train.shape[0]), y_test.values.reshape(y_test.shape[0])\n",
    "\n",
    "        forest_regressor.fit(X_train, y_train)\n",
    "        y_pred = forest_regressor.predict(X_test)\n",
    "        mse_sum += mean_squared_error(y_test, y_pred)\n",
    "        \n",
    "    return ((max_depth, max_features, n_estimators),np.sqrt(mse_sum/cv.n_cv))\n",
    "\n",
    "def run_forest(p):\n",
    "    d, f, ne = p\n",
    "    return forest(X_all, y_df, cv, max_depth=d, max_features=f, n_estimators=ne)\n",
    "\n",
    "# Homemade Tree Parameter Tuning \n",
    "def tree(X_df, y_df, cv, max_depth, max_features, n_components=None):\n",
    "    mse_sum = 0\n",
    "    for train_index, test_index in cv.get_cv(X_df,y_df):\n",
    "        tree_regressor = Pipeline([('imputer', Imputer(strategy='median')), \n",
    "            ('scale', StandardScaler()),\n",
    "            #('dim_red', PCA(n_components = n_components)), \n",
    "            ('regressor', DecisionTreeRegressor(max_depth = max_depth, max_features = max_features))])\n",
    "        X_train, X_test = X_df.iloc[train_index,:], X_df.iloc[test_index,:]\n",
    "        y_train, y_test = y_df.iloc[train_index], y_df.iloc[test_index]\n",
    "        y_train, y_test = y_train.values.reshape(y_train.shape[0]), y_test.values.reshape(y_test.shape[0])\n",
    "        tree_regressor.fit(X_train,y_train)\n",
    "        y_pred = tree_regressor.predict(X_test)\n",
    "        mse_sum += mean_squared_error(y_test, y_pred)\n",
    "        \n",
    "    return ((max_depth, max_features),np.sqrt(mse_sum/cv.n_cv))\n",
    "\n",
    "def run_tree(p):\n",
    "    d, f = p\n",
    "    return tree(X_all, y_df, cv, max_depth=d, max_features=f)\n",
    "\n",
    "# Homemade SVR Parameter Tuning \n",
    "def svr(X_df, y_df, cv, C, gamma, n_components=None):\n",
    "    mse_sum = 0\n",
    "    for train_index, test_index in cv.get_cv(X_df,y_df):\n",
    "        rbf_regressor = Pipeline([('imputer', Imputer(strategy='median')),\n",
    "            ('scale', StandardScaler()),\n",
    "            #('dim_red', PCA(n_components = n_components)), \n",
    "            ('regressor', SVR(kernel = 'rbf', C = C, gamma = gamma))])\n",
    "        \n",
    "        X_train, X_test = X_df.iloc[train_index,:], X_df.iloc[test_index,:]\n",
    "        y_train, y_test = y_df.iloc[train_index], y_df.iloc[test_index]\n",
    "        y_train, y_test = y_train.values.reshape(y_train.shape[0]), y_test.values.reshape(y_test.shape[0])\n",
    "        rbf_regressor.fit(X_train,y_train)\n",
    "        y_pred = rbf_regressor.predict(X_test)\n",
    "        mse_sum += mean_squared_error(y_test, y_pred)\n",
    "        \n",
    "    return ((C, gamma),np.sqrt(mse_sum/cv.n_cv))\n",
    "\n",
    "def run_svr(p):\n",
    "    c, g = p\n",
    "    return svr(X_all, y_df, cv, C = c, gamma = g)\n",
    "\n",
    "# Homemade Boosted Forest Parameter Tuning \n",
    "def boost(X_df, y_df, cv, max_depth, max_features, n_estimators, n_components=None):\n",
    "    mse_sum = 0\n",
    "    for train_index, test_index in get_cv(X_df,y_df):\n",
    "        boost_regressor = Pipeline([('imputer', Imputer(strategy='median')), \n",
    "            ('scale', StandardScaler()),\n",
    "            #('dim_red', PCA(n_components = n_components)), \n",
    "            ('regressor', GradientBoostingRegressor(max_depth = max_depth, max_features = max_features, n_estimators = n_estimators))])\n",
    "\n",
    "        X_train, X_test = X_df.iloc[train_index,:], X_df.iloc[test_index,:]\n",
    "        y_train, y_test = y_df.iloc[train_index], y_df.iloc[test_index]\n",
    "        y_train, y_test = y_train.values.reshape(y_train.shape[0]), y_test.values.reshape(y_test.shape[0])\n",
    "\n",
    "        forest_regressor.fit(X_train,y_train)\n",
    "        y_pred = forest_regressor.predict(X_test)\n",
    "        mse_sum += mean_squared_error(y_test, y_pred)\n",
    "        \n",
    "    return ((max_depth, max_features, n_estimators),np.sqrt(mse_sum/cv.n_cv))\n",
    "\n",
    "def run_boost(p):\n",
    "    d, f, ne = p\n",
    "    return boost(X_all, y_df, cv, max_depth=d, max_features=f, n_estimators=ne)\n",
    "\n",
    "# Homemade Lasso Parameter Tuning \n",
    "def lasso(X_df, y_df, cv, alpha, n_components=None):\n",
    "    lasso_dict = {}\n",
    "    for a in alpha:\n",
    "        mse_sum = 0\n",
    "        for train_index, test_index in cv.get_cv(X_df,y_df):\n",
    "            lasso_regressor = Pipeline([('imputer', Imputer(strategy='median')),\n",
    "                ('scale', StandardScaler()),\n",
    "                #('dim_red', PCA(n_components = n_components)), \n",
    "                ('regressor', Lasso(alpha = a))])\n",
    "\n",
    "            X_train, X_test = X_df.iloc[train_index,:], X_df.iloc[test_index,:]\n",
    "            y_train, y_test = y_df.iloc[train_index], y_df.iloc[test_index]\n",
    "            y_train, y_test = y_train.values.reshape(y_train.shape[0]), y_test.values.reshape(y_test.shape[0])\n",
    "\n",
    "            lasso_regressor.fit(X_train,y_train)\n",
    "            y_pred = lasso_regressor.predict(X_test)\n",
    "            mse_sum += mean_squared_error(y_test, y_pred)\n",
    "\n",
    "        lasso_dict[a] = np.sqrt(mse_sum/cv.n_cv)\n",
    "    return lasso_dict\n",
    "\n",
    "# Homemade Ridge Parameter Tuning \n",
    "def ridge(X_df, y_df, cv, alpha, n_components=None):\n",
    "    ridge_dict = {}\n",
    "    for a in alpha:\n",
    "        mse_sum = 0\n",
    "        for train_index, test_index in get_cv(X_df,y_df):\n",
    "            ridge_regressor = Pipeline([('imputer', Imputer(strategy='median')),\n",
    "                ('scale', StandardScaler()),\n",
    "                #('dim_red', PCA(n_components = n_components)), \n",
    "                ('regressor', Ridge(alpha = a))])\n",
    "\n",
    "            X_train, X_test = X_df.iloc[train_index,:], X_df.iloc[test_index,:]\n",
    "            y_train, y_test = y_df.iloc[train_index], y_df.iloc[test_index]\n",
    "            y_train, y_test = y_train.values.reshape(y_train.shape[0]), y_test.values.reshape(y_test.shape[0])\n",
    "\n",
    "            ridge_regressor.fit(X_train,y_train)\n",
    "            y_pred = ridge_regressor.predict(X_test)\n",
    "            mse_sum += mean_squared_error(y_test, y_pred)\n",
    "\n",
    "        ridge_dict[a] = np.sqrt(mse_sum/cv.n_cv)\n",
    "    return ridge_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/mchifala/csci-5622-project/sea_ice/sea_ice_X.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-7336b741e772>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Import Data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/mchifala/csci-5622-project/sea_ice/sea_ice_X.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0my_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/mchifala/csci-5622-project/sea_ice/sea_ice_y.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m39\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m58\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    368\u001b[0m     \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_pathlib_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/mchifala/csci-5622-project/sea_ice/sea_ice_X.npy'"
     ]
    }
   ],
   "source": [
    "# Import Data\n",
    "X_all = np.load('/Users/mchifala/csci-5622-project/sea_ice/sea_ice_X.npy')\n",
    "y_df = pd.DataFrame(np.load('/Users/mchifala/csci-5622-project/sea_ice/sea_ice_y.npy'))\n",
    "X_all = pd.DataFrame(np.reshape(X_all, (1500, (8*39*58))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#from mpl_toolkits.basemap import Basemap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "# Load data\n",
    "X_ds = xr.open_dataset('data/train.nc')\n",
    "y_array = np.load('data/train.npy') \n",
    "X_arr = X_ds.to_array().values\n",
    "X_arr = np.swapaxes(X_arr, 0, 1)\n",
    "X_all = pd.DataFrame(X_arr.reshape(X_arr.shape[0], X_arr.shape[1]*X_arr.shape[2]*X_arr.shape[3]))\n",
    "y_df = pd.DataFrame(y_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-5085dcfab44d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mN_param_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mparams_forest_boost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams_forest_boost\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_forest_boost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_param_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mresults_forest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_forest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_forest_boost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mreg_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'forest_regressor'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_forest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "reg_dict = {}\n",
    "\n",
    "max_depth = [5,10,20,25]\n",
    "max_depth_tree = [5,10,15,20,25,50,100]\n",
    "max_features = ['sqrt', 'log2']\n",
    "n_estimators = [10,25,50,100,200]\n",
    "alpha_lasso = [.00001, .0001, .001, .01, .1, 1]\n",
    "alpha_ridge = [1, 2, 5, 10, 25, 50, 100, 200, 500, 1000]\n",
    "C = np.logspace(-2, 2, num=5) \n",
    "gamma = np.logspace(-4, 2, num=7)\n",
    "\n",
    "params_forest_boost = list(itertools.product(max_depth, max_features, n_estimators))\n",
    "params_tree = list(itertools.product(max_depth, max_features))\n",
    "params_svr = list(itertools.product(C, gamma))\n",
    "\n",
    "results_forest = parallel(run_forest, params_forest_boost)\n",
    "reg_dict['forest_regressor'] = dict(results_forest)\n",
    "np.save('sea_ice_all_tuning.npy', reg_dict)\n",
    "print(\"Saved!\")\n",
    "\n",
    "results_tree = parallel(run_tree, params_tree)\n",
    "reg_dict['tree_regressor'] = dict(results_tree)\n",
    "np.save('sea_ice_all_tuning.npy', reg_dict)\n",
    "print(\"Saved!\")\n",
    "\n",
    "results_boost = parallel(run_boost, params_forest_boost)\n",
    "reg_dict['boost_regressor'] = dict(results_boost)\n",
    "np.save('sea_ice_all_tuning.npy',reg_dict)\n",
    "print(\"Saved!\")\n",
    "\n",
    "results_svr = parallel(run_svr, params_svr)\n",
    "reg_dict['svr_regressor'] = dict(results_svr)\n",
    "np.save('sea_ice_all_tuning.npy', reg_dict)\n",
    "print(\"Saved!\")\n",
    "\n",
    "reg_dict['ridge_regressor'] = ridge(X_all, y_df, alpha_ridge)\n",
    "np.save('sea_ice_all_tuning.npy', reg_dict)\n",
    "print(\"Saved!\")\n",
    "\n",
    "reg_dict['lasso_regressor'] = lasso(X_all, y_df, alpha_lasso)\n",
    "np.save('sea_ice_all_tuning.npy', reg_dict)\n",
    "print(\"Saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
